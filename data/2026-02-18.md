<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 43]
- [cs.CL](#cs.CL) [Total: 37]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GRAFNet: Multiscale Retinal Processing via Guided Cortical Attention Feedback for Enhancing Medical Image Polyp Segmentation](https://arxiv.org/abs/2602.15072)
*Abdul Joseph Fofanah,Lian Wen,Alpha Alimamy Kamara,Zhongyi Zhang,David Chen,Albert Patrick Sankoh*

Main category: cs.CV

TL;DR: GRAFNet是一种受生物视觉系统启发的结肠息肉分割网络，通过模拟人类视觉层次结构，整合了三个关键模块来解决息肉分割中的形态变异大、视觉相似性高和多尺度检测需求等挑战，在多个公开基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 结肠息肉分割对于癌症预防至关重要，但面临三个主要挑战：(1)息肉形态高度变异（从扁平到突出病变），(2)与正常结构（如褶皱和血管）视觉相似度高，(3)需要鲁棒的多尺度检测。现有深度学习方法存在单向处理、多尺度融合弱、缺乏解剖约束等问题，导致假阳性和假阴性结果。

Method: 提出GRAFNet架构，模拟人类视觉系统的层次组织，包含三个关键模块：1)引导不对称注意力模块（GAAM）-模拟方向调谐皮层神经元以强调息肉边界；2)多尺度视网膜模块（MSRM）-复制视网膜神经节细胞通路进行并行多特征分析；3)引导皮层注意力反馈模块（GCAFM）-应用预测编码进行迭代细化。这些模块通过息肉编码器-解码器模块（PEDM）统一，通过分辨率自适应反馈强制执行空间语义一致性。

Result: 在五个公开基准测试（Kvasir-SEG、CVC-300、CVC-ColonDB、CVC-Clinic和PolypGen）上进行了广泛实验，结果显示：Dice系数提升3-8%，泛化能力提高10-20%，优于领先方法，同时提供可解释的决策路径。

Conclusion: 这项工作建立了一个新范式，通过神经计算原理弥合了AI准确性与临床可信推理之间的差距，为医学图像分割提供了既准确又可解释的解决方案。

Abstract: Accurate polyp segmentation in colonoscopy is essential for cancer prevention but remains challenging due to: (1) high morphological variability (from flat to protruding lesions), (2) strong visual similarity to normal structures such as folds and vessels, and (3) the need for robust multi-scale detection. Existing deep learning approaches suffer from unidirectional processing, weak multi-scale fusion, and the absence of anatomical constraints, often leading to false positives (over-segmentation of normal structures) and false negatives (missed subtle flat lesions). We propose GRAFNet, a biologically inspired architecture that emulates the hierarchical organisation of the human visual system. GRAFNet integrates three key modules: (1) a Guided Asymmetric Attention Module (GAAM) that mimics orientation-tuned cortical neurones to emphasise polyp boundaries, (2) a MultiScale Retinal Module (MSRM) that replicates retinal ganglion cell pathways for parallel multi-feature analysis, and (3) a Guided Cortical Attention Feedback Module (GCAFM) that applies predictive coding for iterative refinement. These are unified in a Polyp Encoder-Decoder Module (PEDM) that enforces spatial-semantic consistency via resolution-adaptive feedback. Extensive experiments on five public benchmarks (Kvasir-SEG, CVC-300, CVC-ColonDB, CVC-Clinic, and PolypGen) demonstrate consistent state-of-the-art performance, with 3-8% Dice improvements and 10-20% higher generalisation over leading methods, while offering interpretable decision pathways. This work establishes a paradigm in which neural computation principles bridge the gap between AI accuracy and clinically trustworthy reasoning. Code is available at https://github.com/afofanah/GRAFNet.

</details>


### [2] [Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition](https://arxiv.org/abs/2602.15124)
*Shiyu Xuan,Dongkai Wang,Zechao Li,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出解耦框架将目标检测与交互识别分离，利用多模态大语言模型进行零样本交互识别，通过确定性生成方法和空间感知池化模块实现无需训练或微调的零样本HOI检测


<details>
  <summary>Details</summary>
Motivation: 现有方法将交互识别与特定检测器紧密耦合，依赖粗粒度视觉语言模型特征，限制了未见交互的泛化能力

Method: 采用解耦框架，分离目标检测和交互识别；使用确定性生成方法将交互识别建模为视觉问答任务；设计空间感知池化模块整合外观和空间线索；开发单次确定性匹配方法

Result: 在HICO-DET和V-COCO数据集上实现优越的零样本性能，具有强大的跨数据集泛化能力，并能与任何目标检测器集成而无需重新训练

Conclusion: 该解耦框架为HOI检测提供了灵活且可泛化的解决方案，通过MLLMs和空间感知机制有效解决了零样本交互识别挑战

Abstract: Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at https://github.com/SY-Xuan/DA-HOI.

</details>


### [3] [MB-DSMIL-CL-PL: Scalable Weakly Supervised Ovarian Cancer Subtype Classification and Localisation Using Contrastive and Prototype Learning with Frozen Patch Features](https://arxiv.org/abs/2602.15138)
*Marcus Jenkins,Jasenka Mazibrada,Bogdan Leahu,Michal Mackiewicz*

Main category: cs.CV

TL;DR: 提出了一种基于对比学习和原型学习的新方法，用于卵巢癌组织病理学图像亚型分类和定位，使用预计算冻结特征并通过特征空间增强，在保持冻结特征优势的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 卵巢癌组织病理学亚型研究对个性化治疗至关重要，但诊断工作量增加给病理科带来挑战。传统方法依赖预计算特征，而端到端方法虽精度高但训练可扩展性差且实验耗时。

Method: 采用对比学习和原型学习结合预计算冻结特征，通过特征空间增强技术实现亚型分类和定位，避免了端到端训练的计算负担。

Result: 相比DSMIL方法，实例级分类F1分数提升70.4%，切片级分类F1分数提升15.3%，实例定位AUC提升16.9%，切片分类AUC提升2.3%，同时保持使用冻结特征的优势。

Conclusion: 该方法在卵巢癌组织病理学图像分析中实现了性能显著提升，同时保持了计算效率，为临床诊断提供了可行的AI解决方案。

Abstract: The study of histopathological subtypes is valuable for the personalisation of effective treatment strategies for ovarian cancer. However, increasing diagnostic workloads present a challenge for UK pathology departments, leading to the rise in AI approaches. While traditional approaches in this field have relied on pre-computed, frozen image features, recent advances have shifted towards end-to-end feature extraction, providing an improvement in accuracy but at the expense of significantly reduced scalability during training and time-consuming experimentation. In this paper, we propose a new approach for subtype classification and localisation in ovarian cancer histopathology images using contrastive and prototype learning with pre-computed, frozen features via feature-space augmentations. Compared to DSMIL, our method achieves an improvement of 70.4\% and 15.3\% in F1 score for instance- and slide-level classification, respectively, along with AUC gains of 16.9\% for instance localisation and 2.3\% for slide classification, while maintaining the use of frozen patch features.

</details>


### [4] [Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories](https://arxiv.org/abs/2602.15154)
*Praditha Alwis,Soumyadeep Chandra,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CV

TL;DR: 提出基于累积样本损失(CSL)的视频标注错误检测方法，通过分析训练过程中帧级损失轨迹来识别错误标注和时序错乱问题，无需真实错误标签，在EgoPER和Cholec80数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现实世界视频数据集存在标注错误（错误标签和时序错乱），特别是在需要时序一致性的阶段标注任务中，这些错误严重影响模型训练效果。

Method: 提出模型无关的CSL方法：训练视频分割模型并保存每个epoch的权重，使用这些检查点评估测试视频中每帧的损失，将累积损失高的帧标记为可能的标注错误。

Result: 在EgoPER和Cholec80数据集上表现出强大的检测性能，能有效识别错误标签和帧序错乱等细微不一致性。

Conclusion: 该方法为视频机器学习中的数据集审计和训练可靠性改进提供了有力工具，具有通用性和实用性。

Abstract: High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.

</details>


### [5] [Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift](https://arxiv.org/abs/2602.15167)
*Xiaoyi Wen,Fei Jiang*

Main category: cs.CV

TL;DR: 提出一种分布深度学习框架用于4D Flow MRI超分辨率，通过CFD模拟训练和少量真实数据微调，解决临床场景中的域偏移问题，显著优于传统深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 临床医学影像中，低分辨率数据往往来自与简单下采样不同的采集机制，导致传统超分辨率方法因域偏移而泛化性能不佳。

Method: 基于计算流体动力学(CFD)模拟的高分辨率数据及其下采样版本进行初始训练，然后在少量配对的4D Flow MRI和CFD样本上进行微调，采用分布学习框架提高模型鲁棒性。

Result: 理论分析证明了分布估计器的优良性质，实际数据应用显示该框架显著优于传统深度学习方法。

Conclusion: 分布学习方法能有效解决域偏移问题，在临床真实场景中显著提升超分辨率性能，对动脉瘤破裂风险评估等临床应用具有重要意义。

Abstract: Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.

</details>


### [6] [Time-Archival Camera Virtualization for Sports and Visual Performances](https://arxiv.org/abs/2602.15181)
*Yunxiao Zhang,William Stone,Suryansh Kumar*

Main category: cs.CV

TL;DR: 提出一种基于神经体积渲染的相机虚拟化方法，通过多视角刚性变换建模动态场景，支持高效时间归档功能，能够对体育直播等快速动态场景进行高质量的新视角合成和回溯渲染。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的动态场景渲染方法依赖精确点云且无法处理快速非刚性运动，多目标独立运动易破坏高斯跟踪假设，需要一种能够支持时间归档和高质量渲染的解决方案。

Method: 采用神经体积渲染框架，将动态场景建模为多同步相机视角下的刚性变换，通过神经表示学习实现高质量渲染，特别支持时间归档功能。

Result: 方法能够实现时空一致的光真实感渲染，支持用户回溯任意历史时刻的动态场景并进行新视角合成，解决了现有方法无法处理的快速运动和时间归档问题。

Conclusion: 该方法为体育转播和相关应用提供了有效的相机虚拟化解决方案，通过神经体积渲染实现了优于现有方法的时间归档能力和渲染质量。

Abstract: Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...

</details>


### [7] [How to Train Your Long-Context Visual Document Model](https://arxiv.org/abs/2602.15257)
*Austin Veselka*

Main category: cs.CV

TL;DR: 首个大规模长上下文视觉语言模型研究，训练上下文长度达344K，专注于长文档视觉问答，并在长上下文文本任务上表现出迁移能力。研究包括持续预训练、监督微调和偏好优化，在MMLongBenchDoc上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源长上下文视觉语言模型（如Qwen3 VL和GLM 4.5/6V）的训练配方和数据管道不可复现，需要系统研究来填补这一空白。

Method: 系统研究24B和32B参数模型的持续预训练、监督微调和偏好优化，使用广泛的长期上下文评估和消融实验，并开发合成数据管道。

Result: 在MMLongBenchDoc基准测试的两个参数规模上都达到了最先进性能；发现训练上下文长度与评估长度匹配时效果最佳；页面索引训练评估能显著提升性能；视觉长上下文训练能迁移到文本长上下文任务。

Conclusion: 该研究提供了可复现的长上下文视觉语言模型训练框架，发布了修正版基准MMLBD-C，证明了视觉与文本长上下文能力的双向迁移性，为后续研究奠定了基础。

Abstract: We present the first comprehensive, large-scale study of training long-context vision language models up to 344K context, targeting long-document visual question answering with measured transfer to long-context text. While several such strong are open-weight, namely Qwen3 VL and GLM 4.5/6V, their training recipes and data pipelines are not reproducible. We systematically study continued pretraining, supervised finetuning, and preference optimization for 24B and 32B parameter models, backed by extensive LC evaluations and ablations to bridge this gap, and achieve state-of-the-art performance on MMLongBenchDoc for both parameter scales. In addition to this, our key findings include: (i) training on context lengths that match evaluation context lengths outperforms training on longer contexts, (ii) training and evaluating with page indices provides a simple, high-impact boost to long-document performance, (iii) our synthetic data pipelines enable self-improvement via continued pretraining and supervised finetuning, and (iv) we extend the known text-to-visual long context transfer to the reverse, showing that visual long context training transfers to long-context text performance. We also release MMLBD-C, a manually corrected version of MMLongBenchDoc to reduce erroneous and low quality examples in the benchmark.

</details>


### [8] [Accelerating Large-Scale Dataset Distillation via Exploration-Exploitation Optimization](https://arxiv.org/abs/2602.15277)
*Muhammad J. Alahmadi,Peng Gao,Feiyi Wang,Dongkuan,Xu*

Main category: cs.CV

TL;DR: E^2D提出了一种探索-利用蒸馏方法，通过两阶段优化策略减少冗余计算，在大规模数据集蒸馏中同时实现高精度和高效率，在ImageNet-1K上精度超过SOTA且速度快18倍。


<details>
  <summary>Details</summary>
Motivation: 解决现有解耦式数据集蒸馏方法面临的效率-精度权衡问题：基于优化的方法精度高但计算密集，无优化方法高效但精度低。

Method: 采用全图像初始化保持语义完整性和特征多样性，然后通过两阶段优化策略：探索阶段进行均匀更新并识别高损失区域，利用阶段专注于这些区域加速收敛。

Result: 在ImageNet-1K上超越现有最优方法且速度快18倍，在ImageNet-21K上显著提高精度且保持4.3倍速度优势。

Conclusion: 通过有针对性的、减少冗余的更新而非暴力优化，可以弥合大规模数据集蒸馏中精度与效率之间的差距。

Abstract: Dataset distillation compresses the original data into compact synthetic datasets, reducing training time and storage while retaining model performance, enabling deployment under limited resources. Although recent decoupling-based distillation methods enable dataset distillation at large-scale, they continue to face an efficiency gap: optimization-based decoupling methods achieve higher accuracy but demand intensive computation, whereas optimization-free decoupling methods are efficient but sacrifice accuracy. To overcome this trade-off, we propose Exploration-Exploitation Distillation (E^2D), a simple, practical method that minimizes redundant computation through an efficient pipeline that begins with full-image initialization to preserve semantic integrity and feature diversity. It then uses a two-phase optimization strategy: an exploration phase that performs uniform updates and identifies high-loss regions, and an exploitation phase that focuses updates on these regions to accelerate convergence. We evaluate E^2D on large-scale benchmarks, surpassing the state-of-the-art on ImageNet-1K while being 18x faster, and on ImageNet-21K, our method substantially improves accuracy while remaining 4.3x faster. These results demonstrate that targeted, redundancy-reducing updates, rather than brute-force optimization, bridge the gap between accuracy and efficiency in large-scale dataset distillation. Code is available at https://github.com/ncsu-dk-lab.

</details>


### [9] [Visual Persuasion: What Influences Decisions of Vision-Language Models?](https://arxiv.org/abs/2602.15278)
*Manuel Cherep,Pranav M R,Pattie Maes,Nikhil Singh*

Main category: cs.CV

TL;DR: 本文提出一个通过受控图像选择任务研究视觉语言模型(VLMs)视觉偏好的框架，使用视觉提示优化方法系统性地编辑图像并分析模型选择行为，揭示了VLMs的视觉偏好模式和潜在安全风险。


<details>
  <summary>Details</summary>
Motivation: 网络中存在大量图像被VLMs用于决策(点击、推荐、购买等)，但我们对这些AI代理的视觉偏好结构了解甚少，需要系统性的方法来研究其视觉决策机制。

Method: 1) 将VLMs置于受控图像选择任务中；2) 使用视觉提示优化方法，基于图像生成模型对原始图像进行视觉合理的修改(构图、光照、背景等)；3) 通过大规模实验评估编辑对选择概率的影响；4) 开发自动可解释性管道解释视觉偏好。

Result: 优化后的编辑在头对头比较中显著改变了选择概率，识别出了驱动选择的一致视觉主题，证明了该方法能有效揭示VLMs的视觉偏好。

Conclusion: 该方法为发现视觉漏洞和安全问题提供了实用高效的方法，支持对基于图像的AI代理进行更主动的审计和治理，避免在现实环境中被动发现问题。

Abstract: The web is littered with images, once created for human consumption and now increasingly interpreted by agents using vision-language models (VLMs). These agents make visual decisions at scale, deciding what to click, recommend, or buy. Yet, we know little about the structure of their visual preferences. We introduce a framework for studying this by placing VLMs in controlled image-based choice tasks and systematically perturbing their inputs. Our key idea is to treat the agent's decision function as a latent visual utility that can be inferred through revealed preference: choices between systematically edited images. Starting from common images, such as product photos, we propose methods for visual prompt optimization, adapting text optimization methods to iteratively propose and apply visually plausible modifications using an image generation model (such as in composition, lighting, or background). We then evaluate which edits increase selection probability. Through large-scale experiments on frontier VLMs, we demonstrate that optimized edits significantly shift choice probabilities in head-to-head comparisons. We develop an automatic interpretability pipeline to explain these preferences, identifying consistent visual themes that drive selection. We argue that this approach offers a practical and efficient way to surface visual vulnerabilities, safety concerns that might otherwise be discovered implicitly in the wild, supporting more proactive auditing and governance of image-based AI agents.

</details>


### [10] [Consistency-Preserving Diverse Video Generation](https://arxiv.org/abs/2602.15287)
*Xinshuang Liu,Runfa Blark Li,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出一种联合采样框架，通过潜在空间模型改进文本到视频生成中的批次多样性，同时保持时间一致性和颜色自然性，无需视频解码和反向传播。


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成成本高昂，通常每个提示只能生成少量样本。在低样本数量下，需要提高跨视频多样性以最大化批次价值。现有方法虽然能改进图像生成的多样性，但在视频中往往会降低时间一致性且需要昂贵的视频解码器反向传播。

Method: 提出联合采样框架，对流匹配视频生成器应用多样性驱动更新，然后移除会降低时间一致性目标的组件。使用轻量级潜在空间模型计算多样性和时间一致性目标，避免视频解码和解码器反向传播。

Result: 在最先进的文本到视频流匹配模型上的实验显示，该方法在保持与强联合采样基线相当的多样性的同时，显著提高了时间一致性和颜色自然性。

Conclusion: 该方法有效解决了文本到视频生成中多样性与时序一致性的平衡问题，通过潜在空间优化避免了计算开销，为高质量视频生成提供了实用解决方案。

Abstract: Text-to-video generation is expensive, so only a few samples are typically produced per prompt. In this low-sample regime, maximizing the value of each batch requires high cross-video diversity. Recent methods improve diversity for image generation, but for videos they often degrade within-video temporal consistency and require costly backpropagation through a video decoder. We propose a joint-sampling framework for flow-matching video generators that improves batch diversity while preserving temporal consistency. Our approach applies diversity-driven updates and then removes only the components that would decrease a temporal-consistency objective. To avoid image-space gradients, we compute both objectives with lightweight latent-space models, avoiding video decoding and decoder backpropagation. Experiments on a state-of-the-art text-to-video flow-matching model show diversity comparable to strong joint-sampling baselines while substantially improving temporal consistency and color naturalness. Code will be released.

</details>


### [11] [Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models](https://arxiv.org/abs/2602.15315)
*Tai Le-Gia,Jaehyun Ahn*

Main category: cs.CV

TL;DR: 提出了一种无需训练的三维脑MRI零样本异常检测框架，通过聚合多轴切片构建局部体积标记，实现从2D基础模型到3D体积的有效扩展。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测方法主要局限于2D数据集，扩展到3D医学图像时面临挑战，现有方法依赖切片级特征和视觉语言模型，无法捕捉体积结构信息。

Method: 构建局部体积标记，通过聚合多轴切片处理后的特征，使用2D基础模型处理多方向切片，生成紧凑的3D表示，直接与基于距离的批级异常检测流程集成。

Result: 训练免费、批处理的零样本异常检测方法成功从2D编码器扩展到完整3D MRI体积，提供了简单而鲁棒的体积异常检测方法。

Conclusion: 该方法无需微调、提示或监督，可在标准GPU上计算，为3D医学图像异常检测提供了实用且有效的解决方案。

Abstract: Zero-shot anomaly detection (ZSAD) has gained increasing attention in medical imaging as a way to identify abnormalities without task-specific supervision, but most advances remain limited to 2D datasets. Extending ZSAD to 3D medical images has proven challenging, with existing methods relying on slice-wise features and vision-language models, which fail to capture volumetric structure. In this paper, we introduce a fully training-free framework for ZSAD in 3D brain MRI that constructs localized volumetric tokens by aggregating multi-axis slices processed by 2D foundation models. These 3D patch tokens restore cubic spatial context and integrate directly with distance-based, batch-level anomaly detection pipelines. The framework provides compact 3D representations that are practical to compute on standard GPUs and require no fine-tuning, prompts, or supervision. Our results show that training-free, batch-based ZSAD can be effectively extended from 2D encoders to full 3D MRI volumes, offering a simple and robust approach for volumetric anomaly detection.

</details>


### [12] [Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs](https://arxiv.org/abs/2602.15318)
*Libo Zhang,Zhaoning Zhang,Wangyang Hong,Peng Qiao,Dongsheng Li*

Main category: cs.CV

TL;DR: Sparrow框架通过视觉感知文本锚定窗口注意力和中间层视觉状态桥接技术，解决了视频大语言模型中推测解码的性能崩溃问题，在长序列中实现2.82倍加速。


<details>
  <summary>Details</summary>
Motivation: 视频大语言模型中推测解码面临注意力稀释和负视觉增益问题，关键视觉语义在深层交互中被隐式编码到文本隐藏状态，导致原始视觉输入在深度推理中结构冗余。

Method: 1. 视觉感知文本锚定窗口注意力：通过隐藏状态重用将视觉计算完全卸载到目标模型；2. 中间层视觉状态桥接：利用语义丰富的中间状态训练草稿模型，过滤低级视觉噪声；3. 多令牌预测策略：弥合训练-推理分布偏移。

Result: 在包含25k视觉令牌的长序列中平均实现2.82倍加速，有效解决长序列性能退化问题。

Conclusion: Sparrow为实时长视频任务提供了实用解决方案，成功解决了视频大语言模型中推测解码的性能崩溃问题。

Abstract: Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.

</details>


### [13] [EventMemAgent: Hierarchical Event-Centric Memory for Online Video Understanding with Adaptive Tool Use](https://arxiv.org/abs/2602.15329)
*Siwei Wen,Zhangcheng Wang,Xingjian Zhang,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: EventMemAgent是一个基于分层内存模块的主动在线视频理解框架，通过短期内存检测事件边界和事件粒度采样处理流视频，长期内存结构化归档历史事件，结合多粒度感知工具包和Agentic强化学习实现端到端推理。


<details>
  <summary>Details</summary>
Motivation: 解决在线视频理解中无限流媒体输入与多模态大语言模型有限上下文窗口之间的冲突，克服现有被动处理方法在保持长范围上下文和捕获细粒度细节之间的权衡问题。

Method: 采用分层内存结构：短期内存动态检测事件边界并使用事件粒度储层采样处理流视频帧；长期内存按事件结构化归档历史观察；集成多粒度感知工具包进行主动迭代证据捕获；使用Agentic强化学习端到端内化推理和工具使用策略。

Result: 在在线视频基准测试中取得了有竞争力的结果。

Conclusion: EventMemAgent框架通过主动内存管理和强化学习策略，有效解决了在线视频理解中的上下文限制问题，为处理无限流媒体输入提供了可行方案。

Abstract: Online video understanding requires models to perform continuous perception and long-range reasoning within potentially infinite visual streams. Its fundamental challenge lies in the conflict between the unbounded nature of streaming media input and the limited context window of Multimodal Large Language Models (MLLMs). Current methods primarily rely on passive processing, which often face a trade-off between maintaining long-range context and capturing the fine-grained details necessary for complex tasks. To address this, we introduce EventMemAgent, an active online video agent framework based on a hierarchical memory module. Our framework employs a dual-layer strategy for online videos: short-term memory detects event boundaries and utilizes event-granular reservoir sampling to process streaming video frames within a fixed-length buffer dynamically; long-term memory structuredly archives past observations on an event-by-event basis. Furthermore, we integrate a multi-granular perception toolkit for active, iterative evidence capture and employ Agentic Reinforcement Learning (Agentic RL) to end-to-end internalize reasoning and tool-use strategies into the agent's intrinsic capabilities. Experiments show that EventMemAgent achieves competitive results on online video benchmarks. The code will be released here: https://github.com/lingcco/EventMemAgent.

</details>


### [14] [Effective and Robust Multimodal Medical Image Analysis](https://arxiv.org/abs/2602.15346)
*Joy Dhar,Nayyar Zaidi,Maryam Haghighat*

Main category: cs.CV

TL;DR: 提出MAIL和Robust-MAIL网络，通过多注意力机制解决多模态融合学习中的通用性、计算效率和对抗鲁棒性问题，在20个数据集上取得显著性能提升和计算成本降低。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合学习方法存在三个主要局限：1)专注于特定模态，忽视跨模态互补信息，限制多疾病分析通用性；2)计算成本高，在资源受限环境中应用受限；3)缺乏对抗攻击鲁棒性，影响医疗AI可靠性。

Method: 提出MAIL网络，包含两个核心组件：1)高效残差学习注意力块，捕获精细的模态特定多尺度模式；2)高效多模态交叉注意力模块，学习跨模态的丰富互补共享表示。进一步设计Robust-MAIL，通过随机投影滤波器和调制注意力噪声增强对抗鲁棒性。

Result: 在20个公共数据集上的广泛评估显示，MAIL和Robust-MAIL均优于现有方法，性能提升高达9.34%，同时计算成本降低高达78.3%。

Conclusion: 所提方法在多模态医学图像分析中展现出优越性，实现了更可靠的预测性能，同时解决了计算效率和对抗鲁棒性等关键问题，为医疗AI应用提供了更实用的解决方案。

Abstract: Multimodal Fusion Learning (MFL), leveraging disparate data from various imaging modalities (e.g., MRI, CT, SPECT), has shown great potential for addressing medical problems such as skin cancer and brain tumor prediction. However, existing MFL methods face three key limitations: a) they often specialize in specific modalities, and overlook effective shared complementary information across diverse modalities, hence limiting their generalizability for multi-disease analysis; b) they rely on computationally expensive models, restricting their applicability in resource-limited settings; and c) they lack robustness against adversarial attacks, compromising reliability in medical AI applications. To address these limitations, we propose a novel Multi-Attention Integration Learning (MAIL) network, incorporating two key components: a) an efficient residual learning attention block for capturing refined modality-specific multi-scale patterns and b) an efficient multimodal cross-attention module for learning enriched complementary shared representations across diverse modalities. Furthermore, to ensure adversarial robustness, we extend MAIL network to design Robust-MAIL by incorporating random projection filters and modulated attention noise. Extensive evaluations on 20 public datasets show that both MAIL and Robust-MAIL outperform existing methods, achieving performance gains of up to 9.34% while reducing computational costs by up to 78.3%. These results highlight the superiority of our approaches, ensuring more reliable predictions than top competitors. Code: https://github.com/misti1203/MAIL-Robust-MAIL.

</details>


### [15] [CREMD: Crowd-Sourced Emotional Multimodal Dogs Dataset](https://arxiv.org/abs/2602.15349)
*Jinho Baek,Houwei Cao,Kate Blackwell*

Main category: cs.CV

TL;DR: CREMD数据集研究不同呈现方式和标注者特征对狗情绪识别的影响，发现视觉上下文显著提升标注一致性，音频效果不确定，非主人和男性标注者一致性更高，音频增强特定情绪识别信心。


<details>
  <summary>Details</summary>
Motivation: 提升人犬互动、兽医护理和自动化监测系统需要准确的狗情绪识别，但缺乏标准化评估方法和主观性挑战阻碍了进展。

Method: 创建包含923个视频片段的CREMD数据集，采用三种呈现模式（无上下文音频、有上下文无音频、全上下文音频），收集来自不同背景标注者的情绪标注数据。

Result: 视觉上下文显著改善标注一致性；音频效果因设计限制无法确定；非主人和男性标注者比主人和女性标注者一致性更高；专业人士一致性符合预期；音频大幅提升愤怒和恐惧情绪的识别信心。

Conclusion: 多模态数据和多样化标注者群体对狗情绪识别研究至关重要，需要改进实验设计以更好评估音频作用，标注者特征显著影响情绪感知一致性。

Abstract: Dog emotion recognition plays a crucial role in enhancing human-animal interactions, veterinary care, and the development of automated systems for monitoring canine well-being. However, accurately interpreting dog emotions is challenging due to the subjective nature of emotional assessments and the absence of standardized ground truth methods. We present the CREMD (Crowd-sourced Emotional Multimodal Dogs Dataset), a comprehensive dataset exploring how different presentation modes (e.g., context, audio, video) and annotator characteristics (e.g., dog ownership, gender, professional experience) influence the perception and labeling of dog emotions. The dataset consists of 923 video clips presented in three distinct modes: without context or audio, with context but no audio, and with both context and audio. We analyze annotations from diverse participants, including dog owners, professionals, and individuals with varying demographic backgrounds and experience levels, to identify factors that influence reliable dog emotion recognition. Our findings reveal several key insights: (1) while adding visual context significantly improved annotation agreement, our findings regarding audio cues are inconclusive due to design limitations (specifically, the absence of a no-context-with-audio condition and limited clean audio availability); (2) contrary to expectations, non-owners and male annotators showed higher agreement levels than dog owners and female annotators, respectively, while professionals showed higher agreement levels, aligned with our initial hypothesis; and (3) the presence of audio substantially increased annotators' confidence in identifying specific emotions, particularly anger and fear.

</details>


### [16] [DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles](https://arxiv.org/abs/2602.15355)
*Rong Fu,Jiekai Wu,Haiyun Wei,Yee Tan Jia,Wenxin Zhang,Yang Li,Xiaowen Ma,Wangyu Wu,Simon Fong*

Main category: cs.CV

TL;DR: DAV-GSWT：基于扩散先验和主动视角采样的数据高效框架，从最小输入合成高质量高斯溅射Wang Tiles，大幅减少数据需求同时保持视觉完整性


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射方法依赖密集采样的示例重建，在生成大范围景观时存在数据效率低下的限制

Method: 集成分层不确定性量化机制与生成扩散模型，自主识别最具信息量的视角，同时幻觉化缺失的结构细节以确保瓦片无缝过渡

Result: 实验结果表明系统显著减少所需数据量，同时保持大规模虚拟环境所需的视觉完整性和交互性能

Conclusion: 该框架通过扩散先验和主动视图采样实现了数据高效的高质量神经渲染，为大规模环境合成提供了实用解决方案

Abstract: The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments. While procedural methods like Wang Tiles have recently been integrated to facilitate the generation of expansive landscapes, these systems typically remain constrained by a reliance on densely sampled exemplar reconstructions. We present DAV-GSWT, a data-efficient framework that leverages diffusion priors and active view sampling to synthesize high-fidelity Gaussian Splatting Wang Tiles from minimal input observations. By integrating a hierarchical uncertainty quantification mechanism with generative diffusion models, our approach autonomously identifies the most informative viewpoints while hallucinating missing structural details to ensure seamless tile transitions. Experimental results indicate that our system significantly reduces the required data volume while maintaining the visual integrity and interactive performance necessary for large-scale virtual environments.

</details>


### [17] [GMAIL: Generative Modality Alignment for generated Image Learning](https://arxiv.org/abs/2602.15368)
*Shentong Mo,Sukmin Yun*

Main category: cs.CV

TL;DR: GMAIL框架提出了一种新的生成图像判别使用方法，通过多模态学习在潜在空间中桥接真实图像和生成图像两种模态，而不是简单地在像素空间替换，有效提升了多种视觉-语言任务的性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型虽然能合成高真实感图像作为训练数据源，但直接将生成图像当作真实图像使用会导致模态差异问题，甚至引发模式坍塌。需要一种能够区分处理两种模态的方法来充分利用生成图像的优势。

Method: 首先使用跨模态对齐损失在生成图像上微调模型，然后利用这个对齐模型进一步训练各种视觉-语言模型。通过多模态学习方法在相同潜在空间中桥接真实和生成两种模态。

Result: 框架显著提升了图像描述、零样本图像检索、零样本图像分类和长描述检索等任务的性能，显示出良好的生成数据缩放趋势，并在大型多模态模型LLaVA的描述性能上有显著增强。

Conclusion: GMAIL框架通过明确区分生成图像为独立模态并进行跨模态对齐，有效利用了生成模型的优势，为视觉-语言任务提供了一种高效利用生成数据的解决方案，且易于与各种模型集成。

Abstract: Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.

</details>


### [18] [Bridging Day and Night: Target-Class Hallucination Suppression in Unpaired Image Translation](https://arxiv.org/abs/2602.15383)
*Shuwei Li,Lei Tan,Robby T. Tan*

Main category: cs.CV

TL;DR: 提出一种新颖的昼夜图像转换框架，通过双头判别器和类别原型机制检测并抑制目标类别特征的幻觉现象，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决昼夜图像转换中由于外观差异大和缺乏像素级监督导致的语义幻觉问题，这些幻觉会显著降低下游任务的性能表现。

Method: 采用双头判别器进行语义分割以检测背景区域的幻觉内容，构建类别特异性原型作为语义锚点，基于Schrodinger Bridge模型进行迭代优化，将检测到的幻觉特征从类别原型中推离。

Result: 在BDD100K数据集上，日到夜域适应任务mAP提升15.5%，对易产生幻觉的类别（如交通灯）性能提升达31.7%。

Conclusion: 该方法有效解决了无配对图像转换中的语义幻觉问题，通过检测和抑制机制显著改善了跨域图像转换的质量和下游任务的性能。

Abstract: Day-to-night unpaired image translation is important to downstream tasks but remains challenging due to large appearance shifts and the lack of direct pixel-level supervision. Existing methods often introduce semantic hallucinations, where objects from target classes such as traffic signs and vehicles, as well as man-made light effects, are incorrectly synthesized. These hallucinations significantly degrade downstream performance. We propose a novel framework that detects and suppresses hallucinations of target-class features during unpaired translation. To detect hallucination, we design a dual-head discriminator that additionally performs semantic segmentation to identify hallucinated content in background regions. To suppress these hallucinations, we introduce class-specific prototypes, constructed by aggregating features of annotated target-domain objects, which act as semantic anchors for each class. Built upon a Schrodinger Bridge-based translation model, our framework performs iterative refinement, where detected hallucination features are explicitly pushed away from class prototypes in feature space, thus preserving object semantics across the translation trajectory.Experiments show that our method outperforms existing approaches both qualitatively and quantitatively. On the BDD100K dataset, it improves mAP by 15.5% for day-to-night domain adaptation, with a notable 31.7% gain for classes such as traffic lights that are prone to hallucinations.

</details>


### [19] [Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching](https://arxiv.org/abs/2602.15396)
*Jeongwoo Shin,Jinhwan Sul,Joonseok Lee,Jaewong Choi,Jaemoo Choi*

Main category: cs.CV

TL;DR: ASBM是一个新的生成建模框架，通过Schrödinger Bridge方法学习最优轨迹，在非记忆性机制下产生更直、更高效的采样路径，在图像生成中实现更高保真度和更少采样步骤。


<details>
  <summary>Details</summary>
Motivation: 扩散模型由于使用无信息、记忆性的前向过程导致高度弯曲的轨迹和噪声评分目标，需要改进轨迹效率。

Method: 采用两阶段方法：1) 将Schrödinger Bridge前向动态视为耦合构建问题，通过数据到能量采样视角学习；2) 使用简单匹配损失学习后向生成动态。

Result: 在图像生成实验中，ASBM以更少采样步骤实现了更高的保真度，并能蒸馏为一步生成器。

Conclusion: ASBM通过非记忆性机制和最优轨迹学习，在高维数据中显著提高了生成模型的稳定性和效率。

Abstract: Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint Schrödinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the Schrödinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.

</details>


### [20] [Emergent Morphing Attack Detection in Open Multi-modal Large Language Models](https://arxiv.org/abs/2602.15461)
*Marija Ivanovska,Vitomir Štruc*

Main category: cs.CV

TL;DR: 开源多模态大语言模型在零样本条件下首次系统评估用于单图像人脸融合攻击检测，LLaVA1.6-Mistral-7B模型无需微调即达到最先进性能，超越任务专用基线方法23%以上。


<details>
  <summary>Details</summary>
Motivation: 人脸融合攻击威胁生物识别安全，但现有检测系统需要任务特定训练且对未见攻击类型泛化能力差。多模态大语言模型展现出强大的视觉-语言推理能力，但在生物识别取证领域的潜力尚未充分探索。

Method: 使用公开权重的开源多模态大语言模型，采用标准化可复现协议进行零样本评估，测试多种融合技术下的检测能力。

Result: 多数MLLM模型在未经微调的情况下展现出显著区分能力，LLaVA1.6-Mistral-7B在等错误率指标上超越竞争性任务专用基线至少23%，达到最先进性能。

Conclusion: 多模态预训练能够隐式编码指示融合伪影的细粒度面部不一致性，开源MLLM可作为生物识别安全和取证图像分析的可复现、可解释且具有竞争力的基础框架。

Abstract: Face morphing attacks threaten biometric verification, yet most morphing attack detection (MAD) systems require task-specific training and generalize poorly to unseen attack types. Meanwhile, open-source multimodal large language models (MLLMs) have demonstrated strong visual-linguistic reasoning, but their potential in biometric forensics remains underexplored. In this paper, we present the first systematic zero-shot evaluation of open-source MLLMs for single-image MAD, using publicly available weights and a standardized, reproducible protocol. Across diverse morphing techniques, many MLLMs show non-trivial discriminative ability without any fine-tuning or domain adaptation, and LLaVA1.6-Mistral-7B achieves state-of-the-art performance, surpassing highly competitive task-specific MAD baselines by at least 23% in terms of equal error rate (EER). The results indicate that multimodal pretraining can implicitly encode fine-grained facial inconsistencies indicative of morphing artifacts, enabling zero-shot forensic sensitivity. Our findings position open-source MLLMs as reproducible, interpretable, and competitive foundations for biometric security and forensic image analysis. This emergent capability also highlights new opportunities to develop state-of-the-art MAD systems through targeted fine-tuning or lightweight adaptation, further improving accuracy and efficiency while preserving interpretability. To support future research, all code and evaluation protocols will be released upon publication.

</details>


### [21] [RPT-SR: Regional Prior attention Transformer for infrared image Super-Resolution](https://arxiv.org/abs/2602.15490)
*Youngwan Jin,Incheol Park,Yagiz Nalcakan,Hyeongjin Ju,Sanghyeop Yeo,Shiho Kim*

Main category: cs.CV

TL;DR: RPT-SR是一种针对红外图像超分辨率的区域先验注意力Transformer，通过双令牌框架融合场景全局结构先验和局部内容，在固定视角红外成像场景中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 通用超分辨率模型在固定视角红外成像场景（如监控和自动驾驶）中存在效率问题，未能充分利用场景中固有的强空间先验信息，导致冗余学习和次优性能。

Method: 提出区域先验注意力Transformer，采用双令牌框架：可学习的区域先验令牌（存储场景全局结构）和局部令牌（捕获当前帧内容），通过注意力机制让先验动态调制局部重建过程。

Result: 在多个数据集上验证了方法的有效性，在长波红外（LWIR）和短波红外（SWIR）光谱上都实现了新的最先进性能。

Conclusion: RPT-SR成功将场景布局信息编码到注意力机制中，解决了固定视角红外图像超分辨率的效率问题，具有广泛的适用性和多功能性。

Abstract: General-purpose super-resolution models, particularly Vision Transformers, have achieved remarkable success but exhibit fundamental inefficiencies in common infrared imaging scenarios like surveillance and autonomous driving, which operate from fixed or nearly-static viewpoints. These models fail to exploit the strong, persistent spatial priors inherent in such scenes, leading to redundant learning and suboptimal performance. To address this, we propose the Regional Prior attention Transformer for infrared image Super-Resolution (RPT-SR), a novel architecture that explicitly encodes scene layout information into the attention mechanism. Our core contribution is a dual-token framework that fuses (1) learnable, regional prior tokens, which act as a persistent memory for the scene's global structure, with (2) local tokens that capture the frame-specific content of the current input. By utilizing these tokens into an attention, our model allows the priors to dynamically modulate the local reconstruction process. Extensive experiments validate our approach. While most prior works focus on a single infrared band, we demonstrate the broad applicability and versatility of RPT-SR by establishing new state-of-the-art performance across diverse datasets covering both Long-Wave (LWIR) and Short-Wave (SWIR) spectra

</details>


### [22] [LEADER: Lightweight End-to-End Attention-Gated Dual Autoencoder for Robust Minutiae Extraction](https://arxiv.org/abs/2602.15493)
*Raffaele Cappelli,Matteo Ferrara*

Main category: cs.CV

TL;DR: LEADER是一个轻量级端到端注意力门控双自编码器，直接从原始指纹图像提取细节点特征（位置、方向、类型），无需预处理和后处理步骤，参数量仅0.9M，在普通指纹和潜指纹上都表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 指纹识别中的细节点提取正转向深度学习，但真正的端到端方法仍然稀缺，需要消除单独的预处理和后处理步骤。

Method: 提出LEADER神经网络架构，采用新颖的"城堡-护城河-城墙"真值编码和双自编码器结构，通过注意力门控机制连接，集成非极大值抑制和角度解码。

Result: 在NIST SD27数据集上F1分数比专业潜指纹提取器高34%，在47%的样本中排名第一，推理时间GPU仅15ms、CPU 322ms，计算效率优于商业软件。

Conclusion: LEADER实现了真正的端到端细节点提取，学习到的内部表示与指纹领域特征一致，具有优异的准确性和跨域泛化能力，代码和预训练权重已开源。

Abstract: Minutiae extraction, a fundamental stage in fingerprint recognition, is increasingly shifting toward deep learning. However, truly end-to-end methods that eliminate separate preprocessing and postprocessing steps remain scarce. This paper introduces LEADER (Lightweight End-to-end Attention-gated Dual autoencodER), a neural network that maps raw fingerprint images to minutiae descriptors, including location, direction, and type. The proposed architecture integrates non-maximum suppression and angular decoding to enable complete end-to-end inference using only 0.9M parameters. It employs a novel "Castle-Moat-Rampart" ground-truth encoding and a dual-autoencoder structure, interconnected through an attention-gating mechanism. Experimental evaluations demonstrate state-of-the-art accuracy on plain fingerprints and robust cross-domain generalization to latent impressions. Specifically, LEADER attains a 34% higher F1-score on the NIST SD27 dataset compared to specialized latent minutiae extractors. Sample-level analysis on this challenging benchmark reveals an average rank of 2.07 among all compared methods, with LEADER securing the first-place position in 47% of the samples-more than doubling the frequency of the second-best extractor. The internal representations learned by the model align with established fingerprint domain features, such as segmentation masks, orientation fields, frequency maps, and skeletons. Inference requires 15ms on GPU and 322ms on CPU, outperforming leading commercial software in computational efficiency. The source code and pre-trained weights are publicly released to facilitate reproducibility.

</details>


### [23] [Semantic-Guided 3D Gaussian Splatting for Transient Object Removal](https://arxiv.org/abs/2602.15516)
*Aditi Prabakaran,Priyesh Shukla*

Main category: cs.CV

TL;DR: 提出基于视觉语言模型的语义过滤框架，通过CLIP相似度评分和阈值校准来识别并移除3D高斯溅射重建中的瞬态物体，有效解决视差模糊问题，在保持实时渲染性能的同时显著提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统多视图捕捉中的瞬态物体会在3D高斯溅射重建中产生重影伪影，现有解决方案要么依赖高内存成本的场景分解，要么使用容易受视差模糊影响的基于运动的启发式方法。

Method: 使用视觉语言模型构建类别感知的瞬态移除框架：1) 计算渲染视图与干扰文本提示之间的CLIP相似度得分；2) 在训练迭代中为每个高斯累积得分；3) 对超过校准阈值的高斯进行不透明度正则化和周期性剪枝；4) 通过语义分类独立于运动模式识别物体类别。

Result: 在RobustNeRF基准测试的四个序列中，相比原始3DGS方法实现了重建质量的一致提升，同时保持了最小的内存开销和实时渲染性能。阈值校准和基线比较验证了语义引导在可预测干扰类别场景中的有效性。

Conclusion: 语义过滤框架提供了一种实用的瞬态移除策略，通过语义分类有效解决了视差模糊问题，在保持计算效率的同时显著改善了3D高斯溅射的重建质量，特别适用于具有可预测干扰类别的应用场景。

Abstract: Transient objects in casual multi-view captures cause ghosting artifacts in 3D Gaussian Splatting (3DGS) reconstruction. Existing solutions relied on scene decomposition at significant memory cost or on motion-based heuristics that were vulnerable to parallax ambiguity. A semantic filtering framework was proposed for category-aware transient removal using vision-language models. CLIP similarity scores between rendered views and distractor text prompts were accumulated per-Gaussian across training iterations. Gaussians exceeding a calibrated threshold underwent opacity regularization and periodic pruning. Unlike motion-based approaches, semantic classification resolved parallax ambiguity by identifying object categories independently of motion patterns. Experiments on the RobustNeRF benchmark demonstrated consistent improvement in reconstruction quality over vanilla 3DGS across four sequences, while maintaining minimal memory overhead and real-time rendering performance. Threshold calibration and comparisons with baselines validated semantic guidance as a practical strategy for transient removal in scenarios with predictable distractor categories.

</details>


### [24] [Advanced Acceptance Score: A Holistic Measure for Biometric Quantification](https://arxiv.org/abs/2602.15535)
*Aman Verma,Seshan Srirangarajan,Sumantra Dutta Roy*

Main category: cs.CV

TL;DR: 本文提出了一种新的手势生物特征评分评估方法——高级接受分数（advanced acceptance score），通过综合考虑排名顺序、分数相关性、趋势对应性和身份特征解耦等要素，为手势生物特征的质量评估提供了全面的度量标准。


<details>
  <summary>Details</summary>
Motivation: 现有生物特征容量估计文献依赖错误率，但这些指标无法有效评估评分质量的好坏，需要开发更全面的评估方法来量化手势生物特征评分的质量。

Method: 提出基于排名顺序和分数相关性的评估框架，包括：排名偏差分析、高排名手势高分的奖励机制、低排名手势低分的奖励机制、输出分数与真实分数趋势对应性的补偿机制，以及身份特征解耦的折扣因子。通过加权整合这些要素构建高级接受分数。

Result: 在三个数据集上使用五个最先进模型进行深入实验，结果显示基于该度量选择的最优评分比现有其他度量更合适，且与现有度量存在相关性，验证了其可靠性。

Conclusion: 提出的高级接受分数提供了一种全面评估手势生物特征评分质量的有效方法，实验证明其优于现有评估指标，代码已开源供研究社区使用。

Abstract: Quantifying biometric characteristics within hand gestures involve derivation of fitness scores from a gesture and identity aware feature space. However, evaluating the quality of these scores remains an open question. Existing biometric capacity estimation literature relies upon error rates. But these rates do not indicate goodness of scores. Thus, in this manuscript we present an exhaustive set of evaluation measures. We firstly identify ranking order and relevance of output scores as the primary basis for evaluation. In particular, we consider both rank deviation as well as rewards for: (i) higher scores of high ranked gestures and (ii) lower scores of low ranked gestures. We also compensate for correspondence between trends of output and ground truth scores. Finally, we account for disentanglement between identity features of gestures as a discounting factor. Integrating these elements with adequate weighting, we formulate advanced acceptance score as a holistic evaluation measure. To assess effectivity of the proposed we perform in-depth experimentation over three datasets with five state-of-the-art (SOTA) models. Results show that the optimal score selected with our measure is more appropriate than existing other measures. Also, our proposed measure depicts correlation with existing measures. This further validates its reliability. We have made our \href{https://github.com/AmanVerma2307/MeasureSuite}{code} public.

</details>


### [25] [Dynamic Training-Free Fusion of Subject and Style LoRAs](https://arxiv.org/abs/2602.15539)
*Qinglong Cao,Yuntian Chen,Chao Ma,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出无需训练的LoRA动态融合框架，通过特征层KL散度选择和指标引导的潜在调整，实现主题与风格的连贯合成


<details>
  <summary>Details</summary>
Motivation: 现有LoRA融合方法使用静态统计启发式，偏离LoRA自适应特征调整的初衷，且忽略采样输入的随机性

Method: 在前向传播中动态计算基础模型与主题/风格LoRA特征间的KL散度进行权重融合；在反向去噪阶段通过CLIP和DINO等指标进行梯度修正

Result: 在多样化主题-风格组合实验中，方法在定性和定量评估上均优于现有最先进的LoRA融合方法

Conclusion: 通过特征级选择和指标引导的潜在调整双重机制，实现了无需重训练的动态连贯主题-风格合成

Abstract: Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.

</details>


### [26] [Revealing and Enhancing Core Visual Regions: Harnessing Internal Attention Dynamics for Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2602.15556)
*Guangtao Lyu,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Xueting Li,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: PADE是一种无需训练的关注干预方法，通过利用LVLM内部的正向关注动态来识别语义核心视觉区域，减少幻觉现象，提升视觉基础和多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: LVLM在多模态推理方面表现强大但容易产生幻觉输出，现有免训练方法存在计算开销大、可能引入干扰以及易受注意力汇现象影响的问题。

Method: 基于正向关注动态构建PAD图识别语义核心视觉区域，应用每头中位数绝对偏差缩放自适应控制干预强度，利用系统令牌补偿保持对复杂用户指令的关注并支持长期输出一致性。

Result: 在多个LVLM和基准测试上的实验表明，PADE能够改善视觉基础并减少幻觉现象。

Conclusion: 利用内部关注动态进行可靠的干预是提升多模态推理有效性的可行方法，PADE验证了这种方法的有效性。

Abstract: LVLMs have achieved strong multimodal reasoning capabilities but remain prone to hallucinations, producing outputs inconsistent with visual inputs or user instructions. Existing training-free methods, including contrastive decoding and auxiliary expert models, which incur several times more computational overhead and may introduce potential interference, as well as static internal signal enhancement, are often vulnerable to the attention sink phenomenon. We find that internal Positive Attention Dynamics (PAD) in LVLMs naturally reveal semantically core visual regions under the distortions of attention sinks. Based on this, we propose Positive Attention Dynamics Enhancement (PADE), a training-free attention intervention that constructs a PAD map to identify semantically core visual regions, applies per-head Median Absolute Deviation Scaling to adaptively control the intervention strength, and leverages System-Token Compensation to maintain attention to complex user instructions and support long-term output consistency. Experiments on multiple LVLMs and benchmarks show that PADE improves visual grounding and reduces hallucinations, validating the effectiveness of leveraging internal attention dynamics for reliable multimodal reasoning.

</details>


### [27] [Intracoronary Optical Coherence Tomography Image Processing and Vessel Classification Using Machine Learning](https://arxiv.org/abs/2602.15579)
*Amal Lahchim,Lambros Athanasiou*

Main category: cs.CV

TL;DR: 完全自动化的冠状动脉OCT图像血管分割与分类流水线，结合机器学习技术实现高精度像素级分类，准确率达99.68%


<details>
  <summary>Details</summary>
Motivation: 冠状动脉OCT成像存在噪声、伪影和组织结构复杂等问题，需要自动化解决方案来提升血管分析的准确性和效率

Method: 集成图像预处理、导丝伪影去除、极坐标-笛卡尔坐标转换、无监督K-means聚类和局部特征提取，使用逻辑回归和支持向量机进行像素级分类

Result: 实验结果显示优异性能，精确度、召回率和F1分数最高达1.00，总体分类准确率为99.68%

Conclusion: 该方法提供了准确且计算复杂度低的血管边界检测，无需大量人工标注，具有临床决策支持和实时医学图像处理的潜在应用价值

Abstract: Intracoronary Optical Coherence Tomography (OCT) enables high-resolution visualization of coronary vessel anatomy but presents challenges due to noise, imaging artifacts, and complex tissue structures. This paper proposes a fully automated pipeline for vessel segmentation and classification in OCT images using machine learning techniques. The proposed method integrates image preprocessing, guidewire artifact removal, polar-to-Cartesian transformation, unsupervised K-means clustering, and local feature extraction. These features are used to train Logistic Regression and Support Vector Machine classifiers for pixel-wise vessel classification. Experimental results demonstrate excellent performance, achieving precision, recall, and F1-score values up to 1.00 and overall classification accuracy of 99.68%. The proposed approach provides accurate vessel boundary detection while maintaining low computational complexity and requiring minimal manual annotation. This method offers a reliable and efficient solution for automated OCT image analysis and has potential applications in clinical decision support and real-time medical image processing.

</details>


### [28] [An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment](https://arxiv.org/abs/2602.15584)
*Flavien Armangeon,Thibaud Ehret,Enric Meinhardt-Llopis,Rafael Grompone von Gioi,Guillaume Thibault,Marc Petit,Gabriele Facciolo*

Main category: cs.CV

TL;DR: IRIS-v2数据集：用于工业场景2D/3D功能示意图对齐的综合数据集，包含多模态数据并通过分割与图匹配方法减少对齐时间


<details>
  <summary>Details</summary>
Motivation: 老旧工业设施缺乏原生数字模型，现有手动对齐方法（图像和LiDAR）难以规模化应用，且示意图与实际情况存在不一致，工业数据集稀缺

Method: 构建包含图像、点云、2D标注框、分割掩码、CAD模型、3D管道布线信息和P&ID图的综合数据集，采用分割与图匹配相结合的方法进行对齐实验

Result: 开发了IRIS-v2数据集，为工业场景对齐研究提供全面数据支持，通过案例研究验证了方法的可行性

Conclusion: IRIS-v2数据集填补了工业场景对齐研究的数据空白，提出的分割与图匹配方法有望显著减少对齐任务所需时间，推动工业数字孪生发展

Abstract: Aligning functional schematics with 2D and 3D scene acquisitions is crucial for building digital twins, especially for old industrial facilities that lack native digital models. Current manual alignment using images and LiDAR data does not scale due to tediousness and complexity of industrial sites. Inconsistencies between schematics and reality, and the scarcity of public industrial datasets, make the problem both challenging and underexplored. This paper introduces IRIS-v2, a comprehensive dataset to support further research. It includes images, point clouds, 2D annotated boxes and segmentation masks, a CAD model, 3D pipe routing information, and the P&ID (Piping and Instrumentation Diagram). The alignment is experimented on a practical case study, aiming at reducing the time required for this task by combining segmentation and graph matching.

</details>


### [29] [Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation](https://arxiv.org/abs/2602.15650)
*Marco Salmè,Federico Siciliano,Fabrizio Silvestri,Paolo Soda,Rosa Sicilia,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 提出了Concept-Enhanced Multimodal RAG (CEMRAG)框架，通过将视觉表征分解为可解释的临床概念并与多模态检索增强生成相结合，同时提升放射学报告生成的可解释性和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的放射学报告生成方法在可解释性和准确性之间存在权衡，概念解释方法主要关注透明度，而RAG方法主要关注事实基础，缺乏统一解决方案。

Method: CEMRAG框架将视觉表示分解为可解释的临床概念，并与多模态检索增强生成集成，利用丰富的上下文提示来改进放射学报告生成。

Result: 在MIMIC-CXR和IU X-Ray数据集上的实验显示，该方法在临床准确性指标和标准NLP指标上均优于传统RAG和仅概念基线方法。

Conclusion: 该方法挑战了可解释性与性能之间的权衡假设，表明透明的视觉概念可以增强而非损害医学VLM的诊断准确性，为临床可信的AI辅助放射学提供了原则性路径。

Abstract: Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.

</details>


### [30] [A Novel Public Dataset for Strawberry (Fragaria x ananassa) Ripeness Detection and Comparative Evaluation of YOLO-Based Models](https://arxiv.org/abs/2602.15656)
*Mustafa Yurdakul,Zeynep Sena Bastug,Ali Emre Gok,Sakir Taşdemir*

Main category: cs.CV

TL;DR: 本研究提出了一个包含566张图像和1201个标注对象的公开草莓成熟度数据集，在土耳其两个不同温室的可变光照条件下采集。使用YOLOv8、YOLOv9和YOLO11模型进行对比测试，YOLOv8s在mAP@50指标上表现最佳(86.09%)，为智能农业应用建立了基础参考。


<details>
  <summary>Details</summary>
Motivation: 草莓成熟度检测对生产者和消费者都至关重要，但传统视觉评估方法主观性强且误差大。现有文献中缺乏全面公开的数据集，导致该领域研究难以进行比较。

Method: 创建了一个新的公开草莓成熟度数据集，包含566张图像和1201个标注对象，在不同光照和环境条件下采集。使用YOLOv8、YOLOv9和YOLO11等目标检测模型进行性能比较测试。

Result: YOLOv9c模型获得最高精确度(90.94%)，YOLO11s模型获得最高召回率(83.74%)。YOLOv8s在综合性能指标mAP@50上表现最佳，达到86.09%。

Conclusion: 中小型模型在此类数据集上表现更加平衡高效，该研究为智能农业应用提供了重要的基础数据集和性能基准参考。

Abstract: The strawberry (Fragaria x ananassa), known worldwide for its economic value and nutritional richness, is a widely cultivated fruit. Determining the correct ripeness level during the harvest period is crucial for both preventing losses for producers and ensuring consumers receive a quality product. However, traditional methods, i.e., visual assessments alone, can be subjective and have a high margin of error. Therefore, computer-assisted systems are needed. However, the scarcity of comprehensive datasets accessible to everyone in the literature makes it difficult to compare studies in this field. In this study, a new and publicly available strawberry ripeness dataset, consisting of 566 images and 1,201 labeled objects, prepared under variable light and environmental conditions in two different greenhouses in Turkey, is presented to the literature. Comparative tests conducted on the data set using YOLOv8, YOLOv9, and YOLO11-based models showed that the highest precision value was 90.94% in the YOLOv9c model, while the highest recall value was 83.74% in the YOLO11s model. In terms of the general performance criterion mAP@50, YOLOv8s was the best performing model with a success rate of 86.09%. The results show that small and medium-sized models work more balanced and efficiently on this type of dataset, while also establishing a fundamental reference point for smart agriculture applications.

</details>


### [31] [Bayesian Optimization for Design Parameters of 3D Image Data Analysis](https://arxiv.org/abs/2602.15660)
*David Exler,Joaquin Eduardo Urrutia Gómez,Martin Krüger,Maike Schliephake,John Jbeily,Mario Vitacolonna,Rüdiger Rudolf,Markus Reischl*

Main category: cs.CV

TL;DR: 3D数据优化分析流水线：通过两阶段贝叶斯优化自动选择分割和分类模型参数，减少人工调参负担，在四个案例研究中验证有效性


<details>
  <summary>Details</summary>
Motivation: 3D生物医学图像分析中，模型选择和参数调优是主要瓶颈，需要自动化解决方案来减少人工干预

Method: 采用两阶段贝叶斯优化：第一阶段选择分割模型并优化后处理参数，使用领域适应的合成基准数据集和自定义分割质量度量；第二阶段优化分类器设计选择（编码器架构、分类头、先验知识整合和预训练策略），包含辅助类别标注工作流程

Result: 在四个案例研究中，流水线成功为不同数据集找到有效的模型和参数配置

Conclusion: 该方法能够高效自动化3D图像分析流程，显著减少人工标注和参数调优工作量，为生物医学图像分析提供实用解决方案

Abstract: Deep learning-based segmentation and classification are crucial to large-scale biomedical imaging, particularly for 3D data, where manual analysis is impractical. Although many methods exist, selecting suitable models and tuning parameters remains a major bottleneck in practice. Hence, we introduce the 3D data Analysis Optimization Pipeline, a method designed to facilitate the design and parameterization of segmentation and classification using two Bayesian Optimization stages. First, the pipeline selects a segmentation model and optimizes postprocessing parameters using a domain-adapted syntactic benchmark dataset. To ensure a concise evaluation of segmentation performance, we introduce a segmentation quality metric that serves as the objective function. Second, the pipeline optimizes design choices of a classifier, such as encoder and classifier head architectures, incorporation of prior knowledge, and pretraining strategies. To reduce manual annotation effort, this stage includes an assisted class-annotation workflow that extracts predicted instances from the segmentation results and sequentially presents them to the operator, eliminating the need for manual tracking. In four case studies, the 3D data Analysis Optimization Pipeline efficiently identifies effective model and parameter configurations for individual datasets.

</details>


### [32] [Criteria-first, semantics-later: reproducible structure discovery in image-based sciences](https://arxiv.org/abs/2602.15712)
*Jan Bumberger*

Main category: cs.CV

TL;DR: 本文提出从'语义优先'转向'标准优先、语义后置'的图像分析新范式，通过标准驱动的无语义结构提取与下游语义映射分离，解决领域标签在跨传感器、跨站点和长期监测中的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统语义优先的图像分析范式在开放性科学发现、跨传感器/站点可比性以及长期监测中面临领域本体漂移问题，需要更稳定的结构发现方法。

Method: 引入统一的标准优先结构发现框架：1）基于明确最优标准的无语义结构提取；2）下游将结构映射到领域本体或词汇表；3）支持多解释和明确跨映射。

Result: 建立了可重现的分析框架，产生稳定的分区、结构场或层次结构，使结构产品成为FAIR、AI就绪的数字对象，支持长期监测和数字孪生。

Conclusion: 标准优先方法为图像科学提供了领域通用的可重现分析支架，将语义重新定位为下游映射操作，解决了标签不可扩展时的系统性问题，具有重要的理论和实践意义。

Abstract: Across the natural and life sciences, images have become a primary measurement modality, yet the dominant analytic paradigm remains semantics-first. Structure is recovered by predicting or enforcing domain-specific labels. This paradigm fails systematically under the conditions that make image-based science most valuable, including open-ended scientific discovery, cross-sensor and cross-site comparability, and long-term monitoring in which domain ontologies and associated label sets drift culturally, institutionally, and ecologically. A deductive inversion is proposed in the form of criteria-first and semantics-later. A unified framework for criteria-first structure discovery is introduced. It separates criterion-defined, semantics-free structure extraction from downstream semantic mapping into domain ontologies or vocabularies and provides a domain-general scaffold for reproducible analysis across image-based sciences. Reproducible science requires that the first analytic layer perform criterion-driven, semantics-free structure discovery, yielding stable partitions, structural fields, or hierarchies defined by explicit optimality criteria rather than local domain ontologies. Semantics is not discarded; it is relocated downstream as an explicit mapping from the discovered structural product to a domain ontology or vocabulary, enabling plural interpretations and explicit crosswalks without rewriting upstream extraction. Grounded in cybernetics, observation-as-distinction, and information theory's separation of information from meaning, the argument is supported by cross-domain evidence showing that criteria-first components recur whenever labels do not scale. Finally, consequences are outlined for validation beyond class accuracy and for treating structural products as FAIR, AI-ready digital objects for long-term monitoring and digital twins.

</details>


### [33] [ToaSt: Token Channel Selection and Structured Pruning for Efficient ViT](https://arxiv.org/abs/2602.15720)
*Hyunchan Moon,Cheonjun Park,Steven L. Waslander*

Main category: cs.CV

TL;DR: ToaSt是一种解耦的ViT压缩框架，通过头级结构化剪枝和令牌通道选择分别处理多头自注意力和前馈网络，在保持精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers部署受限于高昂计算成本，现有结构化权重剪枝和令牌压缩方法分别面临重训练时间长和全局传播优化挑战的问题。

Method: 采用解耦策略：对多头自注意力模块使用耦合头级结构化剪枝，利用注意力操作特性增强鲁棒性；对前馈网络（占FLOPs 60%以上）引入令牌通道选择(TCS)，避免全局传播问题并提升压缩比。

Result: 在9个模型上验证，包括DeiT、ViT-MAE和Swin Transformer。ViT-MAE-Huge达到88.52%精度（+1.64%）且FLOPs减少39.4%。下游任务迁移效果良好，COCO目标检测达到52.2 mAP。

Conclusion: ToaSt框架通过组件专用压缩策略实现了精度与效率的优越权衡，有效解决了ViT部署中的计算成本问题。

Abstract: Vision Transformers (ViTs) have achieved remarkable success across various vision tasks, yet their deployment is often hindered by prohibitive computational costs. While structured weight pruning and token compression have emerged as promising solutions, they suffer from prolonged retraining times and global propagation that creates optimization challenges, respectively. We propose ToaSt, a decoupled framework applying specialized strategies to distinct ViT components. We apply coupled head-wise structured pruning to Multi-Head Self-Attention modules, leveraging attention operation characteristics to enhance robustness. For Feed-Forward Networks (over 60\% of FLOPs), we introduce Token Channel Selection (TCS) that enhances compression ratios while avoiding global propagation issues. Our analysis reveals TCS effectively filters redundant noise during selection. Extensive evaluations across nine diverse models, including DeiT, ViT-MAE, and Swin Transformer, demonstrate that ToaSt achieves superior trade-offs between accuracy and efficiency, consistently outperforming existing baselines. On ViT-MAE-Huge, ToaSt achieves 88.52\% accuracy (+1.64 \%) with 39.4\% FLOPs reduction. ToaSt transfers effectively to downstream tasks, cccccachieving 52.2 versus 51.9 mAP on COCO object detection. Code and models will be released upon acceptance.

</details>


### [34] [Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation](https://arxiv.org/abs/2602.15724)
*Shutian Gu,Chengkai Huang,Ruoyu Wang,Lina Yao*

Main category: cs.CV

TL;DR: 提出基于检索增强的两级框架提升LLM在视觉语言导航中的效率和稳定性，通过指令级嵌入检索和模仿学习候选剪枝，无需修改或微调底层语言模型。


<details>
  <summary>Details</summary>
Motivation: 基于提示的LLM导航存在决策效率低下的问题，需要重复解释指令并在每一步推理嘈杂冗长的导航候选，影响导航性能。

Method: 采用两级检索机制：1) 指令级嵌入检索器选择语义相似的导航轨迹作为上下文示例；2) 模仿学习的候选检索器在LLM推理前剪枝无关导航方向。两个模块均为轻量级且独立训练。

Result: 在Room-to-Room基准测试中，在seen和unseen环境下的Success Rate、Oracle Success Rate和SPL指标均获得一致提升。

Conclusion: 检索增强的决策支持是提升LLM视觉语言导航效果的有效且可扩展策略，指令级示例检索和候选剪枝在全局指导和逐步决策效率方面提供互补优势。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.

</details>


### [35] [Spanning the Visual Analogy Space with a Weight Basis of LoRAs](https://arxiv.org/abs/2602.15727)
*Hila Manor,Rinon Gal,Haggai Maron,Tomer Michaeli,Gal Chechik*

Main category: cs.CV

TL;DR: LoRWeB提出了一种通过动态组合学习到的变换基元来解决视觉类比学习任务的新方法，使用LoRA基分解和轻量级编码器实现推理时的动态适配，显著提升了未见视觉变换的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用单一LoRA模块来捕捉多样化的视觉变换空间，这种固定适配模块限制了泛化能力。受限于LoRA在约束域中能够跨越有意义、可插值语义空间的启发，需要开发能够为每个类比任务动态选择变换基元的方法。

Method: 提出LoRWeB方法，包含两个核心组件：(1) 可学习的LoRA模块基，用于跨越不同视觉变换空间；(2) 轻量级编码器，根据输入类比对动态选择和加权这些基LoRA模块，实现推理时的动态组合。

Result: 综合评估表明该方法实现了最先进的性能，并显著提高了对未见视觉变换的泛化能力。

Conclusion: LoRA基分解是灵活视觉操纵的一个有前景方向，通过动态组合学习到的变换基元能够有效解决视觉类比学习中的泛化问题。

Abstract: Visual analogy learning enables image manipulation through demonstration rather than textual description, allowing users to specify complex transformations difficult to articulate in words. Given a triplet $\{\mathbf{a}$, $\mathbf{a}'$, $\mathbf{b}\}$, the goal is to generate $\mathbf{b}'$ such that $\mathbf{a} : \mathbf{a}' :: \mathbf{b} : \mathbf{b}'$. Recent methods adapt text-to-image models to this task using a single Low-Rank Adaptation (LoRA) module, but they face a fundamental limitation: attempting to capture the diverse space of visual transformations within a fixed adaptation module constrains generalization capabilities. Inspired by recent work showing that LoRAs in constrained domains span meaningful, interpolatable semantic spaces, we propose LoRWeB, a novel approach that specializes the model for each analogy task at inference time through dynamic composition of learned transformation primitives, informally, choosing a point in a "space of LoRAs". We introduce two key components: (1) a learnable basis of LoRA modules, to span the space of different visual transformations, and (2) a lightweight encoder that dynamically selects and weighs these basis LoRAs based on the input analogy pair. Comprehensive evaluations demonstrate our approach achieves state-of-the-art performance and significantly improves generalization to unseen visual transformations. Our findings suggest that LoRA basis decompositions are a promising direction for flexible visual manipulation. Code and data are in https://research.nvidia.com/labs/par/lorweb

</details>


### [36] [Language and Geometry Grounded Sparse Voxel Representations for Holistic Scene Understanding](https://arxiv.org/abs/2602.15734)
*Guile Wu,David Huang,Bingbing Liu,Dongfeng Bai*

Main category: cs.CV

TL;DR: 提出了一种基于语言和几何约束的稀疏体素表示方法，在统一框架中协同建模3D场景的外观、语义和几何信息，通过特征调制模块和几何蒸馏技术实现整体性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有3D开放词汇场景理解方法主要关注从2D基础模型提取语言特征到3D特征场，但忽视了场景外观、语义和几何之间的协同作用，导致场景理解与底层几何结构脱节。

Method: 使用3D稀疏体素作为基元，构建外观场、密度场、特征场和置信场；通过特征调制模块促进各场之间的协同；从2D基础模型蒸馏语言特征；通过深度相关正则化和模式一致性正则化进行几何蒸馏。

Result: 在整体场景理解和重建任务中，相比最先进方法实现了优越的整体性能。

Conclusion: 该方法成功实现了外观、语义和几何在统一框架中的协同建模，验证了语言和几何约束的稀疏体素表示在3D场景理解中的有效性。

Abstract: Existing 3D open-vocabulary scene understanding methods mostly emphasize distilling language features from 2D foundation models into 3D feature fields, but largely overlook the synergy among scene appearance, semantics, and geometry. As a result, scene understanding often deviates from the underlying geometric structure of scenes and becomes decoupled from the reconstruction process. In this work, we propose a novel approach that leverages language and geometry grounded sparse voxel representations to comprehensively model appearance, semantics, and geometry within a unified framework. Specifically, we use 3D sparse voxels as primitives and employ an appearance field, a density field, a feature field, and a confidence field to holistically represent a 3D scene. To promote synergy among the appearance, density, and feature fields, we construct a feature modulation module and distill language features from a 2D foundation model into our 3D scene model. In addition, we integrate geometric distillation into feature field distillation to transfer geometric knowledge from a geometry foundation model to our 3D scene representations via depth correlation regularization and pattern consistency regularization. These components work together to synergistically model the appearance, semantics, and geometry of the 3D scene within a unified framework. Extensive experiments demonstrate that our approach achieves superior overall performance compared with state-of-the-art methods in holistic scene understanding and reconstruction.

</details>


### [37] [RaCo: Ranking and Covariance for Practical Learned Keypoints](https://arxiv.org/abs/2602.15755)
*Abhiram Shenoi,Philipp Lindenberger,Paul-Edouard Sarlin,Marc Pollefeys*

Main category: cs.CV

TL;DR: RaCo是一种轻量级神经网络，通过集成可重复关键点检测器、可微分排序器和协方差估计器，无需共视图像对即可学习适用于多种3D视觉任务的鲁棒关键点，在旋转鲁棒性和匹配性能方面达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决传统关键点检测方法在旋转鲁棒性、匹配性能和空间不确定性量化方面的局限性，提供一种无需共视图像对和复杂等变网络架构的轻量级解决方案。

Method: 1. 可重复关键点检测器；2. 可微分排序器（限制关键点数量下最大化匹配）；3. 协方差估计器（量化度量尺度空间不确定性）；仅使用透视图像裁剪训练，通过大量数据增强实现旋转鲁棒性。

Result: 在多个挑战性数据集上实现最先进的性能，特别是在关键点可重复性和两视图匹配方面，对大平面内旋转表现出色。

Conclusion: RaCo提供了一种有效且简单的策略，无需额外标签即可独立估计关键点排序和度量协方差，检测可解释且可重复的兴趣点。

Abstract: This paper introduces RaCo, a lightweight neural network designed to learn robust and versatile keypoints suitable for a variety of 3D computer vision tasks. The model integrates three key components: the repeatable keypoint detector, a differentiable ranker to maximize matches with a limited number of keypoints, and a covariance estimator to quantify spatial uncertainty in metric scale. Trained on perspective image crops only, RaCo operates without the need for covisible image pairs. It achieves strong rotational robustness through extensive data augmentation, even without the use of computationally expensive equivariant network architectures. The method is evaluated on several challenging datasets, where it demonstrates state-of-the-art performance in keypoint repeatability and two-view matching, particularly under large in-plane rotations. Ultimately, RaCo provides an effective and simple strategy to independently estimate keypoint ranking and metric covariance without additional labels, detecting interpretable and repeatable interest points. The code is available at https://github.com/cvg/RaCo.

</details>


### [38] [Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models](https://arxiv.org/abs/2602.15772)
*Sen Ye,Mengde Xu,Shuyang Gu,Di He,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: R3框架通过将单步生成任务重构为'生成-理解-再生成'的多步过程，解决了多模态模型中生成能力与理解能力的权衡问题，实现了两者性能的共同提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型面临生成能力增强往往以牺牲理解能力为代价的挑战，研究发现生成与理解之间存在竞争性冲突，需要新的方法来平衡这种优化困境。

Method: 提出Reason-Reflect-Refine (R3)框架，将传统的单步生成过程分解为多步迭代过程，在生成过程中显式利用模型的理解能力进行反思和精炼。

Result: 成功缓解了优化困境，在保持生成能力的同时显著提升了与生成过程相关的理解能力，取得了更强的生成效果。

Conclusion: R3框架为设计下一代统一多模态模型提供了有价值的见解，证明了通过多步生成-理解循环可以有效协调生成与理解能力的协同发展。

Abstract: Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of "generate-understand-regenerate". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.

</details>


### [39] [NeRFscopy: Neural Radiance Fields for in-vivo Time-Varying Tissues from Endoscopy](https://arxiv.org/abs/2602.15775)
*Laura Salort-Benejam,Antonio Agudo*

Main category: cs.CV

TL;DR: NeRFscopy是一种基于神经渲染的自监督管道，用于从单目内窥镜视频中实现可变形组织的新视角合成和3D重建，通过SE(3)变换参数化的变形场和规范辐射场实现，无需模板或预训练模型。


<details>
  <summary>Details</summary>
Motivation: 内窥镜在医学成像中至关重要，但组织可变形、单目相机、光照变化、遮挡和未知相机轨迹等挑战阻碍了动态3D重建。开发稳健的3D重建管道可增强可视化、提高诊断准确性并辅助手术规划。

Method: 提出NeRFscopy管道，包含规范辐射场和基于SE(3)变换的时间相关变形场。引入复杂项有效利用彩色图像，仅从数据学习3D隐式模型，无需模板或预训练模型。

Result: NeRFscopy在新视角合成方面取得准确结果，在各种具有挑战性的内窥镜场景中优于竞争方法。

Conclusion: 该方法为内窥镜视频的3D重建提供了一种有效的自监督解决方案，能够处理组织变形等挑战，在医学成像应用中具有重要价值。

Abstract: Endoscopy is essential in medical imaging, used for diagnosis, prognosis and treatment. Developing a robust dynamic 3D reconstruction pipeline for endoscopic videos could enhance visualization, improve diagnostic accuracy, aid in treatment planning, and guide surgery procedures. However, challenges arise due to the deformable nature of the tissues, the use of monocular cameras, illumination changes, occlusions and unknown camera trajectories. Inspired by neural rendering, we introduce NeRFscopy, a self-supervised pipeline for novel view synthesis and 3D reconstruction of deformable endoscopic tissues from a monocular video. NeRFscopy includes a deformable model with a canonical radiance field and a time-dependent deformation field parameterized by SE(3) transformations. In addition, the color images are efficiently exploited by introducing sophisticated terms to learn a 3D implicit model without assuming any template or pre-trained model, solely from data. NeRFscopy achieves accurate results in terms of novel view synthesis, outperforming competing methods across various challenging endoscopy scenes.

</details>


### [40] [Meteorological data and Sky Images meets Neural Models for Photovoltaic Power Forecasting](https://arxiv.org/abs/2602.15782)
*Ines Montoya-Espinagosa,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出基于天空图像、光伏历史数据和气象数据的多模态混合方法，用于短期和长期光伏预测，特别关注多云条件下的爬坡事件预测和预测鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 应对光伏能源生产的可变性挑战，提高爬坡事件预测准确性，增强多云条件下的预测鲁棒性，支持电网高效运行和太阳能变异性管理。

Method: 使用深度神经网络模型，结合天空图像、光伏历史数据、多种气象变量（包括地表长波辐射、向下辐射、风速等）和太阳位置分析，开发多模态混合预测方法。

Result: 气象数据的加入显著改善了当前和未来预测的准确性，特别是在多云天气条件下，地表长波辐射、向下辐射以及风速与太阳位置的组合效果尤为明显。

Conclusion: 整合多样化数据源对于提高太阳能预测模型的可靠性和可解释性至关重要，多模态方法在光伏预测中展现出显著优势。

Abstract: Due to the rise in the use of renewable energies as an alternative to traditional ones, and especially solar energy, there is increasing interest in studying how to address photovoltaic forecasting in the face of the challenge of variability in photovoltaic energy production, using different methodologies. This work develops a hybrid approach for short and long-term forecasting based on two studies with the same purpose. A multimodal approach that combines images of the sky and photovoltaic energy history with meteorological data is proposed. The main goal is to improve the accuracy of ramp event prediction, increase the robustness of forecasts in cloudy conditions, and extend capabilities beyond nowcasting, to support more efficient operation of the power grid and better management of solar variability. Deep neural models are used for both nowcasting and forecasting solutions, incorporating individual and multiple meteorological variables, as well as an analytical solar position. The results demonstrate that the inclusion of meteorological data, particularly the surface long-wave, radiation downwards, and the combination of wind and solar position, significantly improves current predictions in both nowcasting and forecasting tasks, especially on cloudy days. This study highlights the importance of integrating diverse data sources to improve the reliability and interpretability of solar energy prediction models.

</details>


### [41] [Context-aware Skin Cancer Epithelial Cell Classification with Scalable Graph Transformers](https://arxiv.org/abs/2602.15783)
*Lucas Sancéré,Noémie Moreau,Katarzyna Bozek*

Main category: cs.CV

TL;DR: 该研究提出使用可扩展的图变换器在全WSI细胞图上进行皮肤鳞状细胞癌(cSCC)中健康与肿瘤上皮细胞的分类，相比传统图像方法显著提升了分类准确率，并证明了细胞周围环境信息的重要性。


<details>
  <summary>Details</summary>
Motivation: 全切片图像(WSIs)包含丰富的医学诊断信息，但现有深度学习方法依赖基于补丁的表征，丢失了重要的组织层面上下文信息。特别是在cSCC中，健康与肿瘤上皮细胞形态相似，图像方法难以区分。

Method: 采用可扩展图变换器(SGFormer和DIFFormer)在全WSI细胞图上进行分类，比较了多种节点特征配置，包括形态学、纹理特征以及非上皮细胞的细胞类别信息。

Result: 在单WSI上，图变换器方法达到85.2%和85.1%的平衡准确率，优于图像方法的81.2%。在多WSI设置下，DIFFormer达到83.6%的准确率，优于CellViT256的78.1%。

Conclusion: 图变换器方法在细胞分类任务中显著优于传统图像方法，证明了利用细胞间上下文信息的重要性，特别是在形态相似细胞的区分任务中表现出色。

Abstract: Whole-slide images (WSIs) from cancer patients contain rich information that can be used for medical diagnosis or to follow treatment progress. To automate their analysis, numerous deep learning methods based on convolutional neural networks and Vision Transformers have been developed and have achieved strong performance in segmentation and classification tasks. However, due to the large size and complex cellular organization of WSIs, these models rely on patch-based representations, losing vital tissue-level context. We propose using scalable Graph Transformers on a full-WSI cell graph for classification. We evaluate this methodology on a challenging task: the classification of healthy versus tumor epithelial cells in cutaneous squamous cell carcinoma (cSCC), where both cell types exhibit very similar morphologies and are therefore difficult to differentiate for image-based approaches. We first compared image-based and graph-based methods on a single WSI. Graph Transformer models SGFormer and DIFFormer achieved balanced accuracies of $85.2 \pm 1.5$ ($\pm$ standard error) and $85.1 \pm 2.5$ in 3-fold cross-validation, respectively, whereas the best image-based method reached $81.2 \pm 3.0$. By evaluating several node feature configurations, we found that the most informative representation combined morphological and texture features as well as the cell classes of non-epithelial cells, highlighting the importance of the surrounding cellular context. We then extended our work to train on several WSIs from several patients. To address the computational constraints of image-based models, we extracted four $2560 \times 2560$ pixel patches from each image and converted them into graphs. In this setting, DIFFormer achieved a balanced accuracy of $83.6 \pm 1.9$ (3-fold cross-validation), while the state-of-the-art image-based model CellViT256 reached $78.1 \pm 0.5$.

</details>


### [42] [Task-Agnostic Continual Learning for Chest Radiograph Classification](https://arxiv.org/abs/2602.15811)
*Muthu Subash Kavitha,Anas Zafar,Amgad Muneer,Jia Wu*

Main category: cs.CV

TL;DR: CARL-XRay提出了一种基于适配器的持续学习方法，用于胸片分类的连续学习场景，通过固定主干网络并增量添加轻量级任务特定适配器和分类头，支持无需原始图像存储的稳定任务识别和适应。


<details>
  <summary>Details</summary>
Motivation: 临床胸片分类器部署需要能够在新数据集可用时更新模型，而无需重新训练先前观察到的数据或降低已验证性能。研究在任务标识符在推理时不可用的异构胸片数据集顺序到达的场景下的持续学习问题。

Method: 采用基于适配器的路由学习策略，维护固定高容量主干网络，增量分配轻量级任务特定适配器和分类器头。潜在任务选择器基于任务适应特征操作，利用通过紧凑原型和特征级经验重放保留的当前和历史上下文。

Result: 在大规模公共胸片数据集上表现出稳健的性能保持和可靠的任务感知推理。在任务未知部署下优于联合训练，路由准确率达到75.0%（vs. 62.5%），在oracle设置下AUROC为0.74，任务未知推理下为0.75，使用显著更少的可训练参数。

Conclusion: 该框架为临床持续部署提供了联合训练和重复完全重新训练的实用替代方案，支持稳定的任务识别和适应，同时避免原始图像存储。

Abstract: Clinical deployment of chest radiograph classifiers requires models that can be updated as new datasets become available without retraining on previously ob- served data or degrading validated performance. We study, for the first time, a task-incremental continual learning setting for chest radiograph classification, in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers are unavailable at inference. We propose a continual adapter-based routing learning strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone and incrementally allocates lightweight task-specific adapters and classifier heads. A latent task selector operates on task-adapted features and leverages both current and historical context preserved through compact prototypes and feature-level experience replay. This design supports stable task identification and adaptation across sequential updates while avoiding raw-image storage. Experiments on large-scale public chest radiograph datasets demonstrate robust performance retention and reliable task-aware inference under continual dataset ingestion. CARL-XRay outperforms joint training under task-unknown deployment, achieving higher routing accuracy (75.0\% vs.\ 62.5\%), while maintaining competitive diagnostic performance with AUROC of 0.74 in the oracle setting with ground-truth task identity and 0.75 under task-unknown inference, using significantly fewer trainable parameters. Finally, the proposed framework provides a practical alternative to joint training and repeated full retraining in continual clinical deployment.

</details>


### [43] [VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation](https://arxiv.org/abs/2602.15819)
*Hui Ren,Yuval Alaluf,Omer Bar Tal,Alexander Schwing,Antonio Torralba,Yael Vinker*

Main category: cs.CV

TL;DR: 提出一种基于文本到视频扩散模型的数据高效序列草图生成方法，通过两阶段微调策略分离笔画顺序学习和视觉外观学习，仅需少量手绘数据即可生成高质量时序草图。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型将草图视为静态图像，忽略了绘制过程中的时序结构和创作顺序，而草图创作本质上是探索和精炼想法的序列过程。

Method: 利用LLM进行语义规划和笔画排序，视频扩散模型作为渲染器生成时序一致的视觉效果。采用两阶段微调：使用合成形状组合学习笔画顺序，仅需7个手绘过程学习视觉外观。

Result: 尽管使用极少量人类手绘数据，方法仍能生成高质量序列草图，准确遵循文本指定的顺序并展现丰富视觉细节。支持笔刷风格条件和自回归生成等扩展功能。

Conclusion: 该方法成功将预训练文本到视频扩散模型适配于序列草图生成，通过互补利用LLM和视频模型的优势，实现了数据高效且高质量的时序草图生成，为交互式协作绘图提供了新可能。

Abstract: Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [44] [EduResearchBench: A Hierarchical Atomic Task Decomposition Benchmark for Full-Lifecycle Educational Research](https://arxiv.org/abs/2602.15034)
*Houping Yue,Zixiang Di,Mei Jiang,Bingdong Li,Hao Hao,Yu Song,Bo Jiang,Aimin Zhou*

Main category: cs.CL

TL;DR: EduResearchBench是首个专门评估教育学术写作的综合平台，通过分层原子任务分解框架将研究流程分解为6个研究模块和24个原子任务，提供细粒度评估。基于此开发了EduWrite模型，在数据质量密度和分层训练课程方面优于更大规模的通用模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在社会科学AI应用中重塑范式，但现有基准测试主要关注单次生成任务，缺乏对复杂学术研究工作流程的细粒度评估能力，无法准确反映模型在学术写作中的具体能力瓶颈。

Method: 提出分层原子任务分解(HATD)框架，将端到端研究流程分解为6个专业研究模块(定量分析、定性研究、政策研究等)和24个细粒度原子任务。采用课程学习策略，从基础技能逐步构建到复杂方法推理和论证能力。基于55K原始学术样本构建11K高质量指令对训练EduWrite专业模型。

Result: EduWrite(30B参数)在多项核心指标上显著优于更大规模(72B)的通用模型，证明在垂直领域中，数据质量密度和分层训练课程比参数规模更具决定性。

Conclusion: 该研究展示了通过细粒度任务分解和课程学习方法，可以在相对较小的参数规模下实现专业领域学术写作的卓越性能，为AI4SS领域的模型评估和专业模型开发提供了新的方法论和基准平台。

Abstract: While Large Language Models (LLMs) are reshaping the paradigm of AI for Social Science (AI4SS), rigorously evaluating their capabilities in scholarly writing remains a major challenge. Existing benchmarks largely emphasize single-shot, monolithic generation and thus lack the fine-grained assessments required to reflect complex academic research workflows. To fill this gap, we introduce EduResearchBench, the first comprehensive evaluation platform dedicated to educational academic writing. EduResearchBench is built upon our Hierarchical Atomic Task Decomposition (HATD) framework, which decomposes an end-to-end research workflow into six specialized research modules (e.g., Quantitative Analysis, Qualitative Research, and Policy Research) spanning 24 fine-grained atomic tasks. This taxonomy enables an automated evaluation pipeline that mitigates a key limitation of holistic scoring, where aggregate scores often obscure specific capability bottlenecks, and instead provides fine-grained, diagnostic feedback on concrete deficiencies. Moreover, recognizing the high cognitive load inherent in scholarly writing, we propose a curriculum learning strategy that progressively builds competence from foundational skills to complex methodological reasoning and argumentation. Leveraging 55K raw academic samples, we curate 11K high-quality instruction pairs to train EduWrite, a specialized educational scholarly writing model. Experiments show that EduWrite (30B) substantially outperforms larger general-purpose models (72B) on multiple core metrics, demonstrating that in vertical domains, data quality density and hierarchically staged training curricula are more decisive than parameter scale.

</details>


### [45] [Indic-TunedLens: Interpreting Multilingual Models in Indian Languages](https://arxiv.org/abs/2602.15038)
*Mihir Panchal,Deeksha Varshney,Mamta,Asif Ekbal*

Main category: cs.CL

TL;DR: Indic-TunedLens：针对印度语言的可解释性框架，通过学习共享仿射变换改进多语言LLM的跨语言表示解码，在10种印度语言上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在印度等语言多样化地区部署增加，但现有可解释性工具主要针对英语，LLM存在英语中心化表示空间问题，需要解决跨语言可解释性挑战

Method: 提出Indic-TunedLens框架，为每种目标语言学习共享仿射变换，调整隐藏状态以与目标输出分布对齐，相比标准Logit Lens能更忠实地解码模型表示

Result: 在MMLU基准测试的10种印度语言上显著优于最先进的可解释性方法，特别在形态丰富、低资源语言上表现突出

Conclusion: 该框架为多语言变换器的分层语义编码提供了重要见解，解决了多语言LLM在非英语语言上的可解释性难题

Abstract: Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretability a pressing concern. We introduce Indic-TunedLens, a novel interpretability framework specifically for Indian languages that learns shared affine transformations. Unlike the standard Logit Lens, which directly decodes intermediate activations, Indic-TunedLens adjusts hidden states for each target language, aligning them with the target output distributions to enable more faithful decoding of model representations. We evaluate our framework on 10 Indian languages using the MMLU benchmark and find that it significantly improves over SOTA interpretability methods, especially for morphologically rich, low resource languages. Our results provide crucial insights into the layer-wise semantic encoding of multilingual transformers. Our model is available at https://huggingface.co/spaces/AnonymousAccountACL/IndicTunedLens. Our code is available at https://github.com/AnonymousAccountACL/IndicTunedLens.

</details>


### [46] [CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding](https://arxiv.org/abs/2602.15139)
*Tahir Hussain,Saddam Hussain Khan*

Main category: cs.CL

TL;DR: 提出了CGRA DeBERTa模型，一个基于概念引导残差域增强的transformer框架，专门用于伊斯兰圣训文本的问答任务，在EM分数上达到97.85，显著优于BERT和DeBERTa基线模型。


<details>
  <summary>Details</summary>
Motivation: 伊斯兰经典文本的问答面临领域特定语义、长上下文依赖和概念敏感推理的挑战，需要专门的方法来提高神学问答的准确性。

Method: 基于定制化的DeBERTa transformer架构，结合轻量级LoRA适配和残差概念感知门控机制。通过伊斯兰概念词典引入神学先验知识，使用重要性加权注意力选择性增强关键语义标记。

Result: 在42,591个QA对的数据集上，CGRA DeBERTa取得97.85的EM分数，相比BERT(75.87)和DeBERTa(89.77)有显著提升，同时仅增加约8%的推理开销。

Conclusion: 该研究提出了高效、可解释且准确的圣训问答系统，能够为教育材料提供必要的神学细微差别，在保持计算效率的同时显著提升了领域特定问答的性能。

Abstract: Accurate QA over classical Islamic texts remains challenging due to domain specific semantics, long context dependencies, and concept sensitive reasoning. Therefore, a new CGRA DeBERTa, a concept guided residual domain augmentation transformer framework, is proposed that enhances theological QA over Hadith corpora. The CGRA DeBERTa builds on a customized DeBERTa transformer backbone with lightweight LoRA based adaptations and a residual concept aware gating mechanism. The customized DeBERTa embedding block learns global and positional context, while Concept Guided Residual Blocks incorporate theological priors from a curated Islamic Concept Dictionary of 12 core terms. Moreover, the Concept Gating Mechanism selectively amplifies semantically critical tokens via importance weighted attention, applying differential scaling from 1.04 to 3.00. This design preserves contextual integrity, strengthens domain-specific semantic representations, and enables accurate, efficient span extraction while maintaining computational efficiency. This paper reports the results of training CGRA using a specially constructed dataset of 42591 QA pairs from the text of Sahih alBukhari and Sahih Muslim. While BERT achieved an EM score of 75.87 and DeBERTa one of 89.77, our model scored 97.85 and thus surpassed them by 8.08 on an absolute scale, all while adding approximately 8 inference overhead due to parameter efficient gating. The qualitative evaluation noted better extraction and discrimination and theological precision. This study presents Hadith QA systems that are efficient, interpretable, and accurate and that scale provide educational materials with necessary theological nuance.

</details>


### [47] [AIC CTU@AVerImaTeC: dual-retriever RAG for image-text fact checking](https://arxiv.org/abs/2602.15190)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 本文介绍了AVerImaTeC共享任务中排名第三的系统，该系统将去年的检索增强生成(RAG)管道与反向图像搜索(RIS)模块相结合，使用GPT5.1以单次多模态LLM调用实现每次事实核查仅需$0.013的成本。


<details>
  <summary>Details</summary>
Motivation: 开发一个简单但性能竞争的事实核查系统，作为进一步实验的可访问起点，同时保持低成本和易复现性。

Method: 系统包含三个解耦模块：基于相似性搜索的文本检索模块、基于API访问的反向图像搜索模块、以及使用GPT5.1的生成模块。

Result: 在AVerImaTeC共享任务中获得第三名，每次事实核查平均成本仅为$0.013，具有竞争性性能。

Conclusion: 该系统提供了一个简单、经济高效且易于复现的事实核查解决方案，适合作为进一步研究的基础，作者已公开代码、提示词和向量存储库。

Abstract: In this paper, we present our 3rd place system in the AVerImaTeC shared task, which combines our last year's retrieval-augmented generation (RAG) pipeline with a reverse image search (RIS) module. Despite its simplicity, our system delivers competitive performance with a single multimodal LLM call per fact-check at just $0.013 on average using GPT5.1 via OpenAI Batch API. Our system is also easy to reproduce and tweak, consisting of only three decoupled modules - a textual retrieval module based on similarity search, an image retrieval module based on API-accessed RIS, and a generation module using GPT5.1 - which is why we suggest it as an accesible starting point for further experimentation. We publish its code and prompts, as well as our vector stores and insights into the scheme's running costs and directions for further improvement.

</details>


### [48] [OpaqueToolsBench: Learning Nuances of Tool Behavior Through Interaction](https://arxiv.org/abs/2602.15197)
*Skyler Hallinan,Thejas Venkatesh,Xiang Ren,Sai Praneeth Karimireddy,Ashwin Paranjape,Yuhao Zhang,Jack Hessel*

Main category: cs.CL

TL;DR: 提出了OpaqueToolsBench基准测试，用于评估LLM在不透明工具环境中的表现，并开发了ToolObserver框架通过迭代观察执行反馈来改进工具文档，显著优于现有方法且更高效。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的工具往往缺乏清晰文档和最佳实践，现有基准假设工具文档完美，无法反映真实场景中LLM代理处理不透明工具的挑战。

Method: 创建包含三个任务环境的OpaqueToolsBench基准，提出ToolObserver框架，通过迭代观察工具调用轨迹的执行反馈来改进工具文档。

Result: 现有自动文档方法在不透明工具环境下昂贵且不可靠，ToolObserver在所有数据集上表现优于基线方法，在测试时工具探索场景中token消耗减少3.5-7.5倍。

Conclusion: ToolObserver框架有效解决了LLM在不透明工具环境中的性能问题，通过迭代文档改进实现了更好的工具使用效果和更高的效率。

Abstract: Tool-calling is essential for Large Language Model (LLM) agents to complete real-world tasks. While most existing benchmarks assume simple, perfectly documented tools, real-world tools (e.g., general "search" APIs) are often opaque, lacking clear best practices or failure modes. Can LLM agents improve their performance in environments with opaque tools by interacting and subsequently improving documentation? To study this, we create OpaqueToolsBench, a benchmark consisting of three distinct task-oriented environments: general function calling, interactive chess playing, and long-trajectory agentic search. Each environment provides underspecified tools that models must learn to use effectively to complete the task. Results on OpaqueToolsBench suggest existing methods for automatically documenting tools are expensive and unreliable when tools are opaque. To address this, we propose a simple framework, ToolObserver, that iteratively refines tool documentation by observing execution feedback from tool-calling trajectories. Our approach outperforms existing methods on OpaqueToolsBench across datasets, even in relatively hard settings. Furthermore, for test-time tool exploration settings, our method is also efficient, consuming 3.5-7.5x fewer total tokens than the best baseline.

</details>


### [49] [Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement](https://arxiv.org/abs/2602.15312)
*Stephan Ludwig,Peter J. Danaher,Xiaohao Yang,Yu-Ting Lin,Ehsan Abedin,Dhruv Grewal,Lan Du*

Main category: cs.CL

TL;DR: 本研究开发了语言提取器(LX)，这是一个基于消费者文本和自评情感标签微调的大语言模型，在情感和评价检测方面优于GPT-4 Turbo等主流模型，准确率达81-95%，并通过实际应用验证了情感对购买行为的影响。


<details>
  <summary>Details</summary>
Motivation: 准确测量消费者从非结构化文本中表达的情感和评价是营销研究的核心挑战，现有模型在消费者情感检测方面存在局限。

Method: 使用消费者自评的16种消费相关情感和4种评价构念标签，对大型语言模型进行微调，开发LX模型，并采用看似无关回归分析验证模型应用效果。

Result: LX在开放式调查回复中达到81%的宏F1准确率，在第三方标注的亚马逊和Yelp评论中准确率超过95%，情感表达通过产品评分中介影响购买行为，部分情感如不满和平静直接影响购买决策。

Conclusion: LX为消费者感知测量建立了新的方法论基础，证明大语言模型可有效推进营销研究，实现了从消费者数据中验证营销构念检测，并提供免费无代码网络应用支持规模化分析。

Abstract: Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.

</details>


### [50] [Mnemis: Dual-Route Retrieval on Hierarchical Graphs for Long-Term LLM Memory](https://arxiv.org/abs/2602.15313)
*Zihao Tang,Xin Yu,Ziyu Xiao,Zengxuan Wen,Zelin Li,Jiaxi Zhou,Hualei Wang,Haohua Wang,Haizhen Huang,Weiwei Deng,Feng Sun,Qi Zhang*

Main category: cs.CL

TL;DR: Mnemis是一个新颖的记忆框架，将基于相似性的System-1检索与全局推理的System-2机制相结合，通过双层图结构实现语义和结构相关的记忆检索，在长期记忆基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG和Graph-RAG方法主要依赖相似性检索机制，在处理需要全局推理或全面覆盖相关信息的场景时存在局限性，需要更智能的记忆检索方法。

Method: Mnemis采用双层图结构：基础图用于相似性检索，分层图支持自上而下的语义层次遍历，结合System-1相似性搜索和System-2全局选择机制。

Result: 在长期记忆基准测试中表现优异，LoCoMo得分93.9，LongMemEval-S得分91.6（使用GPT-4.1-mini），在所有对比方法中达到最先进性能。

Conclusion: 通过整合System-1和System-2两种检索机制的优势，Mnemis能够检索到语义和结构都相关的记忆项，为LLMs提供了更有效的记忆管理解决方案。

Abstract: AI Memory, specifically how models organizes and retrieves historical messages, becomes increasingly valuable to Large Language Models (LLMs), yet existing methods (RAG and Graph-RAG) primarily retrieve memory through similarity-based mechanisms. While efficient, such System-1-style retrieval struggles with scenarios that require global reasoning or comprehensive coverage of all relevant information. In this work, We propose Mnemis, a novel memory framework that integrates System-1 similarity search with a complementary System-2 mechanism, termed Global Selection. Mnemis organizes memory into a base graph for similarity retrieval and a hierarchical graph that enables top-down, deliberate traversal over semantic hierarchies. By combining the complementary strength from both retrieval routes, Mnemis retrieves memory items that are both semantically and structurally relevant. Mnemis achieves state-of-the-art performance across all compared methods on long-term memory benchmarks, scoring 93.9 on LoCoMo and 91.6 on LongMemEval-S using GPT-4.1-mini.

</details>


### [51] [NeuroSymActive: Differentiable Neural-Symbolic Reasoning with Active Exploration for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.15353)
*Rong Fu,Yang Li,Zeyu Zhang,Jiekai Wu,Yaohua Liu,Shuaishuai Cao,Yangchen Zeng,Yuhang Zhang,Xiaojing Du,Chuang Zhao,Kangning Cui,Simon Fong*

Main category: cs.CL

TL;DR: NeuroSymActive是一个结合神经符号推理与主动探索的模块化框架，用于知识图谱问答，通过软统一符号模块和蒙特卡洛探索策略在减少计算成本的同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型和神经推理系统在需要精确结构化多跳推理的知识密集型查询中的挑战，同时避免单纯符号方法的高检索成本和神经方法的低效性问题。

Method: 结合可微分神经符号推理层与基于价值的主动探索控制器，使用软统一风格符号模块、神经路径评估器和蒙特卡洛探索策略来优先选择高价值路径扩展。

Result: 在标准KGQA基准测试中实现了强答案准确性，同时相比常见的检索增强基线减少了昂贵的图查找和模型调用次数。

Conclusion: NeuroSymActive框架成功地将神经与符号方法相结合，在知识图谱问答任务中实现了效率与准确性的平衡，为知识密集型推理提供了有效的解决方案。

Abstract: Large pretrained language models and neural reasoning systems have advanced many natural language tasks, yet they remain challenged by knowledge-intensive queries that require precise, structured multi-hop inference. Knowledge graphs provide a compact symbolic substrate for factual grounding, but integrating graph structure with neural models is nontrivial: naively embedding graph facts into prompts leads to inefficiency and fragility, while purely symbolic or search-heavy approaches can be costly in retrievals and lack gradient-based refinement. We introduce NeuroSymActive, a modular framework that combines a differentiable neural-symbolic reasoning layer with an active, value-guided exploration controller for Knowledge Graph Question Answering. The method couples soft-unification style symbolic modules with a neural path evaluator and a Monte-Carlo style exploration policy that prioritizes high-value path expansions. Empirical results on standard KGQA benchmarks show that NeuroSymActive attains strong answer accuracy while reducing the number of expensive graph lookups and model calls compared to common retrieval-augmented baselines.

</details>


### [52] [Far Out: Evaluating Language Models on Slang in Australian and Indian English](https://arxiv.org/abs/2602.15373)
*Deniz Kaya Dilsiz,Dipankar Srirag,Aditya Joshi*

Main category: cs.CL

TL;DR: 本研究系统评估了7个先进语言模型对印度英语和澳大利亚英语俚语的理解能力，通过两个数据集和三个任务发现：判别任务表现优于生成任务，真实数据优于合成数据，印度英语理解优于澳大利亚英语，揭示了生成式与判别式能力在方言俚语理解上的不对称性。


<details>
  <summary>Details</summary>
Motivation: 语言模型在处理非标准语言变体时存在系统性性能差距，但对特定方言俚语的理解能力在多语言环境中仍未得到充分探索，特别是在印度英语和澳大利亚英语这样的变体中。

Method: 构建了两个互补数据集：Web数据集（377个来自Urban Dictionary的真实使用示例）和Gen数据集（1,492个合成生成的俚语使用场景）。评估了7个先进语言模型在三个任务上的表现：目标词预测(TWP)、引导目标词预测(TWP*)和目标词选择(TWS)。

Result: 1) TWS任务平均准确率(0.49)显著高于TWP和TWP*(0.03)；2) Web数据集表现优于Gen数据集，相似度分数分别提高0.03和0.05；3) 印度英语任务(0.54)整体优于澳大利亚英语(0.44)，TWS任务差异最大；4) 揭示了生成式与判别式能力在方言俚语理解上的根本不对称性。

Conclusion: 尽管英语是技术丰富的语言，但语言模型在处理方言特定俚语表达时，生成能力和判别能力存在显著不对称性，这为改进多语言和方言NLP系统提供了重要洞见。

Abstract: Language models exhibit systematic performance gaps when processing text in non-standard language varieties, yet their ability to comprehend variety-specific slang remains underexplored for several languages. We present a comprehensive evaluation of slang awareness in Indian English (en-IN) and Australian English (en-AU) across seven state-of-the-art language models. We construct two complementary datasets: \textsc{web}, containing 377 web-sourced usage examples from Urban Dictionary, and \textsc{gen}, featuring 1,492 synthetically generated usages of these slang terms, across diverse scenarios. We assess language models on three tasks: target word prediction (TWP), guided target word prediction (TWP$^*$) and target word selection (TWS). Our results reveal four key findings: (1) Higher average model performance TWS versus TWP and TWP$^*$, with average accuracy score increasing from 0.03 to 0.49 respectively (2) Stronger average model performance on \textsc{web} versus \textsc{gen} datasets, with average similarity score increasing by 0.03 and 0.05 across TWP and TWP$^*$ tasks respectively (3) en-IN tasks outperform en-AU when averaged across all models and datasets, with TWS demonstrating the largest disparity, increasing average accuracy from 0.44 to 0.54. These findings underscore fundamental asymmetries between generative and discriminative competencies for variety-specific language, particularly in the context of slang expressions despite being in a technologically rich language such as English.

</details>


### [53] [Orchestration-Free Customer Service Automation: A Privacy-Preserving and Flowchart-Guided Framework](https://arxiv.org/abs/2602.15377)
*Mengze Hong,Chen Jason Zhang,Zichang Guo,Hanlin Gu,Di Jiang,Li Qing*

Main category: cs.CL

TL;DR: 提出基于任务导向流程图的无编排框架，通过流程图构建算法从服务对话中提取过程知识，使用小型语言模型实现端到端客服自动化，解决现有方法编排复杂或泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有客服自动化方法要么依赖复杂的模块化编排系统，要么使用过于简化的指令模式，导致指导有限且泛化能力差，需要一种更高效、通用的端到端自动化解决方案。

Method: 定义任务导向流程图组件和评估指标，提出成本优化的流程图构建算法从对话数据中抽象过程知识，采用小型语言模型本地部署和基于流程图的去中心化蒸馏方法解决数据稀缺和隐私问题。

Result: 在多种服务任务上的大量实验验证了方法的有效性，相比强基线方法和市场产品展现出更优的量化指标和应用性能。

Conclusion: 该框架通过流程图驱动的自动化实现了无需人工干预的端到端客服服务，发布了基于web的系统演示和案例研究，旨在促进未来服务自动化的简化创建。

Abstract: Customer service automation has seen growing demand within digital transformation. Existing approaches either rely on modular system designs with extensive agent orchestration or employ over-simplified instruction schemas, providing limited guidance and poor generalizability. This paper introduces an orchestration-free framework using Task-Oriented Flowcharts (TOFs) to enable end-to-end automation without manual intervention. We first define the components and evaluation metrics for TOFs, then formalize a cost-efficient flowchart construction algorithm to abstract procedural knowledge from service dialogues. We emphasize local deployment of small language models and propose decentralized distillation with flowcharts to mitigate data scarcity and privacy issues in model training. Extensive experiments validate the effectiveness in various service tasks, with superior quantitative and application performance compared to strong baselines and market products. By releasing a web-based system demonstration with case studies, we aim to promote streamlined creation of future service automation.

</details>


### [54] [Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language](https://arxiv.org/abs/2602.15378)
*Prathamesh Devadiga,Paras Chopra*

Main category: cs.CL

TL;DR: 通过结构化提示而非微调，使大语言模型在缺乏训练数据的图鲁语中实现基本对话能力，词汇污染从80%降至5%，语法准确率达85%


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型是否能在训练数据极少的情况下（如图鲁语，200万使用者但数字存在感低）仅通过提示工程实现跨语言对话能力

Method: 结合显式语法文档、负约束抑制相关语言高概率词、罗马化标准化、通过自我对弈生成质量可控的合成数据，在三个LLM（Gemini 2.0 Flash、GPT-4o、Llama 3.1 70B）上进行系统实验

Result: 词汇污染从80%降至5%，语法准确率达到85%。负约束带来12-18个百分点的稳定提升，语法文档效果因模型架构而异（8-22个百分点）

Conclusion: 仅通过结构化提示工程即可在零训练数据条件下实现低资源语言的对话能力，负约束是关键改进因素，模型架构影响语法文档的有效性

Abstract: Can large language models converse in languages virtually absent from their training data? We investigate this question through a case study on Tulu, a Dravidian language with over 2 million speakers but minimal digital presence. Rather than fine-tuning an LLM, we examine whether structured prompts alone can elicit basic conversational ability under controlled prompting. We systematically tackle various challenges posed by absence of training data for Tulu by combining explicit grammar documentation, negative constraints to suppress high-probability tokens from related languages, romanization standardization, and quality-controlled synthetic data generation via self-play. Evaluated on a manually curated held-out set across three LLMs (Gemini 2.0 Flash, GPT-4o, Llama 3.1 70B) and validated by native speakers, our approach reduces vocabulary contamination from 80% to 5% while achieving 85% grammatical accuracy. Cross-model analysis reveals that negative constraints provide consistent improvements (12--18 percentage points), while grammar documentation effects vary by model architecture (8--22 points).

</details>


### [55] [The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems](https://arxiv.org/abs/2602.15382)
*Xiaoze Liu,Ruowang Zhang,Weichen Yu,Siheng Xiong,Liu He,Feijie Wu,Hoin Jung,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.CL

TL;DR: Vision Wormhole框架通过视觉语言模型的视觉接口实现模型无关的无文本通信，使用通用视觉编解码器将异构推理轨迹映射到共享连续潜在空间，显著减少多智能体系统通信开销。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的多智能体系统受限于离散文本通信的低效率和信息量化损失，而异构潜在状态传输方法要么假设同构架构，要么依赖特定配对的学习转换器，限制了跨不同模型族的可扩展性和模块化。

Method: 提出Vision Wormhole框架，利用视觉语言模型的视觉接口作为通用通信端口，采用中心辐射拓扑结构将成对对齐复杂度从O(N²)降低到O(N)，使用无标签的师生蒸馏目标对齐高速视觉通道与文本通道路径。

Result: 在异构模型家族（如Qwen-VL、Gemma）上的广泛实验表明，Vision Wormhole在受控比较中减少了端到端运行时间，同时保持了与标准基于文本的多智能体系统相当的推理保真度。

Conclusion: Vision Wormhole提供了一个高效、模型无关的多智能体通信解决方案，通过视觉接口实现低延迟、高带宽的潜在状态传输，为异构大语言模型的协作推理开辟了新途径。

Abstract: Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas

</details>


### [56] [Measuring Social Integration Through Participation: Categorizing Organizations and Leisure Activities in the Displaced Karelians Interview Archive using LLMs](https://arxiv.org/abs/2602.15436)
*Joonatan Laato,Veera Schroderus,Jenna Kanerva,Jenni Kauppi,Virpi Lummaa,Filip Ginter*

Main category: cs.CL

TL;DR: 开发了一个分类框架来对大规模历史档案中的休闲活动和组织成员进行自动分类，使用大语言模型实现了接近专家水平的标注效果，为研究社会整合提供了结构化数据资源


<details>
  <summary>Details</summary>
Motivation: 数字化历史档案包含大量日常生活信息，但原始文本数据过于庞大且难以直接用于定量研究，需要将35万个活动和组织名称进行有效分类以支持历史学和社会学研究

Method: 构建包含活动类型、社交性、规律性和体力需求四个维度的分类框架，创建专家标注的金标准数据集，采用多轮投票的大语言模型方法进行大规模自动分类

Result: 开源大语言模型通过简单投票方法能够达到接近专家判断的标注效果，成功为35万个实体完成了分类标注

Conclusion: 该方法有效解决了大规模历史文本数据的结构化问题，为研究社会整合等相关议题提供了可靠的数据基础，证明了LLM在历史社会学研究中的实用价值

Abstract: Digitized historical archives make it possible to study everyday social life on a large scale, but the information extracted directly from text often does not directly allow one to answer the research questions posed by historians or sociologists in a quantitative manner. We address this problem in a large collection of Finnish World War II Karelian evacuee family interviews. Prior work extracted more than 350K mentions of leisure time activities and organizational memberships from these interviews, yielding 71K unique activity and organization names -- far too many to analyze directly.
  We develop a categorization framework that captures key aspects of participation (the kind of activity/organization, how social it typically is, how regularly it happens, and how physically demanding it is). We annotate a gold-standard set to allow for a reliable evaluation, and then test whether large language models can apply the same schema at scale. Using a simple voting approach across multiple model runs, we find that an open-weight LLM can closely match expert judgments. Finally, we apply the method to label the 350K entities, producing a structured resource for downstream studies of social integration and related outcomes.

</details>


### [57] [TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models](https://arxiv.org/abs/2602.15449)
*Chansung Park,Juyong Jiang,Fan Wang,Sayak Paul,Jiasi Shen,Jing Tang,Jianguo Li*

Main category: cs.CL

TL;DR: TAROT是一种自适应课程强化微调方法，通过构建四层次测试套件（基础、中级、复杂、边缘）并根据模型能力动态调整课程策略，显著提升代码生成的功能正确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有强化微调方法忽视测试用例的异质难度和粒度，导致奖励信号分布不平衡和训练中的梯度偏差，需要更系统的方法来激励LLMs的深度推理能力。

Method: 提出TAROT方法：1）为每个问题构建四层次测试套件；2）将课程进度与原始奖励分数解耦；3）基于模型能力进行条件评估；4）从课程策略组合中进行原则性选择。

Result: 实验表明RFT的最优课程策略与模型内在能力密切相关：能力较弱的模型在易到难课程中获益更大，而能力较强的模型在难到易课程中表现更佳。

Conclusion: TAROT提供了一种可复现的方法，能自适应地根据模型能力定制课程设计，持续提升生成代码的功能正确性和鲁棒性，所有代码和数据已开源。

Abstract: Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.

</details>


### [58] [In Agents We Trust, but Who Do Agents Trust? Latent Source Preferences Steer LLM Generations](https://arxiv.org/abs/2602.15456)
*Mohammad Aflah Khan,Mahsa Amani,Soumi Das,Bishwamittra Ghosh,Qinyuan Wu,Krishna P. Gummadi,Manish Gupta,Abhilasha Ravichander*

Main category: cs.CL

TL;DR: LLM代理在处理信息时存在系统性的来源偏好，会优先选择某些特定来源的信息，这种偏好受上下文框架影响且难以通过提示消除。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理被广泛部署为在线平台的信息接口，它们通过筛选、排序和综合信息来影响用户接收的内容。虽然已有研究关注LLM生成内容的偏见，但对其选择信息时的来源偏好研究较少。

Method: 对来自6个模型提供商的12个LLM进行控制实验，涵盖合成和真实世界任务，系统测试模型在不同情境下对信息来源的偏好表现。

Result: 多个模型表现出强烈且可预测的来源偏好，这些偏好对上下文框架敏感，可能压倒内容本身的影响，且无法通过明确提示消除。这些偏好解释了先前研究中观察到的新闻推荐左倾偏差现象。

Conclusion: 研究结果表明需要深入探究这些偏好的起源，并建立机制为用户提供透明度和控制权，以管理LLM驱动代理的偏见问题。

Abstract: Agents based on Large Language Models (LLMs) are increasingly being deployed as interfaces to information on online platforms. These agents filter, prioritize, and synthesize information retrieved from the platforms' back-end databases or via web search. In these scenarios, LLM agents govern the information users receive, by drawing users' attention to particular instances of retrieved information at the expense of others. While much prior work has focused on biases in the information LLMs themselves generate, less attention has been paid to the factors that influence what information LLMs select and present to users. We hypothesize that when information is attributed to specific sources (e.g., particular publishers, journals, or platforms), current LLMs exhibit systematic latent source preferences- that is, they prioritize information from some sources over others. Through controlled experiments on twelve LLMs from six model providers, spanning both synthetic and real-world tasks, we find that several models consistently exhibit strong and predictable source preferences. These preferences are sensitive to contextual framing, can outweigh the influence of content itself, and persist despite explicit prompting to avoid them. They also help explain phenomena such as the observed left-leaning skew in news recommendations in prior work. Our findings advocate for deeper investigation into the origins of these preferences, as well as for mechanisms that provide users with transparency and control over the biases guiding LLM-powered agents.

</details>


### [59] [Towards Expectation Detection in Language: A Case Study on Treatment Expectations in Reddit](https://arxiv.org/abs/2602.15504)
*Aswathy Velutharambath,Amelie Wührl*

Main category: cs.CL

TL;DR: 本文提出了期望检测新任务，构建了RedHOTExpect语料库（4500条Reddit帖子），使用大语言模型进行银标注并人工验证（准确率约78%），分析了医疗期望的语言模式和患者期望内容。


<details>
  <summary>Details</summary>
Motivation: 患者期望对治疗效果有重要影响，但临床环境外的期望表达未被充分研究。在线患者平台可能包含患者不愿在其他地方分享的期望信息，而NLP领域此前未系统研究期望检测任务。

Method: 构建RedHOTExpect语料库，使用大语言模型进行银标注并人工验证标注质量，分析期望的语言特征模式，探索患者期望的内容和原因。

Result: 发现身体疾病或治疗相关帖子中的乐观和积极表达比心理健康语境更明显，患者主要讨论积极益处而非负面结果。标注准确率达到约78%。

Conclusion: 期望检测是NLP领域的重要新任务，RedHOTExpect语料库为研究医疗期望提供了有价值的数据资源，有助于意见挖掘和产品设计等应用。

Abstract: Patients' expectations towards their treatment have a substantial effect on the treatments' success. While primarily studied in clinical settings, online patient platforms like medical subreddits may hold complementary insights: treatment expectations that patients feel unnecessary or uncomfortable to share elsewhere. Despite this, no studies examine what type of expectations users discuss online and how they express them. Presumably this is because expectations have not been studied in natural language processing (NLP) before. Therefore, we introduce the task of Expectation Detection, arguing that expectations are relevant for many applications, including opinion mining and product design. Subsequently, we present a case study for the medical domain, where expectations are particularly crucial to extract. We contribute RedHOTExpect, a corpus of Reddit posts (4.5K posts) to study expectations in this context. We use a large language model (LLM) to silver-label the data and validate its quality manually (label accuracy ~78%). Based on this, we analyze which linguistic patterns characterize expectations and explore what patients expect and why. We find that optimism and proactive framing are more pronounced in posts about physical or treatment-related illnesses compared to mental-health contexts, and that in our dataset, patients mostly discuss benefits rather than negative outcomes. The RedHOTExpect corpus can be obtained from https://www.ims.uni-stuttgart.de/data/RedHOTExpect

</details>


### [60] [LuxMT Technical Report](https://arxiv.org/abs/2602.15506)
*Nils Rehlinger*

Main category: cs.CL

TL;DR: LuxMT是基于Gemma 3 27B的卢森堡语翻译系统，专用于卢森堡语到法语和英语的翻译。通过LuxAlign语料和人工翻译数据训练，使用LuxEmbedder过滤低质量数据，在多个语言对上都展现出优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 解决卢森堡语（LB）机器翻译资源稀缺的问题，构建专门针对卢森堡语到法语（FR）和英语（EN）的高质量翻译系统。

Method: 使用Gemma 3 27B作为基础模型，在LuxAlign平行语料（多语言卢森堡新闻文章和议会记录）和Google Translate增强数据上进行微调。采用LuxEmbedder（卢森堡语句嵌入）过滤低等价性片段对。

Result: LuxMT在LB-FR和LB-EN翻译任务上相比Gemma 3基线有显著提升，甚至在未训练过的LB-DE翻译上也表现出色。LuxEmbedder作为质量评估指标与其他基于参考的指标有强相关性。

Conclusion: 该系统为卢森堡语机器翻译提供了有效解决方案，LuxEmbedder显示出作为质量评估指标的潜力，但需要进一步研究验证其可靠性，建议谨慎使用。

Abstract: We introduce LuxMT, a machine translation system based on Gemma 3 27B and fine-tuned for translation from Luxembourgish (LB) into French (FR) and English (EN). To assess translation performance, we construct a novel benchmark covering LB-FR, LB-EN, and LB-FR using human-translated data from Luci, a tourist magazine about Luxembourg. Training data stems from LuxAlign, a parallel corpus of multilingual Luxembourgish news articles, and LB parliamentary transcripts augmented with Google Translate. We filter the data using LuxEmbedder, LB sentence embeddings, to remove low-equivalence segment-pairs. Overall, LuxMT's results suggest strong improvements over the Gemma 3 baseline, even for translating LB to German (DE), despite the training data not containing any DE. We also explore LuxEmbedder's potential to be used as a quality estimation metric and find strong correlations with other reference-based metrics. However, we call for further research to fully assess the metric's utility and advise using it with caution.

</details>


### [61] [Fine-Refine: Iterative Fine-grained Refinement for Mitigating Dialogue Hallucination](https://arxiv.org/abs/2602.15509)
*Xiangyan Chen,Yujian Gan,Matthew Purver*

Main category: cs.CL

TL;DR: Fine-Refine是一个细粒度精炼框架，通过将对话响应分解为原子单元，利用外部知识验证每个单元，并通过困惑度评估流畅性，迭代修正细粒度错误，显著提升大语言模型对话的事实准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在对话系统中存在幻觉问题，产生事实错误的响应误导用户并损害系统信任。现有精炼方法通常在响应层面操作，忽略了单个响应可能包含多个可验证或不可验证事实的问题。

Method: 提出Fine-Refine框架：1）将响应分解为原子单元；2）使用外部知识验证每个单元的事实准确性；3）通过困惑度评估流畅性；4）迭代修正细粒度错误。

Result: 在HybriDialogue和OpendialKG数据集上的实验显示，Fine-Refine显著提升了事实准确性，对话事实得分最高提升7.63个百分点，对话质量仅有小幅下降。

Conclusion: Fine-Refine通过细粒度的响应分解和验证机制，有效解决了对话系统中的幻觉问题，在保持对话质量的同时大幅提升了事实准确性，为构建更可靠的大语言模型对话系统提供了有效方案。

Abstract: The tendency for hallucination in current large language models (LLMs) negatively impacts dialogue systems. Such hallucinations produce factually incorrect responses that may mislead users and undermine system trust. Existing refinement methods for dialogue systems typically operate at the response level, overlooking the fact that a single response may contain multiple verifiable or unverifiable facts. To address this gap, we propose Fine-Refine, a fine-grained refinement framework that decomposes responses into atomic units, verifies each unit using external knowledge, assesses fluency via perplexity, and iteratively corrects granular errors. We evaluate factuality across the HybriDialogue and OpendialKG datasets in terms of factual accuracy (fact score) and coverage (Not Enough Information Proportion), and experiments show that Fine-Refine substantially improves factuality, achieving up to a 7.63-point gain in dialogue fact score, with a small trade-off in dialogue quality.

</details>


### [62] [DependencyAI: Detecting AI Generated Text through Dependency Parsing](https://arxiv.org/abs/2602.15514)
*Sara Ahmed,Tracy Hammond*

Main category: cs.CL

TL;DR: DependencyAI是一种基于语言依存关系标签的AI生成文本检测方法，在单语言、多生成器和多语言场景下均表现优异，具有高可解释性且无需神经网络。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型普及，需要可靠的方法检测AI生成文本来降低潜在风险，现有方法缺乏可解释性且依赖复杂神经网络。

Method: 仅使用语言依存关系标签作为特征，通过分析依存关系模式来区分AI生成和人类撰写文本，提供高度可解释的特征重要性分析。

Result: 在多个测试场景中达到竞争性性能，发现生成器特定写作风格会影响跨域泛化，某些模型在未见领域存在系统性过预测现象。

Conclusion: 依存关系单独即可为AI生成文本检测提供强有力信号，DependencyAI成为基于语言学基础、可解释且非神经网络的强基准方法。

Abstract: As large language models (LLMs) become increasingly prevalent, reliable methods for detecting AI-generated text are critical for mitigating potential risks. We introduce DependencyAI, a simple and interpretable approach for detecting AI-generated text using only the labels of linguistic dependency relations. Our method achieves competitive performance across monolingual, multi-generator, and multilingual settings. To increase interpretability, we analyze feature importance to reveal syntactic structures that distinguish AI-generated from human-written text. We also observe a systematic overprediction of certain models on unseen domains, suggesting that generator-specific writing styles may affect cross-domain generalization. Overall, our results demonstrate that dependency relations alone provide a robust signal for AI-generated text detection, establishing DependencyAI as a strong linguistically grounded, interpretable, and non-neural network baseline.

</details>


### [63] [ExpertWeaver: Unlocking the Inherent MoE in Dense LLMs with GLU Activation Patterns](https://arxiv.org/abs/2602.15521)
*Ziyu Zhao,Tong Zhu,Zhi Zhang,Tiantian Fan,Jinluan Yang,Kun Kuang,Zhongyu Wei,Fei Wu,Yu Cheng*

Main category: cs.CL

TL;DR: ExpertWeaver是一个无需训练的框架，利用GLU机制的激活模式将预训练稠密模型转换为稀疏MoE架构，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有稠密到MoE转换方法破坏了稠密模型的内在激活模式，导致专家构建不理想。GLU机制的细粒度激活模式揭示了粗粒度结构，展现了固有的MoE架构。

Method: 基于GLU激活模式对神经元进行分区，构建共享专家和专用路由专家，采用层自适应配置，实现训练自由的动态结构剪枝和降循环策略。

Result: 实验证明ExpertWeaver在作为训练自由的动态结构剪枝技术和MoE初始化策略方面都显著优于现有方法。

Conclusion: GLU机制为稠密到MoE转换提供了自然蓝图，ExpertWeaver框架成功利用了这种内在结构，实现了高质量的模型转换。

Abstract: Mixture-of-Experts (MoE) effectively scales model capacity while preserving computational efficiency through sparse expert activation. However, training high-quality MoEs from scratch is prohibitively expensive. A promising alternative is to convert pretrained dense models into sparse MoEs. Existing dense-to-MoE methods fall into two categories: \textbf{dynamic structural pruning} that converts dense models into MoE architectures with moderate sparsity to balance performance and inference efficiency, and \textbf{downcycling} approaches that use pretrained dense models to initialize highly sparse MoE architectures. However, existing methods break the intrinsic activation patterns within dense models, leading to suboptimal expert construction. In this work, we argue that the Gated Linear Unit (GLU) mechanism provides a natural blueprint for dense-to-MoE conversion. We show that the fine-grained neural-wise activation patterns of GLU reveal a coarse-grained structure, uncovering an inherent MoE architecture composed of consistently activated universal neurons and dynamically activated specialized neurons. Leveraging this discovery, we introduce ExpertWeaver, a training-free framework that partitions neurons according to their activation patterns and constructs shared experts and specialized routed experts with layer-adaptive configurations. Our experiments demonstrate that ExpertWeaver significantly outperforms existing methods, both as a training-free dynamic structural pruning technique and as a downcycling strategy for superior MoE initialization.

</details>


### [64] [ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling](https://arxiv.org/abs/2602.15537)
*Nicol Visser,Simon Malan,Danel Slabbert,Herman Kamper*

Main category: cs.CL

TL;DR: ZeroSyl是一种无需训练的简单方法，直接从冻结的WavLM模型中提取音节边界和嵌入，通过中间层特征的L2范数实现音节分割，在多项基准测试中优于现有音节化方法。


<details>
  <summary>Details</summary>
Motivation: 纯语音语言模型需要从原始音频学习语言，但自监督语音编码器的离散标记会产生过长序列，现有音节方法依赖复杂的多阶段训练流程，需要更简单的解决方案。

Method: 使用冻结WavLM模型中间层特征的L2范数检测音节边界，对得到的片段进行均值池化和K-means离散化，然后用这些标记训练语言模型。

Result: 在词汇、句法和叙事基准测试中优于先前的音节化分词器，缩放实验显示较细粒度单元对词汇任务有益，而发现的音节单元在句法建模中表现出更好的缩放行为。

Conclusion: ZeroSyl提供了一种简单有效的训练无关方法，直接从预训练语音模型中提取音节单元，为纯语音语言建模提供了更优的解决方案。

Abstract: Pure speech language models aim to learn language directly from raw audio without textual resources. A key challenge is that discrete tokens from self-supervised speech encoders result in excessively long sequences, motivating recent work on syllable-like units. However, methods like Sylber and SyllableLM rely on intricate multi-stage training pipelines. We propose ZeroSyl, a simple training-free method to extract syllable boundaries and embeddings directly from a frozen WavLM model. Using L2 norms of features in WavLM's intermediate layers, ZeroSyl achieves competitive syllable segmentation performance. The resulting segments are mean-pooled, discretized using K-means, and used to train a language model. ZeroSyl outperforms prior syllabic tokenizers across lexical, syntactic, and narrative benchmarks. Scaling experiments show that while finer-grained units are beneficial for lexical tasks, our discovered syllabic units exhibit better scaling behavior for syntactic modeling.

</details>


### [65] [Perspectives - Interactive Document Clustering in the Discourse Analysis Tool Suite](https://arxiv.org/abs/2602.15540)
*Tim Fischer,Chris Biemann*

Main category: cs.CL

TL;DR: Perspectives是一个交互式文档分析工具扩展，通过基于提示的文档重写和指令嵌入实现灵活的主题聚类，支持人机协作的聚类优化和嵌入模型微调，帮助数字人文学者探索大型非结构化文档集。


<details>
  <summary>Details</summary>
Motivation: 为数字人文学者提供有效探索和组织大规模非结构化文档集合的工具，通过人机协作的方式提升文档聚类的准确性和相关性。

Method: 采用基于文档重写提示和指令嵌入的方面聚焦文档聚类流水线，结合人机循环优化机制，提供聚类精炼工具和嵌入模型微调功能。

Result: 开发了具有交互式文档地图的Perspectives系统，能够帮助研究者发现主题、情感等相关类别，为后续深度分析准备数据。

Conclusion: Perspectives通过结合自动化聚类和人机交互优化，为数字人文研究提供了有效的文档探索和组织解决方案，展示了在大型非结构化文档分析中的实用价值。

Abstract: This paper introduces Perspectives, an interactive extension of the Discourse Analysis Tool Suite designed to empower Digital Humanities (DH) scholars to explore and organize large, unstructured document collections. Perspectives implements a flexible, aspect-focused document clustering pipeline with human-in-the-loop refinement capabilities. We showcase how this process can be initially steered by defining analytical lenses through document rewriting prompts and instruction-based embeddings, and further aligned with user intent through tools for refining clusters and mechanisms for fine-tuning the embedding model. The demonstration highlights a typical workflow, illustrating how DH researchers can leverage Perspectives's interactive document map to uncover topics, sentiments, or other relevant categories, thereby gaining insights and preparing their data for subsequent in-depth analysis.

</details>


### [66] [jina-embeddings-v5-text: Task-Targeted Embedding Distillation](https://arxiv.org/abs/2602.15547)
*Mohammad Kalim Akram,Saba Sturua,Nastia Havriushenko,Quentin Herreros,Michael Günther,Maximilian Werk,Han Xiao*

Main category: cs.CL

TL;DR: 提出了一种结合模型蒸馏技术和任务特定对比损失的新训练方法，用于训练紧凑高效的小型文本嵌入模型，在保持高性能的同时支持长文本处理和多语言能力。


<details>
  <summary>Details</summary>
Motivation: 现有通用文本嵌入模型通常使用单一或多阶段的对比损失训练，但对于小型模型来说效果有限。需要开发更有效的训练方法来提升小模型的性能。

Method: 结合模型蒸馏技术与任务特定的对比损失函数进行训练，产生紧凑的高性能嵌入模型。支持长达32k token的长文本处理和多语言能力。

Result: 开发的jina-embeddings-v5-text-small和jina-embeddings-v5-text-nano模型在基准测试中达到或超越了同类尺寸模型的最先进性能，嵌入表示在截断和二进制量化下保持鲁棒性。

Conclusion: 该混合训练方法比纯对比学习或纯蒸馏方法更有效地训练小型嵌入模型，为嵌入模型开发提供了新的思路，模型权重已公开以促进进一步研究。

Abstract: Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.

</details>


### [67] [Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL](https://arxiv.org/abs/2602.15564)
*Yihan Wang,Peiyu Liu,Runyu Chen,Wei Xu*

Main category: cs.CL

TL;DR: SquRL：基于强化学习的自适应工作流构建框架，通过动态策略在推理时构建Text-to-SQL工作流，显著优于静态方法，特别是在复杂和分布外查询上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法依赖静态工作流，难以扩展到分布外和长尾场景，需要用户通过大量实验选择合适方法，因此需要系统能在推理时自适应构建工作流。

Method: 提出SquRL强化学习框架，设计基于规则的奖励函数，引入动态参与者掩码机制促进广泛探索，使用伪奖励提高训练效率，增强LLM在自适应工作流构建中的推理能力。

Result: 在广泛使用的Text-to-SQL基准测试中，动态工作流构建方法持续优于最佳静态工作流方法，在复杂和分布外查询上获得特别显著的性能提升。

Conclusion: 动态策略始终优于最佳静态工作流，性能提升主要由候选工作流的异质性驱动，SquRL框架有效解决了静态工作流在现实场景中的可扩展性限制问题。

Abstract: Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs' reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at https://github.com/Satissss/SquRL

</details>


### [68] [Clinically Inspired Symptom-Guided Depression Detection from Emotion-Aware Speech Representations](https://arxiv.org/abs/2602.15578)
*Chaithra Nerella,Chiranjeevi Yarra*

Main category: cs.CL

TL;DR: 提出一个症状导向的抑郁症严重程度评估框架，通过症状引导的交叉注意力机制将PHQ-8问卷症状与语音情感表征对齐，实现症状级别的可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症预测方法多采用二元标签或整体严重程度评分，缺乏对具体症状的建模能力，限制了临床筛查中症状级别分析的价值。

Method: 使用症状引导的交叉注意力机制，将PHQ-8问卷项目与情感感知语音表征对齐；引入可学习的症状特定参数自适应控制注意力分布锐度；在EDAIC临床数据集上进行验证。

Result: 在标准临床数据集EDAIC上性能优于先前工作；注意力分布分析显示对包含多个抑郁症状线索的话语赋予更高注意力，证明方法的可解释性。

Conclusion: 症状引导和情感感知建模对基于语音的抑郁症筛查具有重要意义，该方法提供了症状级别的临床相关分析能力。

Abstract: Depression manifests through a diverse set of symptoms such as sleep disturbance, loss of interest, and concentration difficulties. However, most existing works treat depression prediction either as a binary label or an overall severity score without explicitly modeling symptom-specific information. This limits their ability to provide symptom-level analysis relevant to clinical screening. To address this, we propose a symptom-specific and clinically inspired framework for depression severity estimation from speech. Our approach uses a symptom-guided cross-attention mechanism that aligns PHQ-8 questionnaire items with emotion-aware speech representations to identify which segments of a participant's speech are more important to each symptom. To account for differences in how symptoms are expressed over time, we introduce a learnable symptom-specific parameter that adaptively controls the sharpness of attention distributions. Our results on EDAIC, a standard clinical-style dataset, demonstrate improved performance outperforming prior works. Further, analyzing the attention distributions showed that higher attention is assigned to utterances containing cues related to multiple depressive symptoms, highlighting the interpretability of our approach. These findings outline the importance of symptom-guided and emotion-aware modeling for speech-based depression screening.

</details>


### [69] [STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens](https://arxiv.org/abs/2602.15620)
*Shiqi Liu,Zeyu He,Guojian Zhan,Letian Tao,Zhilong Zheng,Jiang Wu,Yinuo Wang,Yang Guan,Kehua Sheng,Bo Zhang,Keqiang Li,Jingliang Duan,Shengbo Eben Li*

Main category: cs.CL

TL;DR: STAPO是一种针对大语言模型强化学习优化的新方法，通过识别并屏蔽有害的伪令牌梯度更新来解决训练不稳定问题，在数学推理基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RL微调方法依赖启发式技术（如熵正则化和重加权）来维持稳定性，但实践中经常出现后期性能崩溃，导致推理质量下降和训练不稳定。研究发现政策梯度大小与令牌概率和局部政策熵负相关，训练不稳定由约0.01%的伪令牌驱动。

Method: 提出伪令牌感知策略优化（STAPO），选择性屏蔽伪令牌的梯度更新，并在有效令牌上重新归一化损失函数。基于令牌级政策梯度与令牌概率和局部熵的负相关关系，识别并处理有害的梯度更新。

Result: 在六个数学推理基准测试中使用Qwen 1.7B、8B和14B基础模型，STAPO展现出优异的熵稳定性，平均性能比GRPO、20-Entropy和JustRL提高7.13%。

Conclusion: STAPO通过系统识别和处理伪令牌问题，有效解决了RL微调中的训练不稳定问题，为大语言模型的强化学习优化提供了更稳定和高效的解决方案。

Abstract: Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\%, which we term \emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\% over GRPO, 20-Entropy and JustRL.

</details>


### [70] [LLM-to-Speech: A Synthetic Data Pipeline for Training Dialectal Text-to-Speech Models](https://arxiv.org/abs/2602.15675)
*Ahmed Khaled Khamis,Hesham Ali*

Main category: cs.CL

TL;DR: NileTTS：首个公开的埃及阿拉伯语TTS数据集，包含38小时语音数据，采用LLM生成内容+音频合成+自动转录的创新流程，基于XTTS v2微调的开源模型


<details>
  <summary>Details</summary>
Motivation: 解决埃及阿拉伯语（最广泛理解的阿拉伯方言）在神经TTS研究中资源严重不足的问题，目前资源主要集中在现代标准阿拉伯语和海湾方言

Method: 1) 使用大语言模型生成埃及阿拉伯语内容；2) 音频合成工具转换为自然语音；3) 自动转录和说话人分割；4) 人工质量验证；5) 在XTTS v2多语言TTS模型上进行微调

Result: 创建了包含38小时转录语音的NileTTS数据集，涵盖医疗、销售和一般对话等多个领域，两个说话人，微调模型在埃及阿拉伯语TTS任务上表现优于基于其他阿拉伯方言训练的基线模型

Conclusion: 成功填补了埃及阿拉伯语TTS资源空白，提供了可复现的方言TTS合成数据生成流程和开源模型，为埃及阿拉伯语语音合成研究提供了重要资源

Abstract: Despite the advances in neural text to speech (TTS), many Arabic dialectal varieties remain marginally addressed, with most resources concentrated on Modern Spoken Arabic (MSA) and Gulf dialects, leaving Egyptian Arabic -- the most widely understood Arabic dialect -- severely under-resourced. We address this gap by introducing NileTTS: 38 hours of transcribed speech from two speakers across diverse domains including medical, sales, and general conversations. We construct this dataset using a novel synthetic pipeline: large language models (LLM) generate Egyptian Arabic content, which is then converted to natural speech using audio synthesis tools, followed by automatic transcription and speaker diarization with manual quality verification. We fine-tune XTTS v2, a state-of-the-art multilingual TTS model, on our dataset and evaluate against the baseline model trained on other Arabic dialects. Our contributions include: (1) the first publicly available Egyptian Arabic TTS dataset, (2) a reproducible synthetic data generation pipeline for dialectal TTS, and (3) an open-source fine-tuned model. All resources are released to advance Egyptian Arabic speech synthesis research.

</details>


### [71] [Revisiting Northrop Frye's Four Myths Theory with Large Language Models](https://arxiv.org/abs/2602.15678)
*Edirlei Soares de Lima,Marco A. Casanova,Antonio L. Furtado*

Main category: cs.CL

TL;DR: 基于诺斯罗普·弗莱的四种叙事体裁理论，提出结合荣格原型理论的新角色功能框架，通过大语言模型验证角色-体裁对应关系的系统模式，为计算叙事学和交互式叙事应用提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有计算叙事学研究主要关注叙事模式而非角色功能，需要开发一个能够补充模式分析的角色功能框架，以更全面理解弗莱叙事体裁理论。

Method: 从荣格心理结构组件推导出四种通用角色功能（主角、导师、反派、同伴），基于原型作品将其细化为16种体裁特定角色。使用6个先进大语言模型对40部叙事作品的160个有效对应和30个无效对应进行验证。

Result: 大语言模型达到82.5%的平均平衡准确率，模型间一致性高（Fleiss' κ=0.600）。不同体裁准确率72.7%-89.9%，不同角色准确率52.5%-99.2%，定性分析显示变异反映了真实的叙事特性。

Conclusion: 该基于角色的方法展示了LLM支持的计算叙事学方法潜力，为叙事生成方法和交互式叙事应用的未来发展奠定了基础。

Abstract: Northrop Frye's theory of four fundamental narrative genres (comedy, romance, tragedy, satire) has profoundly influenced literary criticism, yet computational approaches to his framework have focused primarily on narrative patterns rather than character functions. In this paper, we present a new character function framework that complements pattern-based analysis by examining how archetypal roles manifest differently across Frye's genres. Drawing on Jungian archetype theory, we derive four universal character functions (protagonist, mentor, antagonist, companion) by mapping them to Jung's psychic structure components. These functions are then specialized into sixteen genre-specific roles based on prototypical works. To validate this framework, we conducted a multi-model study using six state-of-the-art Large Language Models (LLMs) to evaluate character-role correspondences across 40 narrative works. The validation employed both positive samples (160 valid correspondences) and negative samples (30 invalid correspondences) to evaluate whether models both recognize valid correspondences and reject invalid ones. LLMs achieved substantial performance (mean balanced accuracy of 82.5%) with strong inter-model agreement (Fleiss' $κ$ = 0.600), demonstrating that the proposed correspondences capture systematic structural patterns. Performance varied by genre (ranging from 72.7% to 89.9%) and role (52.5% to 99.2%), with qualitative analysis revealing that variations reflect genuine narrative properties, including functional distribution in romance and deliberate archetypal subversion in satire. This character-based approach demonstrates the potential of LLM-supported methods for computational narratology and provides a foundation for future development of narrative generation methods and interactive storytelling applications.

</details>


### [72] [A Content-Based Framework for Cybersecurity Refusal Decisions in Large Language Models](https://arxiv.org/abs/2602.15689)
*Meirav Segal,Noa Linder,Omer Antverg,Gil Gekker,Tomer Fichman,Omri Bodenheimer,Edan Maor,Omer Nevo*

Main category: cs.CL

TL;DR: 提出基于内容而非意图的网络安全拒绝框架，通过五个技术维度（攻击行动贡献、攻击风险、技术复杂度、防御效益、合法用户预期频率）显式建模攻防权衡，解决现有基于主题禁令方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于主题分类和意图判断的网络安全拒绝机制存在不一致性、过度限制合法防御者、对混淆和分段请求表现脆弱等问题，需要更有效的攻防权衡建模方法。

Method: 设计内容基础的拒绝框架，通过五个技术维度分析请求：攻击行动贡献、攻击风险、技术复杂度、防御效益、合法用户预期频率，基于请求的技术实质而非陈述意图。

Result: 该框架能够解决当前前沿模型行为的不一致性，允许组织构建可调节的、风险感知的拒绝策略，相比传统方法表现更优。

Conclusion: 基于内容的攻防权衡框架为网络安全拒绝策略提供了更有效、更一致的设计和审计方法，能够更好地平衡安全风险与合法防御需求。

Abstract: Large language models and LLM-based agents are increasingly used for cybersecurity tasks that are inherently dual-use. Existing approaches to refusal, spanning academic policy frameworks and commercially deployed systems, often rely on broad topic-based bans or offensive-focused taxonomies. As a result, they can yield inconsistent decisions, over-restrict legitimate defenders, and behave brittlely under obfuscation or request segmentation. We argue that effective refusal requires explicitly modeling the trade-off between offensive risk and defensive benefit, rather than relying solely on intent or offensive classification. In this paper, we introduce a content-based framework for designing and auditing cyber refusal policies that makes offense-defense tradeoffs explicit. The framework characterizes requests along five dimensions: Offensive Action Contribution, Offensive Risk, Technical Complexity, Defensive Benefit, and Expected Frequency for Legitimate Users, grounded in the technical substance of the request rather than stated intent. We demonstrate that this content-grounded approach resolves inconsistencies in current frontier model behavior and allows organizations to construct tunable, risk-aware refusal policies.

</details>


### [73] [Rethinking Metrics for Lexical Semantic Change Detection](https://arxiv.org/abs/2602.15716)
*Roksana Goworek,Haim Dubossarsky*

Main category: cs.CL

TL;DR: 论文提出了两种新的词汇语义变化检测指标AMD和SAMD，通过跨时间段的词汇使用局部对应关系来量化语义变化，相比传统APD和PRT方法在多种条件下表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 当前基于上下文语言模型嵌入的词汇语义变化检测主要依赖有限的语义变化度量标准（APD和PRT），需要探索更有效的替代方法。

Method: 引入Average Minimum Distance (AMD)和Symmetric Average Minimum Distance (SAMD)两种新指标，通过计算跨时间段词汇使用之间的局部对应距离来量化语义变化。

Result: 在多种语言、编码器模型和表示空间中，AMD在降维和非专门编码器条件下表现更稳健，SAMD在专门编码器上表现优异。

Conclusion: 词汇语义变化检测应考虑APD和PRT之外的替代度量标准，AMD为基于上下文嵌入的分析提供了稳健的选择。

Abstract: Lexical semantic change detection (LSCD) increasingly relies on contextualised language model embeddings, yet most approaches still quantify change using a small set of semantic change metrics, primarily Average Pairwise Distance (APD) and cosine distance over word prototypes (PRT). We introduce Average Minimum Distance (AMD) and Symmetric Average Minimum Distance (SAMD), new measures that quantify semantic change via local correspondence between word usages across time periods. Across multiple languages, encoder models, and representation spaces, we show that AMD often provides more robust performance, particularly under dimensionality reduction and with non-specialised encoders, while SAMD excels with specialised encoders. We suggest that LSCD may benefit from considering alternative semantic change metrics beyond APD and PRT, with AMD offering a robust option for contextualised embedding-based analysis.

</details>


### [74] [Causal Effect Estimation with Latent Textual Treatments](https://arxiv.org/abs/2602.15730)
*Omri Feldman,Amar Venugopal,Jann Spiess,Amir Feder*

Main category: cs.CL

TL;DR: 提出一个端到端流水线，通过稀疏自编码器生成假设和引导，结合协变量残差化方法，解决文本干预实验中因果效应估计的偏差问题


<details>
  <summary>Details</summary>
Motivation: 理解文本对下游结果的因果效应是许多应用的核心任务，但文本固有的治疗变量与协变量信息混淆问题导致传统估计方法存在显著偏差

Method: 使用稀疏自编码器进行假设生成和文本引导，采用协变量残差化方法解决估计偏差，建立端到端的文本干预生成和因果估计流水线

Result: 实证结果表明，该流水线能有效诱导目标特征的变化并减少估计误差，为文本作为治疗变量的因果效应估计提供了稳健基础

Conclusion: 所提出的方法解决了文本干预实验中的计算和统计挑战，通过系统化的文本变化生成和偏差校正，实现了更准确的因果效应估计

Abstract: Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our work first performs hypothesis generation and steering via sparse autoencoders (SAEs), followed by robust causal estimation. Our pipeline addresses both computational and statistical challenges in text-as-treatment experiments. We demonstrate that naive estimation of causal effects suffers from significant bias as text inherently conflates treatment and covariate information. We describe the estimation bias induced in this setting and propose a solution based on covariate residualization. Our empirical results show that our pipeline effectively induces variation in target features and mitigates estimation error, providing a robust foundation for causal effect estimation in text-as-treatment settings.

</details>


### [75] [Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac](https://arxiv.org/abs/2602.15753)
*Chahan Vidal-Gorène,Bastien Kindt,Florian Cafiero*

Main category: cs.CL

TL;DR: LLMs在低资源语言的词形还原和词性标注任务中表现出色，无需微调即可在few-shot设置下达到或超越传统RNN基线的性能，特别是在处理复杂形态和非拉丁文字语言时显示出巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言在自然语言处理任务中面临的持续挑战，特别是词形还原和词性标注，这些语言由于缺乏标注数据而难以应用传统方法。

Method: 使用GPT-4变体和开源Mistral模型，在few-shot和zero-shot设置下评估四种低资源语言（古希腊语、古典亚美尼亚语、古格鲁吉亚语和叙利亚语）的性能，并与基于RNN的PIE基线进行比较。采用包含对齐训练数据和域外测试数据的新基准。

Result: LLMs在大多数语言的POS标注和词形还原任务中表现出竞争性或优越性能，特别是在few-shot设置下。对于形态复杂和非拉丁文字的语言仍存在挑战，但LLMs可作为无数据情况下启动语言标注任务的有效工具。

Conclusion: 大型语言模型是低资源语言处理中可信且相关的选择，能够有效辅助标注工作，特别是在数据稀缺的情况下。

Abstract: Low-resource languages pose persistent challenges for Natural Language Processing tasks such as lemmatization and part-of-speech (POS) tagging. This paper investigates the capacity of recent large language models (LLMs), including GPT-4 variants and open-weight Mistral models, to address these tasks in few-shot and zero-shot settings for four historically and linguistically diverse under-resourced languages: Ancient Greek, Classical Armenian, Old Georgian, and Syriac. Using a novel benchmark comprising aligned training and out-of-domain test corpora, we evaluate the performance of foundation models across lemmatization and POS-tagging, and compare them with PIE, a task-specific RNN baseline. Our results demonstrate that LLMs, even without fine-tuning, achieve competitive or superior performance in POS-tagging and lemmatization across most languages in few-shot settings. Significant challenges persist for languages characterized by complex morphology and non-Latin scripts, but we demonstrate that LLMs are a credible and relevant option for initiating linguistic annotation tasks in the absence of data, serving as an effective aid for annotation.

</details>


### [76] [Beyond Binary Classification: Detecting Fine-Grained Sexism in Social Media Videos](https://arxiv.org/abs/2602.15757)
*Laura De Grazia,Danae Sánchez Villegas,Desmond Elliott,Mireia Farrús,Mariona Taulé*

Main category: cs.CL

TL;DR: FineMuSe：一个包含二进制和细粒度标注的西班牙语多模态性别歧视检测数据集，提出分层分类法并评估多种LLM在性别歧视检测中的表现，发现多模态LLM在识别细微性别歧视方面表现优异但在视觉线索处理上存在不足


<details>
  <summary>Details</summary>
Motivation: 在线性别歧视表现形式多样且检测困难，现有自动化工具多限于二元分类，缺乏细粒度和上下文敏感的标注，导致更微妙的性别歧视表现难以被检测

Method: 提出FineMuSe数据集（西班牙语多模态性别歧视检测数据集），建立包含性别歧视形式、非性别歧视以及反讽和幽默修辞手法的分层分类法，评估多种大语言模型在二元和细粒度性别歧视检测任务上的性能

Result: 多模态大语言模型在识别细微性别歧视形式方面与人类标注者表现相当，但在处理通过视觉线索传达的共现性别歧视类型时存在困难

Conclusion: 多模态方法在性别歧视检测中具有潜力，但需要进一步改进以更好地处理视觉信息中的复杂性别歧视表达，特别是在共现类型识别方面

Abstract: Online sexism appears in various forms, which makes its detection challenging. Although automated tools can enhance the identification of sexist content, they are often restricted to binary classification. Consequently, more subtle manifestations of sexism may remain undetected due to the lack of fine-grained, context-sensitive labels. To address this issue, we make the following contributions: (1) we present FineMuSe, a new multimodal sexism detection dataset in Spanish that includes both binary and fine-grained annotations; (2) we introduce a comprehensive hierarchical taxonomy that encompasses forms of sexism, non-sexism, and rhetorical devices of irony and humor; and (3) we evaluate a wide range of LLMs for both binary and fine-grained sexism detection. Our findings indicate that multimodal LLMs perform competitively with human annotators in identifying nuanced forms of sexism; however, they struggle to capture co-occurring sexist types when these are conveyed through visual cues.

</details>


### [77] [ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models](https://arxiv.org/abs/2602.15758)
*Manav Nitin Kapadnis,Lawanya Baghel,Atharva Naik,Carolyn Rosé*

Main category: cs.CL

TL;DR: ChartEditBench是一个用于评估多模态大语言模型在增量式图表编辑任务中表现的新基准，包含5000条难度控制的修改链和人工验证子集，通过执行验证、视觉相似性和代码逻辑检查来评估模型在多轮交互中的持续编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在单轮图表生成上表现良好，但在真实世界探索性数据分析所需的多轮交互编辑能力尚未充分研究，用户需要迭代细化可视化、维护共同基础并适应变化偏好。

Method: 提出ChartEditBench基准，包含难度控制的修改链和人工验证子集；建立包含执行验证、像素级视觉相似性和逻辑代码验证的鲁棒评估框架，以缓解LLM-as-a-Judge指标的局限性。

Result: 实验显示最先进MLLM在多轮设置中表现显著下降，主要由于错误累积和共享上下文崩溃，在样式编辑上表现良好但在数据为中心转换上频繁出现执行失败。

Conclusion: ChartEditBench为基于意图感知的多模态编程建立了具有挑战性的测试平台，揭示了当前MLLM在多轮图表编辑任务中的局限性，特别是上下文维护和数据转换能力不足的问题。

Abstract: While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset. Unlike prior one-shot benchmarks, ChartEditBench evaluates sustained, context-aware editing. We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Experiments with state-of-the-art MLLMs reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, with strong performance on stylistic edits but frequent execution failures on data-centric transformations. ChartEditBench, establishes a challenging testbed for grounded, intent-aware multimodal programming.

</details>


### [78] [ViTaB-A: Evaluating Multimodal Large Language Models on Visual Table Attribution](https://arxiv.org/abs/2602.15769)
*Yahia Alqurnawi,Preetom Biswas,Anmol Rao,Tejas Anvekar,Chitta Baral,Vivek Gupta*

Main category: cs.CL

TL;DR: 多模态大语言模型在结构化数据问答中的归因能力研究，发现模型在回答问题时准确率尚可，但在证据归因（指向具体行列）方面表现很差，特别是在JSON格式中接近随机水平，限制了在需要透明度应用中的使用。


<details>
  <summary>Details</summary>
Motivation: 用户需要了解多模态大语言模型在回答结构化数据问题时答案的来源依据，研究模型是否能够准确指向支持答案的具体行和列。

Method: 评估多个多模态大语言模型在不同表格格式（Markdown、JSON、图像）和提示策略下的表现，比较问答准确率和证据归因准确率。

Result: 问答准确率中等，但归因准确率显著更低（JSON输入接近随机水平）；模型在引用行方面比列更可靠；文本格式比图像格式表现更差；不同模型家族存在显著差异。

Conclusion: 当前多模态大语言模型在提供细粒度可信归因方面不可靠，这限制了它们在需要透明度和可追溯性应用中的使用。

Abstract: Multimodal Large Language Models (mLLMs) are often used to answer questions in structured data such as tables in Markdown, JSON, and images. While these models can often give correct answers, users also need to know where those answers come from. In this work, we study structured data attribution/citation, which is the ability of the models to point to the specific rows and columns that support an answer. We evaluate several mLLMs across different table formats and prompting strategies. Our results show a clear gap between question answering and evidence attribution. Although question answering accuracy remains moderate, attribution accuracy is much lower, near random for JSON inputs, across all models. We also find that models are more reliable at citing rows than columns, and struggle more with textual formats than images. Finally, we observe notable differences across model families. Overall, our findings show that current mLLMs are unreliable at providing fine-grained, trustworthy attribution for structured data, which limits their usage in applications requiring transparency and traceability.

</details>


### [79] [*-PLUIE: Personalisable metric with Llm Used for Improved Evaluation](https://arxiv.org/abs/2602.15778)
*Quentin Lemesle,Léane Jourdan,Daisy Munson,Pierre Alain,Jonathan Chevelu,Arnaud Delhay,Damien Lolive*

Main category: cs.CL

TL;DR: 提出*-PLUIE方法，这是ParaPLUIE的任务特定提示变体，用于评估自动生成文本质量，在保持低计算成本的同时获得与人类评分更强的相关性


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-judge方法虽然有效，但计算成本高且需要后处理，需要更高效的文本质量评估方法

Method: 基于ParaPLUIE（基于困惑度的LLM-judge指标）构建*-PLUIE，通过任务特定提示变体来估计Yes/No答案的置信度，无需生成文本

Result: 实验表明个性化*-PLUIE在保持低计算成本的同时，与人类评分具有更强的相关性

Conclusion: *-PLUIE方法为自动文本质量评估提供了计算效率高且与人类判断对齐的有效解决方案

Abstract: Evaluating the quality of automatically generated text often relies on LLM-as-a-judge (LLM-judge) methods. While effective, these approaches are computationally expensive and require post-processing. To address these limitations, we build upon ParaPLUIE, a perplexity-based LLM-judge metric that estimates confidence over ``Yes/No'' answers without generating text. We introduce *-PLUIE, task specific prompting variants of ParaPLUIE and evaluate their alignment with human judgement. Our experiments show that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost.

</details>


### [80] [Avey-B](https://arxiv.org/abs/2602.15814)
*Devang Acharya,Mohammad Hammoud*

Main category: cs.CL

TL;DR: 本文将Avey模型重新设计为仅编码器架构，通过解耦静态动态参数化、稳定性导向归一化和神经压缩等创新，在保持计算效率的同时，在标记分类和信息检索任务上优于主流Transformer编码器。


<details>
  <summary>Details</summary>
Motivation: 在计算和内存受限的工业NLP场景中，需要紧凑高效的预训练双向编码器。虽然自注意力机制能提供高质量双向上下文建模，但需要寻找更高效的注意力替代方案。

Method: 将自回归的Avey模型重构为仅编码器范式，引入：1）解耦的静态和动态参数化；2）稳定性导向的归一化技术；3）神经压缩方法。

Result: 在标准标记分类和信息检索基准测试中，该架构持续优于四种广泛使用的基于Transformer的编码器，且在长上下文场景中扩展效率更高。

Conclusion: 重新设计的Avey编码器架构提供了比传统Transformer更高效的替代方案，在保持性能优势的同时具有更好的可扩展性，适合工业部署。

Abstract: Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.

</details>
