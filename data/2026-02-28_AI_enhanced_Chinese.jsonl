{"id": "2602.23103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23103", "abs": "https://arxiv.org/abs/2602.23103", "authors": ["Fuhao Zhang", "Lei Liu", "Jialin Zhang", "Ya-Nan Zhang", "Nan Mu"], "title": "SpectralMamba-UNet: Frequency-Disentangled State Space Modeling for Texture-Structure Consistent Medical Image Segmentation", "comment": null, "summary": "Accurate medical image segmentation requires effective modeling of both global anatomical structures and fine-grained boundary details. Recent state space models (e.g., Vision Mamba) offer efficient long-range dependency modeling. However, their one-dimensional serialization weakens local spatial continuity and high-frequency representation. To this end, we propose SpectralMamba-UNet, a novel frequency-disentangled framework to decouple the learning of structural and textural information in the spectral domain. Our Spectral Decomposition and Modeling (SDM) module applies discrete cosine transform to decompose low- and high-frequency features, where low frequency contributes to global contextual modeling via a frequency-domain Mamba and high frequency preserves boundary-sensitive details. To balance spectral contributions, we introduce a Spectral Channel Reweighting (SCR) mechanism to form channel-wise frequency-aware attention, and a Spectral-Guided Fusion (SGF) module to achieve adaptively multi-scale fusion in the decoder. Experiments on five public benchmarks demonstrate consistent improvements across diverse modalities and segmentation targets, validating the effectiveness and generalizability of our approach.", "AI": {"tldr": "SpectralMamba-UNet\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u9891\u7387\u89e3\u8026\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u57df\u5206\u89e3\u5c06\u7ed3\u6784\u4fe1\u606f\u4e0e\u7eb9\u7406\u4fe1\u606f\u5206\u79bb\u5b66\u4e60\uff0c\u5229\u7528Mamba\u6a21\u578b\u5904\u7406\u4f4e\u9891\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u9ad8\u9891\u4fdd\u7559\u8fb9\u754c\u7ec6\u8282\uff0c\u5e76\u5f15\u5165\u901a\u9053\u91cd\u52a0\u6743\u548c\u5f15\u5bfc\u878d\u5408\u673a\u5236\u5b9e\u73b0\u591a\u5c3a\u5ea6\u81ea\u9002\u5e94\u878d\u5408\u3002", "motivation": "\u73b0\u6709\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982Vision Mamba\uff09\u867d\u7136\u80fd\u6709\u6548\u5efa\u6a21\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u5176\u4e00\u7ef4\u5e8f\u5217\u5316\u5904\u7406\u524a\u5f31\u4e86\u5c40\u90e8\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u9ad8\u9891\u8868\u793a\u80fd\u529b\uff0c\u65e0\u6cd5\u540c\u65f6\u6709\u6548\u5904\u7406\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5168\u5c40\u89e3\u5256\u7ed3\u6784\u548c\u7ec6\u7c92\u5ea6\u8fb9\u754c\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u9891\u8c31\u5206\u89e3\u4e0e\u5efa\u6a21\uff08SDM\uff09\u6a21\u5757\uff0c\u5e94\u7528\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u5206\u89e3\u4f4e\u9891\u548c\u9ad8\u9891\u7279\u5f81\uff1a\u4f4e\u9891\u901a\u8fc7\u9891\u57dfMamba\u8fdb\u884c\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u9ad8\u9891\u4fdd\u7559\u8fb9\u754c\u654f\u611f\u7ec6\u8282\u3002\u5f15\u5165\u9891\u8c31\u901a\u9053\u91cd\u52a0\u6743\uff08SCR\uff09\u673a\u5236\u5f62\u6210\u901a\u9053\u7ea7\u9891\u7387\u611f\u77e5\u6ce8\u610f\u529b\uff0c\u4ee5\u53ca\u9891\u8c31\u5f15\u5bfc\u878d\u5408\uff08SGF\uff09\u6a21\u5757\u5b9e\u73b0\u89e3\u7801\u5668\u4e2d\u7684\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u878d\u5408\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6a21\u6001\u548c\u5206\u5272\u76ee\u6807\u4e0a\u5747\u83b7\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SpectralMamba-UNet\u901a\u8fc7\u9891\u57df\u89e3\u8026\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5168\u5c40\u7ed3\u6784\u4e0e\u5c40\u90e8\u7ec6\u8282\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u8f83\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.23114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23114", "abs": "https://arxiv.org/abs/2602.23114", "authors": ["Xudong Yan", "Songhe Feng", "Jiaxin Wang", "Xin Su", "Yi Jin"], "title": "WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning", "comment": null, "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual prototypes from historical images for inference. Since the model tends to favor compositions already stored in the queue during testing, we warm-start the queue by initializing it with training images for visual prototypes of seen compositions and generating unseen visual prototypes using the mapping learned between seen and unseen textual prototypes. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. To provide a more reliable evaluation for CZSL, we introduce a new benchmark dataset, C-Fashion, and refine the widely used but noisy MIT-States dataset. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. The source code and datasets are available at https://github.com/xud-yan/WARM-CAT .", "code_url": "https://github.com/xud-yan/WARM-CAT", "code_stars": 0, "code_last_update": "2025-12-23", "AI": {"tldr": "\u63d0\u51faWARM-CAT\u65b9\u6cd5\uff0c\u901a\u8fc7\u7d2f\u79ef\u65e0\u76d1\u7763\u6570\u636e\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u6765\u66f4\u65b0\u6d4b\u8bd5\u65f6\u7684\u539f\u578b\uff0c\u89e3\u51b3\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CZSL\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u56e0\u5305\u542b\u672a\u89c1\u5c5e\u6027-\u5bf9\u8c61\u7ec4\u5408\u800c\u5bfc\u81f4\u6807\u7b7e\u7a7a\u95f4\u5206\u5e03\u504f\u79fb\uff0c\u9020\u6210\u6027\u80fd\u4e0b\u964d\u3002", "method": "1) \u7d2f\u79ef\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u65e0\u76d1\u7763\u77e5\u8bc6\u66f4\u65b0\u591a\u6a21\u6001\u539f\u578b\uff1b2) \u8bbe\u8ba1\u81ea\u9002\u5e94\u66f4\u65b0\u6743\u91cd\u63a7\u5236\u539f\u578b\u8c03\u6574\u7a0b\u5ea6\uff1b3) \u5f15\u5165\u52a8\u6001\u4f18\u5148\u7ea7\u961f\u5217\u5b58\u50a8\u9ad8\u7f6e\u4fe1\u5ea6\u56fe\u50cf\uff1b4) \u901a\u8fc7\u591a\u6a21\u6001\u534f\u540c\u8868\u793a\u5b66\u4e60\u5bf9\u9f50\u6587\u672c\u548c\u89c6\u89c9\u539f\u578b\uff1b5) \u521b\u5efaC-Fashion\u6570\u636e\u96c6\u5e76\u4f18\u5316MIT-States\u6570\u636e\u96c6\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u65e0\u8bba\u662f\u5c01\u95ed\u4e16\u754c\u8fd8\u662f\u5f00\u653e\u4e16\u754c\u8bbe\u7f6e\u4e0b\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u6001\u77e5\u8bc6\u7d2f\u79ef\u548c\u81ea\u9002\u5e94\u539f\u578b\u66f4\u65b0\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86CZSL\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u4e3a\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23117", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23117", "abs": "https://arxiv.org/abs/2602.23117", "authors": ["Xiaosen Wang", "Zhijin Ge", "Bohan Liu", "Zheng Fang", "Fengfan Zhou", "Ruixuan Zhang", "Shaokang Wang", "Yuyang Luo"], "title": "Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation", "comment": "Code is available at https://github.com/Trustworthy-AI-Group/TransferAttack", "summary": "Adversarial transferability refers to the capacity of adversarial examples generated on the surrogate model to deceive alternate, unexposed victim models. This property eliminates the need for direct access to the victim model during an attack, thereby raising considerable security concerns in practical applications and attracting substantial research attention recently. In this work, we discern a lack of a standardized framework and criteria for evaluating transfer-based attacks, leading to potentially biased assessments of existing approaches. To rectify this gap, we have conducted an exhaustive review of hundreds of related works, organizing various transfer-based attacks into six distinct categories. Subsequently, we propose a comprehensive framework designed to serve as a benchmark for evaluating these attacks. In addition, we delineate common strategies that enhance adversarial transferability and highlight prevalent issues that could lead to unfair comparisons. Finally, we provide a brief review of transfer-based attacks beyond image classification.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5bf9\u6297\u6837\u672c\u8fc1\u79fb\u6027\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u8fc1\u79fb\u653b\u51fb\u5206\u4e3a\u516d\u7c7b\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u4e0d\u516c\u5e73\u6bd4\u8f83\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30\u8fc1\u79fb\u653b\u51fb\u7684\u6807\u51c6\u5316\u6846\u67b6\u548c\u6807\u51c6\uff0c\u5bfc\u81f4\u5bf9\u73b0\u6709\u65b9\u6cd5\u7684\u8bc4\u4f30\u53ef\u80fd\u5b58\u5728\u504f\u5dee\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f15\u53d1\u4e86\u4e25\u91cd\u7684\u5b89\u5168\u62c5\u5fe7\u3002", "method": "\u901a\u8fc7\u5bf9\u6570\u767e\u7bc7\u76f8\u5173\u6587\u732e\u7684\u5168\u9762\u56de\u987e\uff0c\u5c06\u5404\u79cd\u57fa\u4e8e\u8fc1\u79fb\u7684\u653b\u51fb\u7ec4\u7ec7\u6210\u516d\u4e2a\u4e0d\u540c\u7c7b\u522b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u4f5c\u4e3a\u57fa\u51c6\u3002", "result": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u660e\u786e\u4e86\u589e\u5f3a\u5bf9\u6297\u8fc1\u79fb\u6027\u7684\u5e38\u89c1\u7b56\u7565\uff0c\u6307\u51fa\u4e86\u53ef\u80fd\u5bfc\u81f4\u4e0d\u516c\u5e73\u6bd4\u8f83\u7684\u666e\u904d\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u56fe\u50cf\u5206\u7c7b\u4e4b\u5916\u8fc1\u79fb\u653b\u51fb\u7684\u7b80\u8981\u56de\u987e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8fc1\u79fb\u653b\u51fb\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u66f4\u516c\u5e73\u3001\u51c6\u786e\u5730\u8bc4\u4f30\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u5bf9\u6297\u8fc1\u79fb\u6027\u7814\u7a76\u7684\u89c4\u8303\u5316\u53d1\u5c55\u3002"}}
{"id": "2602.23120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23120", "abs": "https://arxiv.org/abs/2602.23120", "authors": ["Arian Sabaghi", "Jos\u00e9 Oramas"], "title": "TriLite: Efficient Weakly Supervised Object Localization with Universal Visual Features and Tri-Region Disentanglement", "comment": "This paper consists of 8 pages including 6 figures. Accepted at CVPR 2026", "summary": "Weakly supervised object localization (WSOL) aims to localize target objects in images using only image-level labels. Despite recent progress, many approaches still rely on multi-stage pipelines or full fine-tuning of large backbones, which increases training cost, while the broader WSOL community continues to face the challenge of partial object coverage. We present TriLite, a single-stage WSOL framework that leverages a frozen Vision Transformer with Dinov2 pre-training in a self-supervised manner, and introduces only a minimal number of trainable parameters (fewer than 800K on ImageNet-1K) for both classification and localization. At its core is the proposed TriHead module, which decomposes patch features into foreground, background, and ambiguous regions, thereby improving object coverage while suppressing spurious activations. By disentangling classification and localization objectives, TriLite effectively exploits the universal representations learned by self-supervised ViTs without requiring expensive end-to-end training. Extensive experiments on CUB-200-2011, ImageNet-1K, and OpenImages demonstrate that TriLite sets a new state of the art, while remaining significantly more parameter-efficient and easier to train than prior methods. The code will be released soon.", "AI": {"tldr": "TriLite\u662f\u4e00\u4e2a\u5355\u9636\u6bb5\u5f31\u76d1\u7763\u76ee\u6807\u5b9a\u4f4d\u6846\u67b6\uff0c\u4f7f\u7528\u51bb\u7ed3\u7684Dinov2\u9884\u8bad\u7ec3ViT\uff0c\u4ec5\u5f15\u5165\u4e0d\u523080\u4e07\u53c2\u6570\uff0c\u901a\u8fc7TriHead\u6a21\u5757\u5206\u89e3\u7279\u5f81\u533a\u57df\uff0c\u5b9e\u73b0\u9ad8\u6548\u5206\u7c7b\u548c\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709WSOL\u65b9\u6cd5\u591a\u9636\u6bb5\u6d41\u7a0b\u548c\u5168\u5fae\u8c03\u5927\u6a21\u578b\u5e26\u6765\u7684\u9ad8\u8bad\u7ec3\u6210\u672c\uff0c\u4ee5\u53ca\u90e8\u5206\u76ee\u6807\u8986\u76d6\u95ee\u9898\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u65b9\u5f0f\u5229\u7528\u51bb\u7ed3\u7684Vision Transformer\uff08Dinov2\u9884\u8bad\u7ec3\uff09\uff0c\u5f15\u5165TriHead\u6a21\u5757\u5c06\u8865\u4e01\u7279\u5f81\u5206\u89e3\u4e3a\u524d\u666f\u3001\u80cc\u666f\u548c\u6a21\u7cca\u533a\u57df\uff0c\u5206\u79bb\u5206\u7c7b\u548c\u5b9a\u4f4d\u76ee\u6807\u3002", "result": "\u5728CUB-200-2011\u3001ImageNet-1K\u548cOpenImages\u4e0a\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u53c2\u6570\u91cf\u663e\u8457\u51cf\u5c11\u4e14\u8bad\u7ec3\u66f4\u7b80\u5355\u3002", "conclusion": "TriLite\u901a\u8fc7\u6709\u6548\u5229\u7528\u81ea\u76d1\u7763ViT\u7684\u901a\u7528\u8868\u793a\uff0c\u65e0\u9700\u6602\u8d35\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u5f31\u76d1\u7763\u76ee\u6807\u5b9a\u4f4d\u3002"}}
{"id": "2602.23133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23133", "abs": "https://arxiv.org/abs/2602.23133", "authors": ["Xin Yuan", "Zhiyong Zhang", "Xin Xu", "Zheng Wang", "Chia-Wen Lin"], "title": "From Calibration to Refinement: Seeking Certainty via Probabilistic Evidence Propagation for Noisy-Label Person Re-Identification", "comment": "Accepted by IEEE TMM 2026", "summary": "With the increasing demand for robust person Re-ID in unconstrained environments, learning from datasets with noisy labels and sparse per-identity samples remains a critical challenge. Existing noise-robust person Re-ID methods primarily rely on loss-correction or sample-selection strategies using softmax outputs. However, these methods suffer from two key limitations: 1) Softmax exhibits translation invariance, leading to over-confident and unreliable predictions on corrupted labels. 2) Conventional sample selection based on small-loss criteria often discards valuable hard positives that are crucial for learning discriminative features. To overcome these issues, we propose the CAlibration-to-REfinement (CARE) method, a two-stage framework that seeks certainty through probabilistic evidence propagation from calibration to refinement. In the calibration stage, we propose the probabilistic evidence calibration (PEC) that dismantles softmax translation invariance by injecting adaptive learnable parameters into the similarity function, and employs an evidential calibration loss to mitigate overconfidence on mislabeled samples. In the refinement stage, we design the evidence propagation refinement (EPR) that can more accurately distinguish between clean and noisy samples. Specifically, the EPR contains two steps: Firstly, the composite angular margin (CAM) metric is proposed to precisely distinguish clean but hard-to-learn positive samples from mislabeled ones in a hyperspherical space; Secondly, the certainty-oriented sphere weighting (COSW) is developed to dynamically allocate the importance of samples according to CAM, ensuring clean instances drive model updates. Extensive experimental results on Market1501, DukeMTMC-ReID, and CUHK03 datasets under both random and patterned noises show that CARE achieves competitive performance.", "AI": {"tldr": "CARE\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u566a\u58f0\u9c81\u68d2\u884c\u4eba\u91cd\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u8bc1\u636e\u4f20\u64ad\u4ece\u6821\u51c6\u5230\u7cbe\u70bc\u6765\u89e3\u51b3\u8f6f\u6700\u5927\u51fd\u6570\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\u548c\u5c0f\u635f\u5931\u6837\u672c\u9009\u62e9\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u566a\u58f0\u9c81\u68d2\u884c\u4eba\u91cd\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u8f6f\u6700\u5927\u8f93\u51fa\u8fdb\u884c\u635f\u5931\u6821\u6b63\u6216\u6837\u672c\u9009\u62e9\uff0c\u4f46\u5b58\u5728\u5e73\u79fb\u4e0d\u53d8\u6027\u5bfc\u81f4\u5bf9\u566a\u58f0\u6807\u7b7e\u8fc7\u5ea6\u81ea\u4fe1\u9884\u6d4b\uff0c\u4ee5\u53ca\u5c0f\u635f\u5931\u51c6\u5219\u4e22\u5f03\u91cd\u8981\u96be\u6b63\u6837\u672c\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1)\u6821\u51c6\u9636\u6bb5\u4f7f\u7528\u6982\u7387\u8bc1\u636e\u6821\u51c6(PEC)\u6253\u7834\u8f6f\u6700\u5927\u5e73\u79fb\u4e0d\u53d8\u6027\u5e76\u7f13\u89e3\u8bef\u6807\u6837\u672c\u7684\u8fc7\u5ea6\u81ea\u4fe1\uff1b2)\u7cbe\u70bc\u9636\u6bb5\u4f7f\u7528\u8bc1\u636e\u4f20\u64ad\u7cbe\u70bc(EPR)\uff0c\u901a\u8fc7\u590d\u5408\u89d2\u5ea6\u8fb9\u754c(CAM)\u5ea6\u91cf\u7cbe\u786e\u533a\u5206\u96be\u6b63\u6837\u672c\u548c\u8bef\u6807\u6837\u672c\uff0c\u5e76\u4f7f\u7528\u9762\u5411\u786e\u5b9a\u6027\u7684\u7403\u9762\u52a0\u6743(COSW)\u52a8\u6001\u5206\u914d\u6837\u672c\u91cd\u8981\u6027\u3002", "result": "\u5728Market1501\u3001DukeMTMC-ReID\u548cCUHK03\u6570\u636e\u96c6\u4e0a\u7684\u968f\u673a\u548c\u6a21\u5f0f\u566a\u58f0\u5b9e\u9a8c\u663e\u793a\uff0cCARE\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "CARE\u65b9\u6cd5\u901a\u8fc7\u6982\u7387\u8bc1\u636e\u4f20\u64ad\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u6807\u7b7e\u73af\u5883\u4e0b\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u96be\u6b63\u6837\u672c\u548c\u8bef\u6807\u6837\u672c\u533a\u5206\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
