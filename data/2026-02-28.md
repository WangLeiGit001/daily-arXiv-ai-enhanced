<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SpectralMamba-UNet: Frequency-Disentangled State Space Modeling for Texture-Structure Consistent Medical Image Segmentation](https://arxiv.org/abs/2602.23103)
*Fuhao Zhang,Lei Liu,Jialin Zhang,Ya-Nan Zhang,Nan Mu*

Main category: cs.CV

TL;DR: SpectralMamba-UNet是一个新颖的频率解耦医学图像分割框架，通过频域分解将结构信息与纹理信息分离学习，利用Mamba模型处理低频全局上下文，高频保留边界细节，并引入通道重加权和引导融合机制实现多尺度自适应融合。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型（如Vision Mamba）虽然能有效建模长程依赖关系，但其一维序列化处理削弱了局部空间连续性和高频表示能力，无法同时有效处理医学图像分割中的全局解剖结构和细粒度边界细节。

Method: 提出频谱分解与建模（SDM）模块，应用离散余弦变换分解低频和高频特征：低频通过频域Mamba进行全局上下文建模，高频保留边界敏感细节。引入频谱通道重加权（SCR）机制形成通道级频率感知注意力，以及频谱引导融合（SGF）模块实现解码器中的自适应多尺度融合。

Result: 在五个公共基准测试上进行实验，结果显示该方法在不同模态和分割目标上均获得一致性能提升，验证了方法的有效性和泛化能力。

Conclusion: SpectralMamba-UNet通过频域解耦策略成功解决了医学图像分割中全局结构与局部细节的平衡问题，为多模态医学图像分割提供了有效解决方案，具有较好的通用性和实用性。

Abstract: Accurate medical image segmentation requires effective modeling of both global anatomical structures and fine-grained boundary details. Recent state space models (e.g., Vision Mamba) offer efficient long-range dependency modeling. However, their one-dimensional serialization weakens local spatial continuity and high-frequency representation. To this end, we propose SpectralMamba-UNet, a novel frequency-disentangled framework to decouple the learning of structural and textural information in the spectral domain. Our Spectral Decomposition and Modeling (SDM) module applies discrete cosine transform to decompose low- and high-frequency features, where low frequency contributes to global contextual modeling via a frequency-domain Mamba and high frequency preserves boundary-sensitive details. To balance spectral contributions, we introduce a Spectral Channel Reweighting (SCR) mechanism to form channel-wise frequency-aware attention, and a Spectral-Guided Fusion (SGF) module to achieve adaptively multi-scale fusion in the decoder. Experiments on five public benchmarks demonstrate consistent improvements across diverse modalities and segmentation targets, validating the effectiveness and generalizability of our approach.

</details>


### [2] [WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning](https://arxiv.org/abs/2602.23114)
*Xudong Yan,Songhe Feng,Jiaxin Wang,Xin Su,Yi Jin*

Main category: cs.CV

TL;DR: 提出WARM-CAT方法，通过累积无监督数据的多模态知识来更新测试时的原型，解决组合零样本学习中的分布偏移问题，在多个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有CZSL方法在测试时因包含未见属性-对象组合而导致标签空间分布偏移，造成性能下降。

Method: 1) 累积文本和视觉模态的无监督知识更新多模态原型；2) 设计自适应更新权重控制原型调整程度；3) 引入动态优先级队列存储高置信度图像；4) 通过多模态协同表示学习对齐文本和视觉原型；5) 创建C-Fashion数据集并优化MIT-States数据集。

Result: 在四个基准数据集上，无论是封闭世界还是开放世界设置下都取得了最先进的性能。

Conclusion: 该方法通过多模态知识累积和自适应原型更新机制，有效解决了CZSL中的分布偏移问题，为组合零样本学习提供了新的解决方案。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual prototypes from historical images for inference. Since the model tends to favor compositions already stored in the queue during testing, we warm-start the queue by initializing it with training images for visual prototypes of seen compositions and generating unseen visual prototypes using the mapping learned between seen and unseen textual prototypes. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. To provide a more reliable evaluation for CZSL, we introduce a new benchmark dataset, C-Fashion, and refine the widely used but noisy MIT-States dataset. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. The source code and datasets are available at https://github.com/xud-yan/WARM-CAT .

</details>


### [3] [Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation](https://arxiv.org/abs/2602.23117)
*Xiaosen Wang,Zhijin Ge,Bohan Liu,Zheng Fang,Fengfan Zhou,Ruixuan Zhang,Shaokang Wang,Yuyang Luo*

Main category: cs.CV

TL;DR: 本文系统综述了对抗样本迁移性研究，提出了标准化评估框架，将迁移攻击分为六类，并指出了当前研究中存在的不公平比较问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估迁移攻击的标准化框架和标准，导致对现有方法的评估可能存在偏差，这在实际应用中引发了严重的安全担忧。

Method: 通过对数百篇相关文献的全面回顾，将各种基于迁移的攻击组织成六个不同类别，并提出了一个全面的评估框架作为基准。

Result: 提出了统一的评估框架，明确了增强对抗迁移性的常见策略，指出了可能导致不公平比较的普遍问题，并提供了图像分类之外迁移攻击的简要回顾。

Conclusion: 该研究为迁移攻击评估提供了标准化基准，有助于更公平、准确地评估不同方法的性能，推动对抗迁移性研究的规范化发展。

Abstract: Adversarial transferability refers to the capacity of adversarial examples generated on the surrogate model to deceive alternate, unexposed victim models. This property eliminates the need for direct access to the victim model during an attack, thereby raising considerable security concerns in practical applications and attracting substantial research attention recently. In this work, we discern a lack of a standardized framework and criteria for evaluating transfer-based attacks, leading to potentially biased assessments of existing approaches. To rectify this gap, we have conducted an exhaustive review of hundreds of related works, organizing various transfer-based attacks into six distinct categories. Subsequently, we propose a comprehensive framework designed to serve as a benchmark for evaluating these attacks. In addition, we delineate common strategies that enhance adversarial transferability and highlight prevalent issues that could lead to unfair comparisons. Finally, we provide a brief review of transfer-based attacks beyond image classification.

</details>


### [4] [TriLite: Efficient Weakly Supervised Object Localization with Universal Visual Features and Tri-Region Disentanglement](https://arxiv.org/abs/2602.23120)
*Arian Sabaghi,José Oramas*

Main category: cs.CV

TL;DR: TriLite是一个单阶段弱监督目标定位框架，使用冻结的Dinov2预训练ViT，仅引入不到80万参数，通过TriHead模块分解特征区域，实现高效分类和定位。


<details>
  <summary>Details</summary>
Motivation: 解决现有WSOL方法多阶段流程和全微调大模型带来的高训练成本，以及部分目标覆盖问题。

Method: 采用自监督方式利用冻结的Vision Transformer（Dinov2预训练），引入TriHead模块将补丁特征分解为前景、背景和模糊区域，分离分类和定位目标。

Result: 在CUB-200-2011、ImageNet-1K和OpenImages上达到新的最先进性能，参数量显著减少且训练更简单。

Conclusion: TriLite通过有效利用自监督ViT的通用表示，无需昂贵的端到端训练，实现了参数高效且性能优越的弱监督目标定位。

Abstract: Weakly supervised object localization (WSOL) aims to localize target objects in images using only image-level labels. Despite recent progress, many approaches still rely on multi-stage pipelines or full fine-tuning of large backbones, which increases training cost, while the broader WSOL community continues to face the challenge of partial object coverage. We present TriLite, a single-stage WSOL framework that leverages a frozen Vision Transformer with Dinov2 pre-training in a self-supervised manner, and introduces only a minimal number of trainable parameters (fewer than 800K on ImageNet-1K) for both classification and localization. At its core is the proposed TriHead module, which decomposes patch features into foreground, background, and ambiguous regions, thereby improving object coverage while suppressing spurious activations. By disentangling classification and localization objectives, TriLite effectively exploits the universal representations learned by self-supervised ViTs without requiring expensive end-to-end training. Extensive experiments on CUB-200-2011, ImageNet-1K, and OpenImages demonstrate that TriLite sets a new state of the art, while remaining significantly more parameter-efficient and easier to train than prior methods. The code will be released soon.

</details>


### [5] [From Calibration to Refinement: Seeking Certainty via Probabilistic Evidence Propagation for Noisy-Label Person Re-Identification](https://arxiv.org/abs/2602.23133)
*Xin Yuan,Zhiyong Zhang,Xin Xu,Zheng Wang,Chia-Wen Lin*

Main category: cs.CV

TL;DR: CARE是一个两阶段噪声鲁棒行人重识别方法，通过概率证据传播从校准到精炼来解决软最大函数的平移不变性和小损失样本选择策略的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有噪声鲁棒行人重识别方法依赖软最大输出进行损失校正或样本选择，但存在平移不变性导致对噪声标签过度自信预测，以及小损失准则丢弃重要难正样本的问题。

Method: 提出两阶段框架：1)校准阶段使用概率证据校准(PEC)打破软最大平移不变性并缓解误标样本的过度自信；2)精炼阶段使用证据传播精炼(EPR)，通过复合角度边界(CAM)度量精确区分难正样本和误标样本，并使用面向确定性的球面加权(COSW)动态分配样本重要性。

Result: 在Market1501、DukeMTMC-ReID和CUHK03数据集上的随机和模式噪声实验显示，CARE取得了竞争性性能。

Conclusion: CARE方法通过概率证据传播有效解决了噪声标签环境下的行人重识别问题，特别是在处理难正样本和误标样本区分方面表现出色。

Abstract: With the increasing demand for robust person Re-ID in unconstrained environments, learning from datasets with noisy labels and sparse per-identity samples remains a critical challenge. Existing noise-robust person Re-ID methods primarily rely on loss-correction or sample-selection strategies using softmax outputs. However, these methods suffer from two key limitations: 1) Softmax exhibits translation invariance, leading to over-confident and unreliable predictions on corrupted labels. 2) Conventional sample selection based on small-loss criteria often discards valuable hard positives that are crucial for learning discriminative features. To overcome these issues, we propose the CAlibration-to-REfinement (CARE) method, a two-stage framework that seeks certainty through probabilistic evidence propagation from calibration to refinement. In the calibration stage, we propose the probabilistic evidence calibration (PEC) that dismantles softmax translation invariance by injecting adaptive learnable parameters into the similarity function, and employs an evidential calibration loss to mitigate overconfidence on mislabeled samples. In the refinement stage, we design the evidence propagation refinement (EPR) that can more accurately distinguish between clean and noisy samples. Specifically, the EPR contains two steps: Firstly, the composite angular margin (CAM) metric is proposed to precisely distinguish clean but hard-to-learn positive samples from mislabeled ones in a hyperspherical space; Secondly, the certainty-oriented sphere weighting (COSW) is developed to dynamically allocate the importance of samples according to CAM, ensuring clean instances drive model updates. Extensive experimental results on Market1501, DukeMTMC-ReID, and CUHK03 datasets under both random and patterned noises show that CARE achieves competitive performance.

</details>
