<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 204]
- [cs.CL](#cs.CL) [Total: 96]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scalable spatial point process models for forensic footwear analysis](https://arxiv.org/abs/2602.07006)
*Alokesh Manna,Neil Spencer,Dipak K. Dey*

Main category: cs.CV

TL;DR: 基于分层贝叶斯模型开发鞋印偶然特征量化方法，通过潜在高斯模型和空间变化系数提升法证鞋印分析的准确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 鞋印证据在法证调查中至关重要，但同款鞋数量庞大，需要通过鞋底磨损等偶然特征进行个体化区分。现有方法需要量化偶然特征模式的稀有性以准确评估证据强度

Method: 开发分层贝叶斯模型，采用潜在高斯模型框架实现大规模标注鞋印数据的高效推断，并引入空间变化系数建模鞋底花纹模式与偶然特征位置的关系

Result: 在保留数据上表现出优越性能，提高了鞋印分析的准确性和可靠性

Conclusion: 该方法通过统计建模创新有效解决了法证鞋印个体化识别中的偶然特征量化问题，为法证科学提供了更可靠的分析工具

Abstract: Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.

</details>


### [2] [Where Not to Learn: Prior-Aligned Training with Subset-based Attribution Constraints for Reliable Decision-Making](https://arxiv.org/abs/2602.07008)
*Ruoyu Chen,Shangquan Sun,Xiaoqing Guo,Sanyi Zhang,Kangwei Liu,Shiming Liu,Zhangcheng Wang,Qunli Zhang,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出基于归因的人类先验对齐方法，通过约束模型在训练过程中依赖人类指定的证据区域来提高模型决策的可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习仅提供类别标签，导致模型可能通过捷径相关性而非预期证据实现高准确率，需要将模型行为与人类先验知识对齐。

Method: 将人类先验编码为模型应依赖的输入区域（如边界框），利用基于子集选择的归因方法暴露模型决策证据，当归因区域偏离先验区域时施加惩罚，通过训练目标实现归因约束。

Result: 在图像分类和点击决策任务中验证，人类先验对齐方法在传统分类和自回归生成设置下均能提高任务准确性并增强决策合理性。

Conclusion: 基于归因的人类先验对齐方法有效解决了模型依赖错误证据的问题，提升了模型的可信度和性能。

Abstract: Reliable models should not only predict correctly, but also justify decisions with acceptable evidence. Yet conventional supervised learning typically provides only class-level labels, allowing models to achieve high accuracy through shortcut correlations rather than the intended evidence. Human priors can help constrain such behavior, but aligning models to these priors remains challenging because learned representations often diverge from human perception. To address this challenge, we propose an attribution-based human prior alignment method. We encode human priors as input regions that the model is expected to rely on (e.g., bounding boxes), and leverage a highly faithful subset-selection-based attribution approach to expose the model's decision evidence during training. When the attribution region deviates substantially from the prior regions, we penalize reliance on off-prior evidence, encouraging the model to shift its attribution toward the intended regions. This is achieved through a training objective that imposes attribution constraints induced by the human prior. We validate our method on both image classification and click decision tasks in MLLM-based GUI agent models. Across conventional classification and autoregressive generation settings, human prior alignment consistently improves task accuracy while also enhancing the model's decision reasonability.

</details>


### [3] [MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation](https://arxiv.org/abs/2602.07011)
*Zhuonan Wang,Zhenxuan Fan,Siwen Tan,Yu Zhong,Yuqian Yuan,Haoyuan Li,Hao Jiang,Wenqiao Zhang,Feifei Shao,Hongwei Wang,Jun Xiao*

Main category: cs.CV

TL;DR: MAU-Set数据集和MAU-GPT模型为工业异常检测提供全面解决方案，通过多领域数据集和新型AMoE-LoRA机制显著提升检测性能


<details>
  <summary>Details</summary>
Motivation: 工业制造规模化发展需要自动化细粒度产品图像分析，但现有方法受限于数据集覆盖不足和模型泛化能力差的问题

Method: 提出MAU-Set多类型工业异常理解数据集，建立严格评估协议，并开发MAU-GPT多模态大模型，采用AMoE-LoRA机制统一异常感知和通用专家适配

Result: 大量实验表明MAU-GPT在所有领域均优于现有最先进方法，展现出强大的可扩展工业检测潜力

Conclusion: 该研究为工业异常检测提供了数据集、评估标准和高效模型三位一体的完整解决方案，显著推进了自动化工业质检的发展

Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.

</details>


### [4] [A General Model for Retinal Segmentation and Quantification](https://arxiv.org/abs/2602.07012)
*Zhonghua Wang,Lie Ju,Sijia Li,Wei Feng,Sijin Zhou,Ming Hu,Jianhao Xiong,Xiaoying Tang,Yifan Peng,Mingquan Lin,Yaodong Ding,Yong Zeng,Wenbin Wei,Li Dong,Zongyuan Ge*

Main category: cs.CV

TL;DR: RetSAM是一个通用的视网膜分割和量化框架，用于眼底成像，提供多目标分割和标准化生物标志物提取，支持大规模眼科研究和oculomics相关分析。


<details>
  <summary>Details</summary>
Motivation: 视网膜成像快速、无创且广泛可用，但大规模分析仍困难，因为缺乏公共多标签数据集和统一的分割到量化流程。

Method: 使用超过20万张眼底图像训练，采用多阶段策略，结合私有和公共数据，支持三个任务类别，分割五个解剖结构、四种视网膜表型模式和20多种病变类型。

Result: 在17个公共数据集上实现优异分割性能，DSC平均提高3.9个百分点，在挑战性多任务基准上最多提高15个百分点，具有良好的泛化能力。

Conclusion: RetSAM将眼底图像转化为标准化、可解释的定量表型，支持主要眼科疾病的系统相关分析，促进大规模眼科研究和转化应用。

Abstract: Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.

</details>


### [5] [Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models](https://arxiv.org/abs/2602.07013)
*Jiaxi Yang,Shicheng Liu,Yuchen Yang,Dongwon Lee*

Main category: cs.CV

TL;DR: CR-VLM提出了一种基于激活导向的可配置拒绝方法，通过教师强制机制提取可配置拒绝向量、门控机制防止过度拒绝，以及反事实视觉增强模块，实现视觉语言模型的自适应安全对齐。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型的拒绝机制是'一刀切'的，无法适应用户需求和上下文约束，导致拒绝不足或过度拒绝的问题。

Method: 1) 教师强制机制提取可配置拒绝向量；2) 门控机制保留范围内查询的接受能力；3) 反事实视觉增强模块对齐视觉表示与拒绝需求。

Result: 在多个数据集和各种VLM上的综合实验表明，CR-VLM实现了有效、高效且鲁棒的可配置拒绝。

Conclusion: 该方法为视觉语言模型的用户自适应安全对齐提供了一条可扩展的路径。

Abstract: With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \textbf{C}onfigurable \textbf{R}efusal in \textbf{VLM}s (\textbf{CR-VLM}), a robust and efficient approach for {\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.

</details>


### [6] [Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation](https://arxiv.org/abs/2602.07014)
*Qingyu Wu,Yuxuan Han,Haijun Li,Zhao Xu,Jianshan Zhao,Xu Jin,Longyue Wang,Weihua Luo*

Main category: cs.CV

TL;DR: Vectra是一个针对电商跨语言图像翻译的无参考视觉质量评估框架，通过多维度质量指标系统、大规模数据集和4B参数MLLM模型，实现了与人类评价高度相关的视觉质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有电商图像翻译研究中，基于参考的方法缺乏可解释性，模型即评判方法缺乏领域特定的细粒度奖励信号，无法有效评估视觉渲染质量对用户参与度的影响。

Method: 提出Vectra框架：1) Vectra Score - 14维可解释质量指标系统，包含空间感知缺陷面积比量化；2) Vectra Dataset - 从110万真实产品图像构建的数据集，包含2K基准测试集、30K推理标注和3.5K专家偏好标注；3) Vectra Model - 4B参数MLLM，可生成量化分数和诊断推理。

Result: 实验表明Vectra在人类排名相关性方面达到最先进水平，在评分性能上优于包括GPT-5和Gemini-3在内的领先MLLM模型。

Conclusion: Vectra填补了电商图像翻译视觉质量评估的空白，提供了可解释、无参考的评估解决方案，数据集和模型将在论文接受后发布。

Abstract: In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.

</details>


### [7] [Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach](https://arxiv.org/abs/2602.07015)
*Subreena,Mohammad Amzad Hossain,Mirza Raquib,Saydul Akbar Murad,Farida Siddiqi Prity,Muhammad Hanif,Nick Rahimi*

Main category: cs.CV

TL;DR: 提出一种基于MobileNetV3-Large和EfficientNetB0混合CNN架构的孟加拉纸币识别系统，结合多层感知机分类器，在资源受限设备上实现高效准确的货币识别。


<details>
  <summary>Details</summary>
Motivation: 解决视障人士依赖他人识别纸币而面临欺诈风险的问题，需要开发可靠的自动货币识别辅助技术。

Method: 构建新的孟加拉纸币数据集并整合四个公共基准数据集；设计混合CNN架构进行特征提取，使用MLP分类器；采用五折交叉验证和七种评估指标；集成LIME和SHAP解释性AI方法。

Result: 在受控数据集上达到97.95%准确率，复杂背景92.84%，综合数据集94.98%；通过多种评估指标验证性能。

Conclusion: 所提混合模型在保持低计算成本的同时实现了高精度纸币识别，适用于资源受限设备，为视障人士提供了可靠的辅助识别解决方案。

Abstract: Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.

</details>


### [8] [Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency](https://arxiv.org/abs/2602.07016)
*Mohsen Mostafa*

Main category: cs.CV

TL;DR: 本文探索了基于LeJEPA的高斯约束表示在无监督3D场景重建中的应用，提出了三个逐步优化的pipeline，在IMC2025挑战中验证了高斯约束对场景分离和相机位姿估计的改进效果。


<details>
  <summary>Details</summary>
Motivation: 解决从非结构化图像集合进行无监督3D重建的挑战，特别是在图像来自多个无关场景且存在显著视觉模糊性的现实条件下，需要同时进行场景发现和相机位姿估计。

Method: 提出三个渐进精炼的pipeline，最终采用受LeJEPA启发的各向同性高斯约束方法对学习到的图像嵌入施加约束，通过经验评估这些约束对聚类一致性和位姿估计鲁棒性的影响。

Result: 在IMC2025数据集上的实验结果表明，与启发式基线方法相比，高斯约束嵌入能够改善场景分离和位姿合理性，特别是在视觉模糊场景中表现更优。

Conclusion: 理论驱动的表示约束为桥接自监督学习原理和实际运动结构恢复pipeline提供了有前景的方向，证明了高斯约束在实际应用中的有效性。

Abstract: Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.

</details>


### [9] [XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models](https://arxiv.org/abs/2602.07017)
*Thuraya Alzubaidi,Sana Ammar,Maryam Alsharqi,Islem Rekik,Muzammil Behzad*

Main category: cs.CV

TL;DR: XAI-CLIP是一个基于多模态视觉-语言模型的ROI引导扰动框架，用于生成更清晰、边界感知的显著性图，显著提高医学图像分割的可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的模型在医学图像分割中表现优异，但其有限的可解释性阻碍了临床信任和部署。现有的XAI技术计算成本高、需要多次前向传播，且经常产生噪声或解剖学无关的解释。

Method: 提出XAI-CLIP框架，利用多模态视觉-语言模型嵌入定位临床相关的解剖区域，并指导解释过程。通过整合语言引导的区域定位与医学图像分割，应用针对性的区域感知扰动。

Result: 在FLARE22和CHAOS数据集上的实验显示：运行时减少60%，Dice分数提高44.6%，基于遮挡的解释的IoU提高96.7%。定性结果证实了更清晰、解剖学一致的归因图，伪影更少。

Conclusion: 将多模态视觉-语言表示整合到基于扰动的XAI框架中，显著提高了可解释性和效率，使医学图像分割系统更加透明且可临床部署。

Abstract: Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\% reduction in runtime, a 44.6\% improvement in dice score, and a 96.7\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.

</details>


### [10] [Deep Learning Based Multi-Level Classification for Aviation Safety](https://arxiv.org/abs/2602.07019)
*Elaheh Sabziyan Varnousfaderani,Syed A. M. Shihab,Jonathan King*

Main category: cs.CV

TL;DR: 基于CNN的图像鸟类分类框架，用于航空安全中的鸟类识别和群体特征分析，以改进鸟类撞击预防策略


<details>
  <summary>Details</summary>
Motivation: 现有鸟类雷达系统无法识别鸟类物种，而不同物种的飞行行为和高度偏好差异显著，这对航空安全至关重要

Method: 使用卷积神经网络(CNN)构建图像分类框架，实现鸟类物种识别、群体形态分类和群体大小估计

Result: 开发了能够识别鸟类物种、群体类型和规模的CNN分类器，为物种特定的飞行路径预测提供关键输入

Conclusion: 该框架通过视觉检测提供物种识别和群体特征信息，显著增强了鸟类撞击风险评估能力，为航空安全提供更精准的预警

Abstract: Bird strikes pose a significant threat to aviation safety, often resulting in loss of life, severe aircraft damage, and substantial financial costs. Existing bird strike prevention strategies primarily rely on avian radar systems that detect and track birds in real time. A major limitation of these systems is their inability to identify bird species, an essential factor, as different species exhibit distinct flight behaviors, and altitudinal preference. To address this challenge, we propose an image-based bird classification framework using Convolutional Neural Networks (CNNs), designed to work with camera systems for autonomous visual detection. The CNN is designed to identify bird species and provide critical input to species-specific predictive models for accurate flight path prediction. In addition to species identification, we implemented dedicated CNN classifiers to estimate flock formation type and flock size. These characteristics provide valuable supplementary information for aviation safety. Specifically, flock type and size offer insights into collective flight behavior, and trajectory dispersion . Flock size directly relates to the potential impact severity, as the overall damage risk increases with the combined kinetic energy of multiple birds.

</details>


### [11] [The Geometry of Representational Failures in Vision Language Models](https://arxiv.org/abs/2602.07025)
*Daniele Savietto,Declan Campbell,André Panisson,Marco Nurisso,Giovanni Petri,Jonathan D. Cohen,Alan Perotti*

Main category: cs.CV

TL;DR: 该研究通过分析视觉语言模型的表征几何结构，揭示了多目标视觉任务中错误模式的内在机制，提出了基于概念向量的定量框架来解释模型行为。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多目标视觉任务中表现出令人困惑的失败模式，如幻觉不存在的元素或无法识别相似物体，这些错误与人类认知限制相似但内在机制不明确。

Method: 分析开源视觉语言模型(Qwen、InternVL、Gemma)的表征几何，通过蒸馏"概念向量"（编码视觉概念的潜在方向），并通过引导干预验证这些向量在简化及自然视觉任务中的有效性。

Result: 发现概念向量之间的几何重叠与特定错误模式强相关，概念向量干预能够可靠地操纵模型行为（如将红色花朵感知为蓝色）。

Conclusion: 提出了一个基于表征几何的定量框架，为理解视觉语言模型内部表征如何塑造模型行为并导致视觉失败提供了机制性见解。

Abstract: Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the "Binding Problem", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill "concept vectors" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.

</details>


### [12] [Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models](https://arxiv.org/abs/2602.07026)
*Xiaomin Yu,Yi Xin,Wenjie Zhang,Chonghan Liu,Hanzhen Zhao,Xiaoxing Hu,Xinlei Yu,Ziyue Qiao,Hao Tang,Xue Yang,Xiaobin Hu,Chengwei Qin,Hui Xiong,Yu Qiao,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出了Fixed-frame Modality Gap Theory来精确表征多模态对比学习中的模态间隙几何形状，并基于此开发了无需训练的ReAlign对齐策略和可扩展的ReVision训练范式，用无配对文本数据有效替代昂贵的图像-文本对来扩展多模态大语言模型。


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习中存在模态间隙问题，即表达相同语义的不同模态嵌入占据系统偏移区域。现有方法受限于过度简化的各向同性假设，无法在大规模场景中有效应用。

Method: 1) 提出Fixed-frame Modality Gap Theory，将模态间隙分解为稳定偏置和各向异性残差；2) 开发ReAlign策略，通过Anchor、Trace和Centroid三步对齐过程，利用无配对数据统计信息对齐文本表示到图像表示分布；3) 提出ReVision训练范式，将ReAlign集成到预训练阶段。

Result: 该框架证明统计对齐的无配对数据可以有效替代昂贵的图像-文本对，为多模态大语言模型的高效扩展提供了可行路径。

Conclusion: 通过精确建模模态间隙几何形状并利用大规模无配对数据统计信息，可以构建高效可扩展的多模态对齐方法，显著降低对高质量图像-文本配对数据的依赖。

Abstract: Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.

</details>


### [13] [Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models](https://arxiv.org/abs/2602.07027)
*Sanggeon Yun,Ryozo Masukawa,SungHeon Jeong,Wenjun Huang,Hanning Chen,Mohsen Imani*

Main category: cs.CV

TL;DR: FCL是一种基于公平性的测试时自适应框架，通过解耦增强探索和公平性驱动校准，避免熵最小化带来的虚假相关性，提升视觉语言模型在分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于提示的TTA方法依赖熵最小化，容易放大虚假相关性和导致过度自信错误，特别是在类别共享视觉特征时。FCL旨在通过显式处理共享证据偏差来解决这一问题。

Method: 提出公平上下文学习(FCL)框架：1) 基于增强的探索识别合理类别候选；2) 公平性驱动校准，通过调整文本上下文来平衡对常见视觉证据的敏感性，避免熵最小化。

Result: FCL在多种域偏移和细粒度基准测试中取得了与最先进TTA方法竞争的自适应性能，实证验证了理论动机的有效性。

Conclusion: FCL通过公平性约束有效缓解了部分特征迷恋问题，实现了无需熵最小化的文本嵌入校准，为视觉语言模型的测试时自适应提供了新思路。

Abstract: Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.

</details>


### [14] [A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures](https://arxiv.org/abs/2602.07028)
*Kaaustaaub Shankar,Bharadwaj Dogga,Kelly Cohen*

Main category: cs.CV

TL;DR: 本研究比较了标准CNN与其ANFIS增强版本在对抗攻击下的性能，发现ANFIS集成对鲁棒性的影响具有架构依赖性，ResNet18-ANFIS表现改善而VGG-ANFIS通常表现不佳


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络缺乏可解释性且易受对抗攻击，神经模糊混合方法如DCNFIS通过用ANFIS替代全连接分类器来提高可解释性，但其鲁棒性尚未充分研究

Method: 在MNIST、Fashion-MNIST、CIFAR-10和CIFAR-100数据集上，使用基于梯度的PGD攻击和无梯度的Square攻击，比较标准CNN（ConvNet、VGG、ResNet18）与其ANFIS增强版本的性能

Result: ANFIS集成并未一致提高干净准确率，对鲁棒性的影响取决于架构：ResNet18-ANFIS表现出改进的对抗鲁棒性，而VGG-ANFIS通常不如其基线

Conclusion: 神经模糊增强可以在特定架构中提高鲁棒性，但并非普遍有益，需要针对具体架构进行优化

Abstract: Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.

</details>


### [15] [UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents](https://arxiv.org/abs/2602.07038)
*Yifan Ji,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Qian Zhang,Zhibo Yang,Junyang Lin,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CV

TL;DR: UNIKIE-BENCH是一个统一基准，用于评估大型多模态模型在关键信息提取任务中的能力，包含约束类别和开放类别两个互补的评估轨道，揭示了当前模型在多样化模式定义、长尾关键字段和复杂布局下的性能挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界文档的关键信息提取面临布局结构、视觉质量和任务特定信息需求的巨大差异，需要系统评估大型多模态模型在此任务上的表现。

Method: 构建UNIKIE-BENCH基准，包含两个评估轨道：约束类别KIE轨道（基于场景预定义模式）和开放类别KIE轨道（提取文档中任何显式存在的关键信息），并在15个最先进的大型多模态模型上进行实验。

Result: 实验显示模型在多样化模式定义、长尾关键字段和复杂布局下存在显著的性能下降，不同文档类型和场景间存在明显的性能差异。

Conclusion: 基于大型多模态模型的关键信息提取在接地准确性和布局感知推理方面仍存在持续挑战，需要进一步研究改进。

Abstract: Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.

</details>


### [16] [OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis](https://arxiv.org/abs/2602.07041)
*Leeje Jang,Yao-Yi Chiang,Angela M. Hastings,Patimaporn Pungchanchaikul,Martha B. Lucas,Emily C. Schultz,Jeffrey P. Louie,Mohamed Estai,Wen-Chen Wang,Ryan H. L. Ip,Boyen Huang*

Main category: cs.CV

TL;DR: OMNI-Dent是一个数据高效且可解释的牙科诊断框架，通过将临床推理原则整合到视觉语言模型中，使用多视角智能手机照片进行牙齿级评估，无需牙科特定的VLM微调。


<details>
  <summary>Details</summary>
Motivation: 当前AI牙科诊断方法主要作为视觉模式识别任务，未能反映牙医的结构化临床推理，需要大量专家标注数据且在多样化现实成像条件下泛化能力有限。

Method: 基于视觉语言模型(VLM)的管道，嵌入牙科专家的诊断启发式方法，使用多视角智能手机照片，无需对VLM进行牙科特定微调，利用VLM现有的视觉-语言能力。

Result: 开发了OMNI-Dent框架，能够进行牙齿级评估，支持在缺乏精选临床影像的环境中提供诊断评估。

Conclusion: OMNI-Dent作为早期辅助工具，帮助用户识别潜在异常并确定何时需要专业评估，为无法获得现场护理的个人提供了实用选择。

Abstract: Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.

</details>


### [17] [COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification](https://arxiv.org/abs/2602.07042)
*Magesh Rajasekaran,Md Saiful Islam Sajol,Frej Berglind,Supratik Mukhopadhyay,Kamalika Das*

Main category: cs.CV

TL;DR: COMBOOD是一个新颖的无监督半参数框架，用于图像识别中的分布外(OOD)检测，通过结合最近邻和马氏距离两种度量信号，在近OOD和远OOD场景下都能提供准确的置信度评分。


<details>
  <summary>Details</summary>
Motivation: 现有的OOD检测方法在远OOD场景表现良好但在近OOD场景表现不佳，需要一种能够同时处理两种场景的统一框架。

Method: 采用半参数化方法，结合非参数的最近邻距离和参数化的马氏距离两种信号，生成综合置信度评分。框架计算复杂度与嵌入空间大小呈线性关系。

Result: 在OpenOOD v1和v1.5基准数据集以及文档数据集上，COMBOOD在准确率方面优于最先进的OOD检测方法，且大多数改进具有统计显著性。

Conclusion: COMBOOD提供了一个有效且可扩展的解决方案，能够同时处理近OOD和远OOD检测问题，适用于实际应用场景。

Abstract: Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.

</details>


### [18] [PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging](https://arxiv.org/abs/2602.07044)
*Tianyi Qu,Songxiao Yang,Haolin Wang,Huadong Song,Xiaoting Guo,Wenguang Hu,Guanlin Liu,Honghe Chen,Yafei Ou*

Main category: cs.CV

TL;DR: PipeMFL-240K是一个大规模、高质量标注的管道漏磁检测数据集，包含24万张图像和19万多个边界框标注，用于解决管道完整性评估中的目标检测挑战，为深度学习模型提供可靠的基准测试平台。


<details>
  <summary>Details</summary>
Motivation: 管道完整性对工业安全和环境保护至关重要，但深度学习在漏磁检测自动化方面进展受限，主要原因是缺乏大规模公开数据集和基准，导致公平比较和可重复评估困难。

Method: 收集来自11条管道约1480公里的检测数据，构建包含240,320张图像和191,530个高质量边界框标注的数据集，涵盖12个类别，具有极端长尾分布、微小目标和高类内变异性等挑战。

Result: 通过最先进的目标检测器进行广泛实验，结果显示现代检测器在处理漏磁数据固有特性方面仍存在困难，表明有显著的改进空间。

Conclusion: PipeMFL-240K作为首个公开的大规模管道漏磁检测数据集和基准，为高效管道诊断和维护规划提供了关键基础，有望加速基于漏磁的管道完整性评估算法创新和可重复研究。

Abstract: Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \textbf{240,320} images and \textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.

</details>


### [19] [VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing](https://arxiv.org/abs/2602.07045)
*Zhiming Luo,Di Wang,Haonan Guo,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: VLRS-Bench是首个专门针对遥感复杂推理任务的基准测试，包含2000个问答对，覆盖14个任务和8个时间阶段，旨在解决现有遥感基准偏向感知任务的问题。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基准测试主要偏向目标识别和场景分类等感知任务，这限制了多模态大语言模型在认知要求较高的遥感应用中的发展。

Method: 通过专门的构建流程，整合遥感特定先验知识和专家知识，确保地理空间真实性和推理复杂性，构建了跨越认知、决策和预测三个核心维度的基准。

Result: 实验结果显示现有最先进的多模态大语言模型存在显著瓶颈，为遥感领域的多模态推理发展提供了关键见解。

Conclusion: VLRS-Bench填补了遥感复杂推理基准的空白，为推进遥感多模态推理技术发展提供了重要工具和评估标准。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.

</details>


### [20] [ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees](https://arxiv.org/abs/2602.07047)
*Muhammad Rashid,Elvio G. Amparore,Enrico Ferrari,Damiano Verda*

Main category: cs.CV

TL;DR: ShapBPT是一种基于分层Shapley值的新型数据感知可解释AI方法，通过二进制分割树(BPT)将多尺度层次结构引入图像特征归因，显著提升计算效率和语义对齐性。


<details>
  <summary>Details</summary>
Motivation: 现有的分层Shapley方法未能充分利用图像数据的多尺度结构，导致收敛速度慢且与形态特征对齐性差，缺乏针对视觉任务的数据感知层次结构。

Method: 提出ShapBPT方法，将Shapley系数分配给专门为图像设计的二进制分割树(BPT)多尺度层次结构，通过数据感知的分层划分确保特征归因与内在图像形态对齐。

Result: 实验结果表明ShapBPT在图像结构对齐性和计算效率方面优于现有XCV方法，20人用户研究证实人类更偏好ShapBPT的解释结果。

Conclusion: ShapBPT成功将分层Shapley方法与图像数据连接，为视觉可解释性提供了更高效和语义意义更强的方法，填补了结构化视觉数据模型可解释性的空白。

Abstract: Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.

</details>


### [21] [Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead](https://arxiv.org/abs/2602.07049)
*Jindong Li,Dario Zanca,Vincent Christlein,Tim Hamann,Jens Barth,Peter Kämpf,Björn Eskofier*

Main category: cs.CV

TL;DR: ECHWR是一种在线手写识别训练框架，通过临时辅助分支和双重对比目标（包括新颖的基于错误的对比损失）来改善特征表示和识别精度，无需增加推理成本，在OnHW-Words500数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决边缘硬件上手写识别面临的内存约束问题，同时提升识别准确率而不增加推理成本。

Method: 提出ECHWR训练框架，使用临时辅助分支将传感器信号与语义文本嵌入对齐，采用双重对比目标：批内对比损失和基于错误的对比损失（区分正确信号与合成难负样本）。

Result: 在OnHW-Words500数据集上，ECHWR显著优于最先进基线，在独立于书写者和依赖于书写者的分割上分别降低了7.4%和10.4%的字符错误率。

Conclusion: ECHWR框架有效提升了手写识别性能，基于错误的对比损失在处理未见书写风格方面表现出有效性，辅助分支在训练后被丢弃，保持部署模型的原始高效架构。

Abstract: Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.

</details>


### [22] [Interpreting Physics in Video World Models](https://arxiv.org/abs/2602.07050)
*Sonia Joseph,Quentin Garrido,Randall Balestriero,Matthew Kowal,Thomas Fel,Shahab Bakhtiari,Blake Richards,Mike Rabbat*

Main category: cs.CV

TL;DR: 研究发现视频编码器通过中间层的"物理涌现区"获取物理信息，采用分布式而非分解式表征方式，运动方向通过高维环形几何结构编码


<details>
  <summary>Details</summary>
Motivation: 探究视频模型是否需要依赖物理变量的分解表征来进行准确物理预测，还是可以通过任务特定的分布式方式隐式表示这些变量

Method: 使用分层探测、子空间几何分析、补丁级解码和针对性注意力消融等方法，分析基于编码器的视频变换器中的物理信息表征

Result: 发现中间深度存在物理涌现区，物理变量在此变得可访问；标量物理量从早期层即可获取，而运动方向仅在物理涌现区可访问；方向通过高维环形几何结构编码

Conclusion: 现代视频模型不使用经典物理引擎式的分解表征，而是采用足以进行物理预测的分布式表征方式

Abstract: A long-standing question in physical reasoning is whether video-based models need to rely on factorized representations of physical variables in order to make physically accurate predictions, or whether they can implicitly represent such variables in a task-specific, distributed manner. While modern video world models achieve strong performance on intuitive physics benchmarks, it remains unclear which of these representational regimes they implement internally. Here, we present the first interpretability study to directly examine physical representations inside large-scale video encoders. Using layerwise probing, subspace geometry, patch-level decoding, and targeted attention ablations, we characterize where physical information becomes accessible and how it is organized within encoder-based video transformers.
  Across architectures, we identify a sharp intermediate-depth transition -- which we call the Physics Emergence Zone -- at which physical variables become accessible. Physics-related representations peak shortly after this transition and degrade toward the output layers. Decomposing motion into explicit variables, we find that scalar quantities such as speed and acceleration are available from early layers onwards, whereas motion direction becomes accessible only at the Physics Emergence Zone. Notably, we find that direction is encoded through a high-dimensional population structure with circular geometry, requiring coordinated multi-feature intervention to control. These findings suggest that modern video models do not use factorized representations of physical variables like a classical physics engine. Instead, they use a distributed representation that is nonetheless sufficient for making physical predictions.

</details>


### [23] [Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning](https://arxiv.org/abs/2602.07051)
*Karthik Sivakoti*

Main category: cs.CV

TL;DR: Neural Sentinel是一个基于视觉语言模型的统一车牌识别系统，通过单次前向传播实现车牌识别、状态分类和车辆属性提取，相比传统多阶段方法精度提升显著，并具备零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统ALPR系统采用多阶段流水线（目标检测+OCR），存在误差累积、延迟高和架构复杂的问题，需要更高效统一的方法。

Method: 使用LoRA微调PaliGemma 3B模型，引入人机协同持续学习框架（HITL），通过经验回放保持70:30的原始训练数据与校正样本比例。

Result: 达到92.3%的车牌识别准确率（比EasyOCR提升14.1%，比PaddleOCR提升9.9%），平均推理延迟152ms，ECE为0.048，零样本任务准确率：车辆颜色89%、安全带82%、乘员计数78%。

Conclusion: 统一视觉语言方法代表了ALPR系统的范式转变，提供了更高的准确性、更低的架构复杂性以及传统方法无法实现的涌现多任务能力。

Abstract: Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.

</details>


### [24] [Toward Accurate and Accessible Markerless Neuronavigation](https://arxiv.org/abs/2602.07052)
*Ziye Xie,Oded Schlesinger,Raj Kundu,Jessica Y. Choi,Pablo Iturralde,Dennis A. Turner,Stefan M. Goetz,Guillermo Sapiro,Angel V. Peterchev,J. Matias Di Martino*

Main category: cs.CV

TL;DR: 提出并验证了无标记神经导航方法，使用低成本可见光和红外光相机结合立体视觉和深度感知，替代传统需要标记物的系统，实现了2.32mm和2.01°的中位跟踪误差。


<details>
  <summary>Details</summary>
Motivation: 传统神经导航系统依赖标记物，需要手动配准、可能移位且造成不适，需要开发更便捷、低成本的无标记解决方案。

Method: 使用低成本可见光和红外光相机，结合立体视觉和深度感知技术，通过算法建模面部几何特征来实现无标记跟踪。

Result: 在50名人类受试者验证中，最佳无标记算法的中位跟踪误差仅为2.32mm和2.01°，相比传统标记系统具有足够精度，且显著优于先前无标记方法。

Conclusion: 无标记神经导航方法可降低设置成本和复杂度，提高患者舒适度，扩展神经导航在临床和研究环境中的可及性，多传感器数据融合可进一步提高精度。

Abstract: Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01°$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.

</details>


### [25] [RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything](https://arxiv.org/abs/2602.07057)
*Di Mo,Mingyang Sun,Chengxiu Yin,Runjia Tian,Yanhong Wu,Liyan Xu*

Main category: cs.CV

TL;DR: RECITYGEN是一个结合潜在扩散模型和交互式语义分割的工具，允许用户通过文本提示交互式生成城市街景的变体图像，用于参与式城市设计。


<details>
  <summary>Details</summary>
Motivation: 传统自上而下的城市设计方法往往忽视公众意见，导致设计愿景与现实需求脱节。数字工具的发展为更广泛的利益相关者参与城市设计提供了机会。

Method: 结合最先进的潜在扩散模型和交互式语义分割技术，开发RECITYGEN工具，用户可以通过文本提示交互式创建城市环境的街景变体图像。

Result: 在北京的试点项目中，用户使用RECITYGEN为正在进行的城市更新项目提出改进建议，工具显示出与公众偏好高度一致的显著潜力。

Conclusion: RECITYGEN展示了向更动态和包容性城市规划方法转变的可能性，尽管存在一些局限性，但在公众参与城市设计方面具有重要应用前景。

Abstract: Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.

</details>


### [26] [FADE: Selective Forgetting via Sparse LoRA and Self-Distillation](https://arxiv.org/abs/2602.07058)
*Carolina R. Kelsch,Leonardo S. B. Pereira,Natnael Mola,Luis H. Arribas,Juan C. S. M. Avedillo*

Main category: cs.CV

TL;DR: FADE是一种用于文本到图像扩散模型的快速数据擦除方法，通过参数定位和自蒸馏两阶段实现高效概念遗忘，在保持整体性能的同时实现轻量级、可逆的模型修改。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘需要从训练模型中移除特定数据或概念的影响，同时保持整体性能，这在文本到图像扩散模型中由于高计算成本和平衡遗忘与保留的难度而具有挑战性。

Method: 两阶段方法：1) 使用基于梯度的显著性识别对遗忘集最负责的参数，通过稀疏LoRA适配器约束更新；2) 应用自蒸馏目标，用用户定义的替代概念覆盖被遗忘概念，同时保留对保留数据的行为。

Result: 在UnlearnCanvas基准测试和多个数据集上评估，FADE实现了最先进的遗忘性能，具有细粒度的遗忘-保留权衡控制，在多个领域实现了强概念擦除和高保留能力。

Conclusion: FADE是一种适用于扩散基图像生成模型中选择性遗忘的合适解决方案，具有内存效率高、可逆、可在运行时合并或移除的特点，适合生产系统部署。

Abstract: Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.

</details>


### [27] [From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal](https://arxiv.org/abs/2602.07062)
*Daniil Storonkin,Ilia Dziub,Maksim Golyadkin,Ilya Makarov*

Main category: cs.CV

TL;DR: 开发了一个基于计算机视觉的辅助系统，用于在铁路车厢卸货过程中自动估算废钢污染百分比并进行废钢分类，通过多实例学习和多任务学习实现，显著减少了人工评估的主观性和安全风险。


<details>
  <summary>Details</summary>
Motivation: 当前废钢质量评估依赖人工视觉检查，存在主观性强、安全性差（粉尘和移动机械危险）的问题，需要客观、自动化的解决方案来提升评估准确性和操作安全性。

Method: 采用多实例学习（MIL）将污染评估构建为回归任务，结合多任务学习（MTL）同时进行废钢分类；系统包括磁铁/车厢检测、版本化推理服务生成置信度评分，并集成主动学习循环持续改进。

Result: 最佳性能：MIL方法达到MAE 0.27和R2 0.83；MTL设置达到MAE 0.36且废钢分类F1分数0.79；系统实现近实时处理并集成到验收工作流程中。

Conclusion: 该管道系统有效减少了主观评估变异性，提高了人员安全性，并能集成到验收和熔炼计划工作流程中，为钢铁制造提供了更客观、安全的废钢质量评估解决方案。

Abstract: Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.

</details>


### [28] [Exploring Physical Intelligence Emergence via Omni-Modal Architecture and Physical Data Engine](https://arxiv.org/abs/2602.07064)
*Minghao Han,Dingkang Yang,Yue Jiang,Yizhou Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: OmniFysics是一个紧凑的全模态模型，通过物理数据引擎注入显式物理知识，统一理解图像、音频、视频和文本，并具备语音和图像生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有全模态模型在物理理解方面存在脆弱性，因为关键物理属性在视觉上具有模糊性且在网络规模数据中稀疏表示。

Method: 构建物理数据引擎FysicsAny（通过层次检索映射对象到物理属性）和FysicsOmniCap（通过音频-视觉一致性过滤蒸馏网络视频），采用分阶段多模态对齐和指令调优训练，使用潜在空间流匹配进行文本到图像生成，通过意图路由器按需激活生成。

Result: 在标准多模态基准测试中表现出竞争力，在物理导向评估中获得改进结果。

Conclusion: 通过显式注入物理知识的紧凑全模态模型能够有效提升物理理解能力，在保持竞争力的同时改善物理相关任务的性能。

Abstract: Physical understanding remains brittle in omni-modal models because key physical attributes are visually ambiguous and sparsely represented in web-scale data. We present OmniFysics, a compact omni-modal model that unifies understanding across images, audio, video, and text, with integrated speech and image generation. To inject explicit physical knowledge, we build a physical data engine with two components. FysicsAny produces physics-grounded instruction--image supervision by mapping salient objects to verified physical attributes through hierarchical retrieval over a curated prototype database, followed by physics-law--constrained verification and caption rewriting. FysicsOmniCap distills web videos via audio--visual consistency filtering to generate high-fidelity video--instruction pairs emphasizing cross-modal physical cues. We train OmniFysics with staged multimodal alignment and instruction tuning, adopt latent-space flow matching for text-to-image generation, and use an intent router to activate generation only when needed. Experiments show competitive performance on standard multimodal benchmarks and improved results on physics-oriented evaluations.

</details>


### [29] [Contactless estimation of continuum displacement and mechanical compressibility from image series using a deep learning based framework](https://arxiv.org/abs/2602.07065)
*A. N. Maria Antony,T. Richter,E. Gladilin*

Main category: cs.CV

TL;DR: 基于深度学习的端到端方法，用于从图像序列直接估计连续位移和材料压缩性，相比传统方法在效率和精度上都有显著提升


<details>
  <summary>Details</summary>
Motivation: 非接触式无创估计物理介质力学特性在工程和生物医学应用中很重要，但传统方法依赖耗时的迭代算法和非刚性图像配准，不适合高通量数据处理

Method: 采用两个深度神经网络分别进行图像配准和材料压缩性估计，构建端到端框架，能够从图像序列直接输出位移和压缩性参数

Result: 实验结果表明，深度学习模型在参考数据集上训练后，即使图像配准预测的映射与参考位移场存在显著局部偏差，仍能准确确定材料压缩性

Conclusion: 深度学习端到端模型的卓越精度源于其评估高阶认知特征（如矢量场涡度）的能力，而非传统的图像位移局部特征

Abstract: Contactless and non-invasive estimation of mechanical properties of physical media from optical observations is of interest for manifold engineering and biomedical applications, where direct physical measurements are not possible. Conventional approaches to the assessment of image displacement and non-contact material probing typically rely on time-consuming iterative algorithms for non-rigid image registration and constitutive modelling using discretization and iterative numerical solving techniques, such as Finite Element Method (FEM) and Finite Difference Method (FDM), which are not suitable for high-throughput data processing. Here, we present an efficient deep learning based end-to-end approach for the estimation of continuum displacement and material compressibility directly from the image series. Based on two deep neural networks for image registration and material compressibility estimation, this framework outperforms conventional approaches in terms of efficiency and accuracy. In particular, our experimental results show that the deep learning model trained on a set of reference data can accurately determine the material compressibility even in the presence of substantial local deviations of the mapping predicted by image registration from the reference displacement field. Our findings suggest that the remarkable accuracy of the deep learning end-to-end model originates from its ability to assess higher-order cognitive features, such as the vorticity of the vector field, rather than conventional local features of the image displacement.

</details>


### [30] [Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.07069)
*Zihao Fan,Xin Lu,Yidi Liu,Jie Huang,Dong Li,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Bird-SR是一个双向奖励引导的扩散框架，通过奖励反馈学习将超分辨率问题转化为轨迹级偏好优化，同时利用合成LR-HR对和真实世界LR图像，在保持结构一致性的同时提升感知质量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的超分辨率方法虽然能合成丰富细节，但在合成数据上训练的模型往往因分布偏移而在真实世界LR图像上表现不佳。需要一种能同时处理合成和真实数据、平衡结构保真度和感知质量的方法。

Method: 采用双向奖励引导扩散框架：1) 在早期扩散步骤直接在合成对上优化以保证结构保真度；2) 在后期采样步骤应用质量引导奖励到合成和真实图像；3) 使用相对优势空间和语义对齐约束防止奖励黑客；4) 动态保真度-感知权重策略平衡不同阶段的学习重点。

Result: 在真实世界超分辨率基准测试中，Bird-SR在感知质量方面持续优于最先进方法，同时保持了结构一致性。

Conclusion: Bird-SR通过奖励反馈学习和动态优化策略，有效解决了真实世界超分辨率中的分布偏移问题，在保持结构保真度的同时显著提升了感知质量，为真实世界超分辨率提供了有效的解决方案。

Abstract: Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.

</details>


### [31] [MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation](https://arxiv.org/abs/2602.07082)
*Haoming Wang,Qiyao Xue,Weichen Liu,Wei Gao*

Main category: cs.CV

TL;DR: MosaicThinker是一种推理时计算技术，通过在设备端构建全局语义地图并利用视觉提示，增强小型视觉语言模型在跨帧空间推理任务中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间推理方面能力较弱，特别是在需要处理多帧视频中复杂空间关系的任务中，缺乏3D空间信息知识。

Method: 将多帧中的碎片化空间信息整合为统一的全局语义地图表示，并通过视觉提示引导VLM在该语义地图上进行空间推理。

Result: 实验结果显示该技术能显著提升资源受限设备上跨帧空间推理的准确性，适用于不同类型和复杂度的推理任务。

Conclusion: MosaicThinker技术有效解决了设备端VLM在跨帧空间推理中的性能瓶颈，为嵌入式AI的视觉空间推理提供了实用解决方案。

Abstract: When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.

</details>


### [32] [WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark](https://arxiv.org/abs/2602.07095)
*Wang Lin,Feng Wang,Majun Zhang,Wentao Hu,Tao Jin,Zhou Zhao,Fei Wu,Jingyuan Chen,Alan Yuille,Sucheng Ren*

Main category: cs.CV

TL;DR: WorldEdit是一个专门针对隐式编辑指令的图像编辑数据集，通过两阶段训练框架和因果验证奖励机制，显著提升了模型在因果推理编辑任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型在处理隐式编辑指令时存在局限，这些指令描述了视觉变化的原因但未明确说明结果，需要复杂的世界知识和推理能力。

Method: 构建高质量的WorldEdit数据集，包含符合现实世界因果逻辑的改写指令；采用两阶段训练框架微调Bagel等模型，并整合因果验证奖励机制。

Result: 提出的数据集和方法显著缩小了与GPT-4o和Nano-Banana的性能差距，在指令遵循和知识合理性方面表现出竞争力。

Conclusion: WorldEdit数据集和训练框架有效解决了隐式图像编辑的挑战，为世界驱动的图像编辑提供了新的解决方案。

Abstract: Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.

</details>


### [33] [TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation](https://arxiv.org/abs/2602.07100)
*Biao Xiong,Zhen Peng,Ping Wang,Qiegen Liu,Xian Zhong*

Main category: cs.CV

TL;DR: TLC-Plan是一种直接从边界输入生成矢量平面图的层次化生成模型，采用双级VQ-VAE和自回归变换器，在RPLAN数据集上实现SOTA性能（FID=1.84，MSE=2.06）。


<details>
  <summary>Details</summary>
Motivation: 现有方法在栅格空间操作并依赖后处理矢量化，导致结构不一致并阻碍端到端学习。受组合空间推理启发，提出与人类基于模块化和可重用模式的建筑工作流程对齐的矢量平面图生成方法。

Method: 采用两级VQ-VAE编码全局布局为语义标记的房间边界框，并使用多边形级代码细化局部几何。通过CodeTree统一层次结构，自回归变换器基于边界条件采样代码生成多样化且拓扑有效的设计。

Result: 在RPLAN数据集上达到最先进性能（FID=1.84，MSE=2.06），在LIFULL数据集上获得领先结果，无需显式房间拓扑或维度先验。

Conclusion: 该框架推进了约束感知和可扩展的矢量平面图生成，适用于实际建筑应用。源代码和训练模型已公开发布。

Abstract: Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.

</details>


### [34] [Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting](https://arxiv.org/abs/2602.07101)
*Zinan Lv,Yeqian Qian,Chen Sang,Hao Liu,Danping Zou,Ming Yang*

Main category: cs.CV

TL;DR: 提出一种基于可重光照3D高斯泼溅的端到端强化学习框架，实现无人机在无结构室外环境中的零样本单目视觉导航，通过光照分解和合成增强训练，使策略具备光照不变性。


<details>
  <summary>Details</summary>
Motivation: 无人机在非结构化室外环境中使用单目视觉导航时，仿真与真实世界之间存在显著的视觉域差距，现有方法将静态光照与几何耦合，严重限制了策略在动态真实光照下的泛化能力。

Method: 1) 构建基于真实世界数据的高保真仿真环境；2) 提出可重光照3D高斯泼溅技术，分解场景组件实现物理基础的光照编辑；3) 使用强化学习训练策略直接从单目RGB观测映射到连续控制命令；4) 通过合成多样化光照条件（从强方向性阳光到漫射阴天）增强训练。

Result: 轻量级四旋翼无人机在复杂森林环境中实现高达10 m/s的鲁棒、无碰撞导航，对剧烈光照变化表现出显著抗性，无需微调。

Conclusion: 该方法通过神经表示中的物理基础光照编辑和光照增强训练，成功解决了仿真到真实世界的视觉域差距问题，实现了无人机在动态光照条件下的零样本鲁棒导航。

Abstract: UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.

</details>


### [35] [Extended to Reality: Prompt Injection in 3D Environments](https://arxiv.org/abs/2602.07104)
*Zhuoheng Li,Ying Chen*

Main category: cs.CV

TL;DR: PI3D是一种针对多模态大语言模型在3D物理环境中的提示注入攻击，通过放置带有文本的物理对象而非数字图像编辑来实现，能够有效诱导模型执行攻击者指定的任务。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在3D环境中的应用日益广泛，但现有研究主要关注文本领域和数字2D图像的提示注入攻击，缺乏对3D物理环境中攻击方式的研究。

Method: 提出PI3D攻击方法，通过优化3D物体姿态（位置和方向）放置带有注入文本的物理对象，确保物理放置的合理性同时最大化攻击效果。

Result: 实验证明PI3D对多种多模态大语言模型在不同相机轨迹下均有效，现有防御措施无法有效抵御此类攻击。

Conclusion: 3D物理环境中的提示注入攻击是一个真实且严重的威胁，需要开发新的防御机制来保护多模态大语言模型在物理世界应用中的安全性。

Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surface emerges: an attacker can place text-bearing physical objects in the environment to override MLLMs' intended task. While prior work has studied prompt injection in the text domain and through digitally edited 2D images, it remains unclear how these attacks function in 3D physical environments. To bridge the gap, we introduce PI3D, a prompt injection attack against MLLMs in 3D environments, realized through text-bearing physical object placement rather than digital image edits. We formulate and solve the problem of identifying an effective 3D object pose (position and orientation) with injected text, where the attacker's goal is to induce the MLLM to perform the injected task while ensuring that the object placement remains physically plausible. Experiments demonstrate that PI3D is an effective attack against multiple MLLMs under diverse camera trajectories. We further evaluate existing defenses and show that they are insufficient to defend against PI3D.

</details>


### [36] [Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models](https://arxiv.org/abs/2602.07106)
*Haoyu Zhang,Zhipeng Li,Yiwen Guo,Tianshu Yu*

Main category: cs.CV

TL;DR: Ex-Omni是一个开源全模态框架，通过解耦语义推理和时间生成，利用语音单元作为时间支架和统一的token-as-query门控融合机制，实现语音伴随的3D面部动画生成。


<details>
  <summary>Details</summary>
Motivation: 现有全模态大语言模型在整合语音与3D面部动画方面存在不足，主要挑战在于离散的token级语义推理与密集的细粒度时间动态之间的表示不匹配，导致在有限数据下直接建模难以优化。

Method: 提出Ex-Omni框架，通过解耦语义推理与时间生成，使用语音单元作为时间支架，采用统一的token-as-query门控融合机制进行可控语义注入，并构建InstructEx数据集来支持模型训练。

Result: 大量实验表明，Ex-Omni在性能上与现有开源全模态大语言模型竞争，同时能够稳定地生成对齐的语音和面部动画。

Conclusion: Ex-Omni成功解决了语音与3D面部动画融合的挑战，通过创新的解耦架构和融合机制，为全模态大语言模型提供了有效的语音-面部动画生成能力。

Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.

</details>


### [37] [Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds](https://arxiv.org/abs/2602.07149)
*Rawisara Lohanimit,Yankun Wu,Amelia Katirai,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

TL;DR: 论文通过分析LAION-400M数据集，发现其中包含大量怀孕超声图像等敏感个人信息，揭示了大模型训练数据集中存在的隐私泄露风险，并提出了数据管理和隐私保护建议。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的兴起，大规模网络数据集的使用增加，但数据管理不足，引发了敏感或隐私信息被包含的担忧，特别是怀孕超声图像这类敏感个人信息。

Method: 使用CLIP嵌入相似性对LAION-400M数据集进行系统检查，检索包含怀孕超声的图像，并检测姓名、位置等数千条隐私信息实体。

Result: 发现多个图像包含高风险信息，可能实现重新识别或冒充，揭示了大模型训练数据集中的严重隐私泄露问题。

Conclusion: 提出了数据集管理、数据隐私和公共图像数据集伦理使用的推荐实践，强调了在AI发展中加强数据隐私保护的重要性。

Abstract: The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.

</details>


### [38] [DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages](https://arxiv.org/abs/2602.07174)
*Yongheng Sun,Jun Shu,Jianhua Ma,Fan Wang*

Main category: cs.CV

TL;DR: DuMeta++是一个无需配对纵向数据的双元学习框架，通过元特征学习和元初始化学习实现跨年龄脑组织分割的泛化，在few-shot设置下优于现有方法


<details>
  <summary>Details</summary>
Motivation: 脑MRI组织分割在神经科学和临床应用中至关重要，但由于年龄相关的脑部外观和形态变化，实现跨生命周期的稳定性能具有挑战性。现有方法需要配对的纵向数据，而这些数据在实践中往往不可得

Method: 提出DuMeta++双元学习框架：1) 元特征学习提取年龄无关的时空演化脑结构语义表示；2) 元初始化学习实现数据高效的分割模型适应；3) 基于记忆库的类感知正则化策略强制纵向一致性，无需显式纵向监督

Result: 在iSeg-2019、IBIS、OASIS、ADNI等多个数据集上的few-shot实验表明，DuMeta++在跨年龄泛化方面优于现有方法，并理论证明了算法的收敛性

Conclusion: DuMeta++成功解决了无配对纵向数据下的跨年龄脑组织分割问题，通过双元学习和记忆库正则化实现了优异的泛化性能，为临床实践中缺乏纵向数据的情况提供了有效解决方案

Abstract: Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.

</details>


### [39] [Condition Matters in Full-head 3D GANs](https://arxiv.org/abs/2602.07198)
*Heyuan Li,Huimin Zhang,Yuda Qiu,Zhengwentai Sun,Keru Zheng,Lingteng Qiu,Peihao Li,Qi Zuo,Ce Chen,Yujian Zheng,Yuming Gu,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 提出使用视角不变语义特征作为条件输入来训练全头部3D GAN，解决传统基于视角条件导致的生成偏差和全局不一致性问题，通过构建合成头部图像数据集实现语义对齐，显著提升了生成质量、多样性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统全头部3D GAN使用视角角度作为条件输入会导致学习到的3D头部空间沿条件视角方向产生偏差，造成条件视角与非条件视角之间生成质量和多样性的显著差异，导致不同头部区域的全局不一致性。

Method: 构建新颖的合成头部图像数据集，利用FLUX.1 Kontext将现有高质量正面人脸数据集扩展到多视角，使用正面视角提取的图像clip特征作为所有扩展图像的共享语义条件，确保语义对齐并消除方向偏差。

Result: 在全头部合成和单视角GAN反转的广泛实验中，该方法实现了显著更高的保真度、多样性和泛化能力，语义条件促进了生成器遵循真实语义分布，实现持续学习和多样化生成。

Conclusion: 视角不变语义条件成功解耦了3D头部生成能力与视角方向的关联，解决了传统方法的偏差问题，通过语义对齐监督加速训练并增强生成3D头部的全局一致性，为3D GAN训练提供了更有效的条件策略。

Abstract: Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.

</details>


### [40] [Understanding Real-World Traffic Safety through RoadSafe365 Benchmark](https://arxiv.org/abs/2602.07212)
*Xinyu Liu,Darryl C. Jacob,Yuxin Liu,Xinsong Du,Muchao Ye,Bolei Zhou,Pan He*

Main category: cs.CV

TL;DR: RoadSafe365是一个大规模视觉语言基准数据集，专门用于细粒度交通安全分析，包含36,196个标注视频片段和864K候选选项，通过层次化分类法系统评估交通安全标准。


<details>
  <summary>Details</summary>
Motivation: 现有交通基准缺乏与官方安全标准的系统性评估，需要填补这一空白，以支持数据驱动的交通安全理解系统。

Method: 使用层次化分类法对碰撞、事故和违规行为进行精细化定义，从行车记录仪和监控摄像头收集多样化真实视频数据，提供丰富的属性标注和多选题问答集。

Result: 建立了强大的基线模型，在RoadSafe365上微调获得一致性能提升，跨域实验验证了其有效性，支持大规模训练和标准化评估。

Conclusion: RoadSafe365为现实世界交通安全分析的可重复研究提供了全面基准，有效连接了官方安全标准与数据驱动系统。

Abstract: Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.

</details>


### [41] [The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models](https://arxiv.org/abs/2602.07251)
*Haley Duba-Sullivan,Steven R. Young,Emma J. Reid*

Main category: cs.CV

TL;DR: AdvSR是一种新型模型级对抗攻击框架，通过在超分辨率模型训练时嵌入对抗行为，无需推理时输入访问即可导致下游分类器误分类，且不影响正常图像质量评估。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的超分辨率方法常作为成像管道预处理步骤，但此前未研究其作为攻击表面的潜力。现有攻击多针对输入扰动或后门触发，而AdvSR探索模型权重级别的隐蔽攻击。

Method: 联合优化重建质量和目标对抗效果，在SR模型训练期间直接嵌入对抗行为。评估了SRCNN、EDSR、SwinIR三种超分辨率架构与YOLOv11分类器的组合。

Result: AdvSR模型在标准图像质量指标下表现正常，但能实现高攻击成功率，且质量退化最小。

Conclusion: 该研究揭示了成像管道中新的模型级威胁，对安全关键应用中模型采购和验证实践具有重要影响。

Abstract: Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.

</details>


### [42] [3D Transport-based Morphometry (3D-TBM) for medical image analysis](https://arxiv.org/abs/2602.07260)
*Hongyu Kan,Kristofor Pas,Ivan Medri,Naqib Sad Pathan,Natasha Ironside,Shinjini Kundu,Jingjia He,Gustavo Kunde Rohde*

Main category: cs.CV

TL;DR: 3D-TBM是一个基于传输形态测量的3D医学图像分析工具，通过可逆变换将图像嵌入传输域进行分析，并支持将结果映射回原始图像空间进行空间解释。


<details>
  <summary>Details</summary>
Motivation: 促进传输形态测量(TBM)在临床影像研究中的更广泛应用，为研究人员提供易用的形态学分析工具。

Method: 开发包含数据预处理、最优传输嵌入计算、主要传输方向可视化、判别方向识别等分析方法的完整框架。

Result: 成功开发了3D-TBM工具，提供全面的文档和实践教程，源代码通过PyTransKit公开可用。

Conclusion: 3D-TBM为医学影像研究提供了有效的传输形态测量分析工具，支持临床特征的直接空间解释，有助于推动TBM在临床研究中的应用。

Abstract: Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.

</details>


### [43] [TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition](https://arxiv.org/abs/2602.07262)
*Junbo Jacob Lian,Feng Xiong,Yujun Sun,Kaichen Ouyang,Mingyang Yu,Shengwei Fu,Zhong Rui,Zhang Yujun,Huiling Chen*

Main category: cs.CV

TL;DR: TwistNet-2D是一个轻量级模块，通过局部成对通道乘积和方向性空间位移来联合编码特征共现位置和交互方式，在纹理识别任务上以极低的计算开销超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在根本性矛盾：双线性池化和Gram矩阵捕获全局通道相关性但破坏空间结构，而自注意力通过加权聚合而非显式成对特征交互来建模空间上下文。

Method: 提出Spiral-Twisted Channel Interaction (STCI)核心组件，在沿预定方向移动一个特征图后进行逐通道元素乘法，捕获结构化纹理的跨位置共现模式；通过四个方向头聚合学习通道重加权，并通过sigmoid门控残差路径注入结果。

Result: 仅增加ResNet-18的3.5%参数和2%FLOPs，但在四个纹理和细粒度识别基准上持续超越参数匹配和显著更大的基线模型，包括ConvNeXt、Swin Transformer和混合CNN-Transformer架构。

Conclusion: TwistNet-2D通过局部成对通道交互有效解决了纹理识别中全局相关性与空间结构保持的矛盾，以极低的计算成本实现了显著的性能提升，为纹理分析提供了新的有效方法。

Abstract: Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.

</details>


### [44] [VideoNeuMat: Neural Material Extraction from Generative Video Models](https://arxiv.org/abs/2602.07272)
*Bowen Xue,Saeed Hadadan,Zheng Zeng,Fabrice Rousselle,Zahra Montazeri,Milos Hasan*

Main category: cs.CV

TL;DR: VideoNeuMat是一个两阶段流水线，从视频扩散模型中提取可重用的神经材质资产，通过受控相机和光照轨迹生成材质样本视频，并重建为紧凑的神经材质表示。


<details>
  <summary>Details</summary>
Motivation: 创建逼真的3D渲染材质需要高超的艺术技能，现有生成模型受限于高质量训练数据的缺乏，视频生成模型中的材质知识仍然与几何和光照纠缠在一起。

Method: 1. 微调大型视频模型(Wan 2.1 14B)生成受控相机和光照轨迹下的材质样本视频；2. 从较小视频骨干(Wan 1.3B)微调大型重建模型(LRM)，从17个生成视频帧进行单次推理预测神经材质参数。

Result: 生成的材质在真实感和多样性方面远超有限的合成训练数据，能够泛化到新的观察和光照条件。

Conclusion: 材质知识可以成功从互联网规模的视频模型转移到独立的、可重用的神经3D资产中。

Abstract: Creating photorealistic materials for 3D rendering requires exceptional artistic skill. Generative models for materials could help, but are currently limited by the lack of high-quality training data. While recent video generative models effortlessly produce realistic material appearances, this knowledge remains entangled with geometry and lighting. We present VideoNeuMat, a two-stage pipeline that extracts reusable neural material assets from video diffusion models. First, we finetune a large video model (Wan 2.1 14B) to generate material sample videos under controlled camera and lighting trajectories, effectively creating a "virtual gonioreflectometer" that preserves the model's material realism while learning a structured measurement pattern. Second, we reconstruct compact neural materials from these videos through a Large Reconstruction Model (LRM) finetuned from a smaller Wan 1.3B video backbone. From 17 generated video frames, our LRM performs single-pass inference to predict neural material parameters that generalize to novel viewing and lighting conditions. The resulting materials exhibit realism and diversity far exceeding the limited synthetic training data, demonstrating that material knowledge can be successfully transferred from internet-scale video models into standalone, reusable neural 3D assets.

</details>


### [45] [Cross-View World Models](https://arxiv.org/abs/2602.07277)
*Rishabh Sharma,Gijs Hogervorst,Wayne E. Mackey,David J. Heeger,Stefano Martiniani*

Main category: cs.CV

TL;DR: XVWM通过跨视角预测目标训练世界模型，利用多视角一致性作为几何正则化，学习环境3D结构的视角不变表示，使智能体能够在不同参考系中进行规划。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型通常采用单一自我中心视角，但某些任务（如导航）从鸟瞰视角规划更有效，需要开发支持多视角预测的世界模型。

Method: 使用跨视角预测目标：给定一个视角的帧序列，预测执行动作后相同或不同视角的未来状态。利用Aimlabs平台的多视角游戏数据进行训练，提供精确对齐的多摄像头记录和高频动作标签。

Result: 模型能够为智能体提供跨视角的并行想象流，在最适合任务的参考系中进行规划，同时从自我中心视角执行。多视角一致性为空间基础表示提供了强大的学习信号。

Conclusion: 跨视角世界模型通过几何正则化学习视角不变表示，支持多视角规划，为多智能体设置中的视角采择奠定了基础。

Abstract: World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.

</details>


### [46] [Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms](https://arxiv.org/abs/2602.07301)
*Aruna Jithesh,Chinmayi Karumuri,Venkata Kiran Reddy Kotha,Meghana Doddapuneni,Taehee Jeong*

Main category: cs.CV

TL;DR: 提出了一种基于注意力机制的DeepLab-V3+模型，用于糖尿病视网膜病变四种病变的像素级分割，在DDR数据集上实现了mAP和IoU指标的显著提升，特别是对微动脉瘤的检测有临床意义的改进。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变可能导致视力丧失，早期筛查至关重要。现有深度学习算法在病变分割方面的临床适用性有限，需要提供像素级注释来支持眼科医生的筛查工作。

Method: 使用Attention-DeepLab模型，将注意力机制与DeepLab-V3+相结合，在757张DDR数据集图像上分割微动脉瘤、软性渗出物、硬性渗出物和出血四种DR相关病变。

Result: 相比基线模型，Attention-DeepLab将mAP从0.3010提升至0.3326，mIoU从0.1791提升至0.1928。微动脉瘤检测从0.0205提升至0.0763，这是DR最早可见症状的重要改进。

Conclusion: 所提出的注意力增强DeepLab-V3+模型在DR病变分割方面表现出色，特别是对早期病变微动脉瘤的检测有显著临床价值，为眼科医生提供了有效的像素级筛查支持。

Abstract: Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.

</details>


### [47] [Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing](https://arxiv.org/abs/2602.07310)
*Kyle Williams,Andrew Seltzman*

Main category: cs.CV

TL;DR: 提出基于线性遗传编程的过滤分割算法，用于自动检测FIB显微图像中的析出物，替代人工标注，加速铜合金开发迭代。


<details>
  <summary>Details</summary>
Motivation: 当前铌基铜合金增材制造的显微图像分析依赖人工标注，由于对比度变化、噪声和图像伪影等问题，严重拖慢了合金开发迭代速度。

Method: 使用线性遗传编程优化图像过滤分割算法，采用特定领域语言构建图像处理块序列，通过遗传算法生成和变异可调参数的程序。

Result: 在60种群规模和最多5个处理块的理想条件下，系统找到接近人工精度的解决方案，像素级XOR误差评估平均为1.8%，处理360万像素图像约需2秒。

Conclusion: 该自动化方法显著加速了材料成分和工艺空间的探索，有助于开发用于增材制造聚变堆部件的高强度、低活化、析出强化铜合金。

Abstract: Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.

</details>


### [48] [LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery](https://arxiv.org/abs/2602.07311)
*Difei Gu,Yunhe Gao,Gerasimos Chatzoudis,Zihan Dong,Guoning Zhang,Bangwei Guo,Yang Zhou,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

TL;DR: LUCID是一种统一的视觉-语言稀疏自编码器，学习图像块和文本标记表示的共享潜在字典，同时保留模态特定细节的私有容量。通过最优传输匹配目标实现特征对齐，无需标注即可获得可解释的共享特征，支持跨模态概念发现和自动化字典解释。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器按模态单独训练，产生的特征字典不可直接理解且无法跨域迁移。需要开发统一的跨模态表示学习方法，实现可解释的概念发现和跨域解释迁移。

Method: 提出LUCID框架：1）学习共享潜在字典处理图像块和文本标记表示；2）保留私有容量处理模态特定细节；3）使用最优传输匹配目标实现特征对齐；4）开发基于术语聚类的自动化字典解释流程。

Result: LUCID产生可解释的共享特征，支持：1）补丁级定位；2）跨模态神经元对应；3）增强对概念聚类问题的鲁棒性；4）捕获超越物体的多样化语义类别（动作、属性、抽象概念）。

Conclusion: LUCID提供了一种全面的可解释多模态表示方法，通过学习统一的视觉-语言稀疏编码，实现了跨模态的概念发现和解释，为多模态理解提供了新的技术路径。

Abstract: Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.

</details>


### [49] [Seeing Roads Through Words: A Language-Guided Framework for RGB-T Driving Scene Segmentation](https://arxiv.org/abs/2602.07343)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.CV

TL;DR: CLARITY是一种动态RGB-热成像融合方法，通过视觉语言模型先验自适应调整融合策略，在恶劣光照条件下实现鲁棒的道路场景语义分割，在MFNet数据集上达到62.3% mIoU和77.5% mAcc的新SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-热成像融合方法采用静态融合策略，导致模态特定噪声在网络中传播，无法适应不同光照条件，影响自动驾驶场景分割的鲁棒性。

Method: 提出CLARITY框架：1) 利用视觉语言模型先验动态调制各模态贡献度；2) 保留暗物体有效语义信息；3) 采用分层解码器确保跨尺度结构一致性以锐化细物体边界。

Result: 在MFNet数据集上达到62.3% mIoU和77.5% mAcc，建立了新的state-of-the-art性能。

Conclusion: 动态融合策略比静态方法更有效，视觉语言模型先验能显著提升恶劣光照条件下的语义分割性能，提出的两种机制有效解决了噪声抑制和边界锐化问题。

Abstract: Robust semantic segmentation of road scenes under adverse illumination, lighting, and shadow conditions remain a core challenge for autonomous driving applications. RGB-Thermal fusion is a standard approach, yet existing methods apply static fusion strategies uniformly across all conditions, allowing modality-specific noise to propagate throughout the network. Hence, we propose CLARITY that dynamically adapts its fusion strategy to the detected scene condition. Guided by vision-language model (VLM) priors, the network learns to modulate each modality's contribution based on the illumination state while leveraging object embeddings for segmentation, rather than applying a fixed fusion policy. We further introduce two mechanisms, i.e., one which preserves valid dark-object semantics that prior noise-suppression methods incorrectly discard, and a hierarchical decoder that enforces structural consistency across scales to sharpen boundaries on thin objects. Experiments on the MFNet dataset demonstrate that CLARITY establishes a new state-of-the-art (SOTA), achieving 62.3% mIoU and 77.5% mAcc.

</details>


### [50] [Optimizing Few-Step Generation with Adaptive Matching Distillation](https://arxiv.org/abs/2602.07345)
*Lichen Bai,Zikai Zhou,Shitong Shao,Wenliang Zhong,Shuo Yang,Shuo Chen,Bojun Chen,Zeke Xie*

Main category: cs.CV

TL;DR: 提出自适应匹配蒸馏（AMD）方法，通过奖励代理显式检测和逃离Forbidden Zone区域，采用结构信号分解动态优先校正梯度，并引入排斥景观锐化来防止失败模式崩溃，显著提升生成模型的样本保真度和训练鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Distribution Matching Distillation（DMD）加速范式在Forbidden Zone区域存在稳定性问题，真实教师提供不可靠指导而虚假教师排斥力不足，需要解决这些受损区域的优化问题。

Method: 提出统一优化框架重新解释现有方法，引入AMD自校正机制：1）使用奖励代理显式检测Forbidden Zone；2）通过结构信号分解动态优先校正梯度；3）采用排斥景观锐化强制建立陡峭能量屏障防止失败模式崩溃。

Result: 在图像和视频生成任务（SDXL、Wan2.1）和基准测试（VBench、GenEval）中，AMD显著提升样本保真度和训练鲁棒性，SDXL的HPSv2分数从30.64提高到31.25，优于最先进基线方法。

Conclusion: 显式校正Forbidden Zone内的优化轨迹对于提升少步生成模型的性能上限至关重要，AMD方法为此提供了有效解决方案。

Abstract: Distribution Matching Distillation (DMD) is a powerful acceleration paradigm, yet its stability is often compromised in Forbidden Zone, regions where the real teacher provides unreliable guidance while the fake teacher exerts insufficient repulsive force. In this work, we propose a unified optimization framework that reinterprets prior art as implicit strategies to avoid these corrupted regions. Based on this insight, we introduce Adaptive Matching Distillation (AMD), a self-correcting mechanism that utilizes reward proxies to explicitly detect and escape Forbidden Zones. AMD dynamically prioritizes corrective gradients via structural signal decomposition and introduces Repulsive Landscape Sharpening to enforce steep energy barriers against failure mode collapse. Extensive experiments across image and video generation tasks (e.g., SDXL, Wan2.1) and rigorous benchmarks (e.g., VBench, GenEval) demonstrate that AMD significantly enhances sample fidelity and training robustness. For instance, AMD improves the HPSv2 score on SDXL from 30.64 to 31.25, outperforming state-of-the-art baselines. These findings validate that explicitly rectifying optimization trajectories within Forbidden Zones is essential for pushing the performance ceiling of few-step generative models.

</details>


### [51] [Row-Column Separated Attention Based Low-Light Image/Video Enhancement](https://arxiv.org/abs/2602.07428)
*Chengqi Dong,Zhiyuan Cao,Tuoshi Qi,Kexin Wu,Yixing Gao,Fan Tang*

Main category: cs.CV

TL;DR: 提出一种结合改进U-Net和行列分离注意力模块(RCSA)的低光图像/视频增强方法，通过全局信息引导局部信息，减少参数并保持时间一致性


<details>
  <summary>Details</summary>
Motivation: 传统U-Net在低光增强中缺乏全局信息指导，导致局部噪声和细节丢失；注意力机制能更好利用全局信息但参数和计算量过大

Method: 在改进U-Net后插入行列分离注意力模块(RCSA)，输入为特征图行列的均值和最大值，用较少参数实现全局信息指导；提出两种时间损失函数用于视频增强的时间一致性保持

Result: 在LOL、MIT Adobe FiveK图像数据集和SDSD视频数据集上的大量实验证明了方法的有效性

Conclusion: RCSA模块能有效利用全局信息指导低光增强，在减少参数的同时保持时间一致性，为低光图像和视频增强提供了有效解决方案

Abstract: U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.

</details>


### [52] [Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction](https://arxiv.org/abs/2602.07444)
*Ondrej Hlinka,Georg Kaniak,Christian Kapeller*

Main category: cs.CV

TL;DR: 提出了一种透视感知的对数深度融合方法，用于从单视角相机获取的深度和法向图重建3D表面，通过显式处理透视投影实现度量精确的3D重建，并能利用法向信息修复缺失深度测量。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的深度-法向融合方法主要针对正交投影，而实际传感器系统基于透视相机，需要处理透视投影效应以获得度量精确的3D重建。

Method: 透视感知对数深度融合方法，扩展了现有的正交梯度基深度-法向融合技术，显式考虑透视投影，同时利用表面法向信息修复深度图中的缺失测量。

Result: 在DiLiGenT-MV数据集上的实验证明了该方法的有效性，并凸显了透视感知深度-法向融合的重要性。

Conclusion: 该方法成功解决了透视投影下的深度-法向融合问题，实现了度量精确的3D表面重建，为基于单视角相机的传感器系统提供了有效的重建解决方案。

Abstract: We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.

</details>


### [53] [PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization](https://arxiv.org/abs/2602.07446)
*Naqcho Ali Mehdi*

Main category: cs.CV

TL;DR: PTB-XL-Image-17K是一个包含17,271个高质量12导联心电图图像的合成数据集，提供五种互补数据类型，支持心电图数字化研究的完整流程。


<details>
  <summary>Details</summary>
Motivation: 解决心电图数字化研究中缺乏大规模同时包含心电图图像和对应真实信号标注数据集的问题，以利用数十年的临床遗留数据进行深度学习应用。

Method: 基于PTB-XL信号数据库生成合成心电图图像，提供开源Python框架实现可定制化数据集生成，包括纸张速度、电压标度、采样率、网格外观和波形特征等可控参数。

Result: 成功生成17,271个样本，达到100%生成成功率，平均处理时间为每样本1.35秒，提供完整的五类数据标注。

Conclusion: PTB-XL-Image-17K填补了心电图数字化研究的关键空白，为导联检测、波形分割和信号提取提供了首个大规模完整真实标注资源，支持严格评估。

Abstract: Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.

</details>


### [54] [SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads](https://arxiv.org/abs/2602.07449)
*Tan Yu,Qian Qiao,Le Shen,Ke Zhou,Jincheng Hu,Dian Sheng,Bo Hu,Haoming Qin,Jun Gao,Changhai Zhou,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: SoulX-FlashHead是一个1.3B参数的实时高保真音频驱动人像生成框架，通过流式时空预训练和双向蒸馏技术，解决了长序列生成中的稳定性问题，在HDTF和VFHQ基准上达到SOTA性能，Lite版本在RTX 4090上实现96 FPS推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决音频驱动人像生成中高保真视觉质量与低延迟流式传输之间的平衡问题，现有大型模型计算成本过高，轻量级方案则牺牲了面部表示完整性和时间稳定性。

Method: 提出Streaming-Aware Spatiotemporal Pre-training（配备时序音频上下文缓存机制）确保短音频片段稳健特征提取；Oracle-Guided Bidirectional Distillation利用真实运动先验提供精确物理指导；构建VividHead数据集（782小时严格对齐素材）。

Result: 在HDTF和VFHQ基准测试中达到最先进性能，Lite变体在单张NVIDIA RTX 4090上实现96 FPS推理速度，支持超快速交互且不牺牲视觉连贯性。

Conclusion: SoulX-FlashHead成功实现了实时、无限长度、高保真的流式视频生成，在计算效率和视觉质量之间取得了良好平衡，为音频驱动人像生成提供了实用的解决方案。

Abstract: Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.

</details>


### [55] [SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning](https://arxiv.org/abs/2602.07458)
*Yancheng Long,Yankai Yang,Hongyang Wei,Wei Chen,Tianke Zhang,Haonan fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.CV

TL;DR: SpatialReward是一个通过空间推理解决在线强化学习中奖励信号稀缺问题的模型，在多个基准测试中达到最先进性能，显著提升图像编辑的评估准确性和强化学习效果


<details>
  <summary>Details</summary>
Motivation: 现有评估器存在'注意力崩溃'问题，即模型忽视跨图像比较和细粒度细节，导致感知不准确和评分失准，需要更可靠的细粒度奖励信号

Method: 提出SpatialReward奖励模型，通过显式空间推理进行精确验证，将语义判断锚定在预测的编辑区域像素级证据上，使用26万空间感知数据集训练

Result: 在MMRB2和EditReward-Bench上达到最先进性能，在MultiEditReward-Bench上超越专有评估器，在在线RL中使OmniGen2在GEdit-Bench上提升+0.90，超越领先判别模型和GPT-4.1

Conclusion: 空间推理对于实现图像编辑中的有效对齐至关重要，SpatialReward为解决奖励信号稀缺问题提供了有效解决方案

Abstract: Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term "Attention Collapse," where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.

</details>


### [56] [GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring](https://arxiv.org/abs/2602.07463)
*Misbah Ijaz,Saif Ur Rehman Khan,Abd Ur Rehman,Tayyaba Asif,Sebastian Vollmer,Andreas Dengel,Muhammad Nabeel Asim*

Main category: cs.CV

TL;DR: 提出GlobalWasteData (GWD)数据集，整合多个公开垃圾分类数据集，提供89,807张图像、14个主类别和68个子类别的统一标注，解决现有数据集碎片化、不一致和偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有垃圾分类数据集存在碎片化、标注不一致、图像条件差异和类别分布偏差等问题，难以训练泛化能力强的模型，需要构建统一的大规模数据集。

Method: 整合多个公开数据集，进行质量过滤、去重和元数据生成等预处理，创建包含89,807张图像的GWD数据集，统一标注标准和类别体系。

Result: 构建了GWD数据集，提供一致的标注、改进的领域多样性和更平衡的类别分布，为开发鲁棒的垃圾识别模型提供基础。

Conclusion: GWD数据集为环境监测、回收自动化和垃圾识别等机器学习应用提供了可靠基础，促进未来研究和可重复性。

Abstract: The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.

</details>


### [57] [Thermal odometry and dense mapping using learned ddometry and Gaussian splatting](https://arxiv.org/abs/2602.07493)
*Tianhao Zhou,Yujia Chen,Zhihao Zhan,Yuhang Ming,Jianzhu Huai*

Main category: cs.CV

TL;DR: TOM-GS：首个基于高斯泼溅的热成像SLAM系统，结合学习里程计与稠密建图，在恶劣条件下实现鲁棒的运动估计和高质量重建


<details>
  <summary>Details</summary>
Motivation: 现有热成像里程计和建图方法主要基于几何方法，在多样化数据集上表现不佳且无法生成稠密地图，而高斯泼溅技术具有高效和高品质重建能力

Method: 集成学习式里程计与基于高斯泼溅的稠密建图，包含专门的热图像增强和单目深度集成模块

Result: 在运动估计和新视角渲染实验中显著优于现有学习方法，验证了学习式流程对鲁棒热成像里程计和稠密重建的优势

Conclusion: TOM-GS成功展示了基于学习的方法在热成像SLAM中的有效性，为恶劣环境下的视觉导航提供了新解决方案

Abstract: Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.

</details>


### [58] [Learning Brain Representation with Hierarchical Visual Embeddings](https://arxiv.org/abs/2602.07495)
*Jiawen Zheng,Haonan Jia,Ming Li,Yuhui Zheng,Yufeng Zeng,Yang Gao,Chen Liang*

Main category: cs.CV

TL;DR: 提出一种利用多预训练视觉编码器捕获分层多尺度视觉表示的大脑-图像对齐策略，通过对比学习实现脑信号与视觉嵌入的有效对齐，并引入融合先验增强跨模态分布一致性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉解码方法大多关注高层语义特征而忽略像素级细节，限制了人类视觉系统的理解，需要探索更有效的脑信号视觉信息编码策略。

Method: 使用多个具有不同归纳偏好的预训练视觉编码器捕获分层多尺度表示，采用对比学习目标实现脑信号与视觉嵌入对齐，引入在大规模视觉数据上学习的融合先验来匹配脑特征。

Result: 大量定量和定性实验表明，该方法在检索准确性和重建保真度之间取得了良好的平衡。

Conclusion: 所提出的多编码器对齐策略和融合先验方法有效提升了脑信号视觉解码的性能，为理解人类视觉系统提供了新视角。

Abstract: Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.

</details>


### [59] [IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation](https://arxiv.org/abs/2602.07498)
*Zhufeng Xu,Xuan Gao,Feng-Lin Liu,Haoxian Zhang,Zhixue Fang,Yu-Kun Lai,Xiaoqiang Liu,Pengfei Wan,Lin Gao*

Main category: cs.CV

TL;DR: 提出一种新颖的隐式运动表示方法IM-Animation，通过将逐帧运动压缩为紧凑的1D运动token，解决现有显式和隐式方法在角色动画中的空间错配、身份泄漏和运动-外观纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 现有显式方法（如骨架、DWPose）难以处理空间错配和身体比例变化，而隐式方法虽然能捕获高层运动语义，但存在身份信息泄漏和运动-外观纠缠问题。

Method: 设计1D运动token表示放松2D表示的空间约束；提出基于时间一致掩码token的重定向模块，建立时间训练瓶颈；采用三阶段训练策略提高训练效率和保真度。

Result: 大量实验表明，该方法在生成能力上达到或超越了现有最先进方法的性能。

Conclusion: 提出的隐式运动表示和IM-Animation框架有效解决了角色动画中的关键挑战，在保持身份一致性的同时实现了高质量的运动重定向。

Abstract: Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.

</details>


### [60] [Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection](https://arxiv.org/abs/2602.07512)
*Tao Wang,Chenyu Lin,Chenwei Tang,Jizhe Zhou,Deng Xiong,Jianan Li,Jian Zhao,Jiancheng Lv*

Main category: cs.CV

TL;DR: ZoomDet是一个针对无人机图像小目标检测的自适应放大框架，通过非均匀放大目标区域来提升检测性能，具有架构无关性和高效性。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的目标通常较小且稀疏，这阻碍了有效目标检测器的优化，需要通过自适应放大目标来更好地捕捉目标特征。

Method: 提出轻量级偏移预测方案和基于边界框的放大目标，学习输入图像的非均匀放大变换；采用角点对齐的边界框变换方法，在训练时将真实边界框变换到放大空间，推理时将预测边界框变换回原始空间。

Result: 在VisDrone、UAVDT和SeaDronesSee三个无人机数据集上进行了广泛实验，其中在SeaDronesSee数据集上使用Faster R-CNN模型获得了8.4%的mAP绝对提升，仅增加约3ms延迟。

Conclusion: ZoomDet是一个简单高效的框架，能够显著提升无人机图像小目标检测性能，且可与任意目标检测架构兼容，具有很好的实用价值。

Abstract: Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.

</details>


### [61] [CA-YOLO: Cross Attention Empowered YOLO for Biomimetic Localization](https://arxiv.org/abs/2602.07523)
*Zhen Zhang,Qing Zhao,Xiuhe Li,Cheng Wang,Guoqiang Zhu,Yu Zhang,Yining Huo,Hongyi Yu,Yi Zhang*

Main category: cs.CV

TL;DR: 提出基于CA-YOLO的仿生稳定定位系统，通过集成仿生模块和小目标检测头，结合前庭眼反射启发的云台跟踪策略，显著提升目标定位精度和小目标识别能力


<details>
  <summary>Details</summary>
Motivation: 现代复杂环境中现有系统在定位精度和小目标识别能力方面存在局限，需要更准确高效的目标定位解决方案

Method: 在YOLO骨干网络中集成仿生模块，包括小目标检测头和特征融合注意力机制(CFAM)；开发基于前庭眼反射的仿生云台跟踪控制策略，包含中心定位、稳定性优化、自适应控制系数调整和智能重捕获功能

Result: CA-YOLO在COCO和VisDrone数据集上平均精度分别提升3.94%和4.90%，时间敏感目标定位实验验证了系统的有效性和实用性

Conclusion: 该仿生稳定定位系统成功结合了生物视觉机制和前庭反射原理，为复杂环境下的目标定位提供了有效的技术解决方案，在精度和实用性方面表现出色

Abstract: In modern complex environments, achieving accurate and efficient target localization is essential in numerous fields. However, existing systems often face limitations in both accuracy and the ability to recognize small targets. In this study, we propose a bionic stabilized localization system based on CA-YOLO, designed to enhance both target localization accuracy and small target recognition capabilities. Acting as the "brain" of the system, the target detection algorithm emulates the visual focusing mechanism of animals by integrating bionic modules into the YOLO backbone network. These modules include the introduction of a small target detection head and the development of a Characteristic Fusion Attention Mechanism (CFAM). Furthermore, drawing inspiration from the human Vestibulo-Ocular Reflex (VOR), a bionic pan-tilt tracking control strategy is developed, which incorporates central positioning, stability optimization, adaptive control coefficient adjustment, and an intelligent recapture function. The experimental results show that CA-YOLO outperforms the original model on standard datasets (COCO and VisDrone), with average accuracy metrics improved by 3.94%and 4.90%, respectively.Further time-sensitive target localization experiments validate the effectiveness and practicality of this bionic stabilized localization system.

</details>


### [62] [Evaluating Object-Centric Models beyond Object Discovery](https://arxiv.org/abs/2602.07532)
*Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 提出了一种新的目标中心学习(OCL)评估框架，使用指令调优的视觉语言模型作为评估器，通过统一的任务和指标同时评估目标定位能力和表示有用性。


<details>
  <summary>Details</summary>
Motivation: 现有OCL评估方法存在两个局限：1)无法充分评估表示的有用性；2)定位能力和表示有用性使用分离的指标评估，导致不一致性。

Method: 使用指令调优的视觉语言模型(VLMs)作为评估器，在多样化VQA数据集上进行可扩展基准测试；引入统一评估任务和指标，联合评估定位(where)和表示有用性(what)；包含多特征重建基线作为参考。

Result: 开发了新的评估框架，能够更全面地评估OCL模型在复杂推理任务中的表现，解决了现有评估方法的局限性。

Conclusion: 提出的评估方法为OCL模型提供了更全面和一致的评估标准，有助于更好地衡量模型在组合泛化和OOD鲁棒性方面的能力。

Abstract: Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.

</details>


### [63] [Fine-Grained Cat Breed Recognition with Global Context Vision Transformer](https://arxiv.org/abs/2602.07534)
*Mowmita Parvin Hera,Md. Shahriar Mahmud Kallol,Shohanur Rahman Nirob,Md. Badsha Bulbul,Jubayer Ahmed,M. Zhourul Islam,Hazrat Ali,Mohammmad Farhad Bulbul*

Main category: cs.CV

TL;DR: 基于GCViT-Tiny架构的深度学习模型在牛津-IIIT宠物数据集子集上实现猫品种分类，测试准确率92.00%，验证准确率94.54%，展示了Transformer架构在细粒度图像分类中的有效性。


<details>
  <summary>Details</summary>
Motivation: 猫品种识别因毛色图案、面部结构和颜色等细微差异而具有挑战性，需要开发高精度分类方法用于兽医诊断、动物收容所管理和移动端识别系统等应用场景。

Method: 采用Global Context Vision Transformer (GCViT)-Tiny架构，使用牛津-IIIT宠物数据集的高分辨率图像，通过旋转、水平翻转和亮度调整等数据增强技术提升模型泛化能力。

Result: 模型在测试集上达到92.00%的准确率，验证集准确率为94.54%，证明了Transformer架构在细粒度图像分类任务中的优越性能。

Conclusion: 研究成功验证了基于Transformer的GCViT架构在猫品种识别中的有效性，为相关应用提供了技术基础，并提供了Hugging Face演示平台供实际测试使用。

Abstract: Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.

</details>


### [64] [Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis](https://arxiv.org/abs/2602.07535)
*Md Sazidur Rahman,Kjersti Engan,Kathinka Dæhli Kurz,Mahdieh Khanmohammadi*

Main category: cs.CV

TL;DR: 提出双时间点分析框架，使用统计描述符、放射组学纹理特征和深度特征嵌入分析缺血性卒中组织的生物学异质性和时间演变，发现深度特征空间能够有效区分可挽救与不可挽救组织。


<details>
  <summary>Details</summary>
Motivation: 单时间点分割无法捕捉卒中的生物学异质性和时间演变过程，需要开发能够表征组织状态转变的分析方法。

Method: 使用入院时CTP和随访DWI的双时间点数据，提取统计、放射组学和深度特征（mJ-Net和nnU-Net），构建6个ROI区域，分析特征空间中的聚类模式。

Result: 深度特征空间（特别是mJ-Net）显示可挽救与不可挽救组织间存在显著分离，半暗带区域的特征根据最终状态呈现显著差异，而核心区域差异不显著。

Conclusion: 编码器衍生的特征流形反映了基础组织表型和状态转变，为基于影像的卒中演变量化提供了新见解。

Abstract: Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.

</details>


### [65] [LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing](https://arxiv.org/abs/2602.07540)
*Huimin Yan,Liang Bai,Xian Yang,Long Chen*

Main category: cs.CV

TL;DR: LGDEA方法通过LLM提取诊断证据构建共享空间，实现证据级别的跨模态对齐，显著减少对配对数据的依赖，在医学视觉-语言任务中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP式医学视觉-语言预训练方法存在全局对齐易受非诊断信息干扰、局部对齐无法整合关键诊断证据的问题，导致在配对数据有限场景下难以学习可靠诊断表示。

Method: 利用LLM从放射学报告中提取关键诊断证据，构建共享诊断证据空间，实现证据感知的跨模态对齐，有效利用大量未配对的医学图像和报告。

Result: 在短语定位、图像-文本检索和零样本分类任务上获得一致且显著的性能提升，性能甚至可与依赖大量配对数据的预训练方法相媲美。

Conclusion: 证据级别的对齐方法更符合医学诊断过程，能够有效缓解对配对数据的依赖，为医学视觉-语言预训练提供了新的有效途径。

Abstract: Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.

</details>


### [66] [MUFASA: A Multi-Layer Framework for Slot Attention](https://arxiv.org/abs/2602.07544)
*Sebastian Bock,Leonie Schüßler,Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: MUFASA是一个轻量级即插即用框架，通过在多层ViT特征上执行slot attention来提升无监督物体中心学习的分割性能，利用各层的语义信息并融合成统一表示，在多个数据集上达到新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前slot attention方法仅使用ViT最后一层特征，忽略了其他层包含的丰富语义信息，需要更好地利用这些潜在语义信息来改进无监督物体分割。

Method: 提出MUFASA框架，在ViT编码器的多层特征上计算slot attention，设计融合策略将多层获得的slot聚合成统一的物体中心表示，可集成到现有OCL方法中。

Result: 集成MUFASA显著提升了现有OCL方法的分割结果，在多个数据集上达到新的state of the art，同时改善了训练收敛性且仅带来轻微推理开销。

Conclusion: 通过充分利用ViT多层语义信息，MUFASA框架有效提升了slot attention方法的性能，证明了多层特征融合在无监督物体中心学习中的重要性。

Abstract: Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.

</details>


### [67] [Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation](https://arxiv.org/abs/2602.07550)
*Hussni Mohd Zakir,Eric Tatt Wei Ho*

Main category: cs.CV

TL;DR: FSSDINO是一个无需训练的少样本语义分割基线方法，使用冻结的DINOv3特征、类别特定原型和Gram矩阵精炼，在多个基准测试中与复杂方法竞争，揭示了基础模型中存在语义选择间隙问题。


<details>
  <summary>Details</summary>
Motivation: 研究冻结DINOv3特征的内在少样本语义分割能力，探索无需复杂解码器或测试时适应的最小化方法，并分析不同层级特征的性能差异。

Method: 提出FSSDINO方法：使用冻结的DINOv3特征提取器，通过类别特定原型进行分割，采用Gram矩阵精炼技术，在最终骨干层应用这一无需训练的方法。

Result: 在二元、多类和跨域少样本分割基准测试中，该方法与需要复杂解码器或测试时适应的专门方法具有竞争力，但发现标准最后一层特征与全局最优中间表示之间存在显著性能差距。

Conclusion: 研究确立了最后一层作为欺骗性强的基线，揭示了基础模型中存在的语义选择间隙问题，即传统启发式方法无法可靠识别高保真特征，同时证明了更高性能在理论上是可实现的。

Abstract: Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a "Safest vs. Optimal" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a "Semantic Selection Gap" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the "Last-Layer" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.

</details>


### [68] [FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation](https://arxiv.org/abs/2602.07554)
*Guandong Li,Yijun Ding*

Main category: cs.CV

TL;DR: FlexID是一个无需训练的新框架，通过意图感知调制解决个性化文本到图像生成中身份保真度与文本适应性的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有免训练方法依赖刚性视觉特征注入，导致身份保真度与文本适应性之间存在冲突，需要一种更灵活的解决方案。

Method: 提出FlexID框架，正交解耦身份为两个维度：语义身份投影器(SIP)在语言空间注入高级先验，视觉特征锚点(VFA)在潜在空间确保结构保真度。引入上下文感知自适应门控(CAG)机制，根据编辑意图和扩散时间步动态调制两个流权重。

Result: 在IBench数据集上的大量实验表明，FlexID在身份一致性和文本遵循度之间达到了最先进的平衡。

Conclusion: FlexID通过意图感知调制实现了身份保持与语义变化的协同，为复杂叙事生成提供了高效解决方案。

Abstract: Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.

</details>


### [69] [VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation](https://arxiv.org/abs/2602.07555)
*Francesco Taioli,Shiping Yang,Sonia Raychaudhuri,Marco Cristani,Unnat Jain,Angel X Chang*

Main category: cs.CV

TL;DR: 提出了一个3B参数的视觉-语言-动作代理，通过显式图像推理进行目标识别和动作选择，替代多模型流水线，提升可解释性、泛化能力和导航效率


<details>
  <summary>Details</summary>
Motivation: 现有方法存在泛化能力差、缺乏可解释性、错误传播、计算成本高和难以整合推理到导航策略等问题

Method: 采用三阶段推理过程（思考、思考总结、动作），通过显式图像推理直接回答目标对象识别和动作选择问题

Result: 实现了改进的可解释性、更强的泛化能力和更高效的导航性能

Conclusion: 该紧凑型VLA代理通过人类化的具身推理，有效解决了现有方法的局限性，为语言驱动对象导航提供了新方案

Abstract: Language-driven object navigation requires agents to interpret natural language descriptions of target objects, which combine intrinsic and extrinsic attributes for instance recognition and commonsense navigation. Existing methods either (i) use end-to-end trained models with vision-language embeddings, which struggle to generalize beyond training data and lack action-level explainability, or (ii) rely on modular zero-shot pipelines with large language models (LLMs) and open-set object detectors, which suffer from error propagation, high computational cost, and difficulty integrating their reasoning back into the navigation policy. To this end, we propose a compact 3B-parameter Vision-Language-Action (VLA) agent that performs human-like embodied reasoning for both object recognition and action selection, removing the need for stitched multi-model pipelines. Instead of raw embedding matching, our agent employs explicit image-grounded reasoning to directly answer "Is this the target object?" and "Why should I take this action?" The reasoning process unfolds in three stages: "think", "think summary", and "action", yielding improved explainability, stronger generalization, and more efficient navigation. Code and dataset available upon acceptance.

</details>


### [70] [SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens](https://arxiv.org/abs/2602.07564)
*Xiaoyan Zhang,Zechen Bai,Haofan Wang,Yiren Song*

Main category: cs.CV

TL;DR: SIGMA是一个统一的后训练框架，通过引入选择性多属性令牌和交错多条件生成，扩展了Bagel等统一模型的能力，支持组合编辑、选择性属性迁移和多模态对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的统一模型如Bagel仅限于单条件输入，缺乏从多个异构源合成结果的灵活性，需要更灵活的跨条件生成能力。

Method: 提出选择性交错多属性令牌（风格、内容、主体和身份令牌），在Bagel统一骨干网络上进行70万交错样本的后训练，使模型能够解释和组合交错文本-图像序列中的多个视觉条件。

Result: 实验表明SIGMA在多样编辑和生成任务中显著提升了可控性、跨条件一致性和视觉质量，在组合任务上相比Bagel有显著提升。

Conclusion: SIGMA框架成功实现了扩散变换器中的交错多条件生成，为统一视觉任务模型提供了更强的灵活性和组合能力。

Abstract: Recent unified models such as Bagel demonstrate that paired image-edit data can effectively align multiple visual tasks within a single diffusion transformer. However, these models remain limited to single-condition inputs and lack the flexibility needed to synthesize results from multiple heterogeneous sources. We present SIGMA (Selective-Interleaved Generation with Multi-Attribute Tokens), a unified post-training framework that enables interleaved multi-condition generation within diffusion transformers. SIGMA introduces selective multi-attribute tokens, including style, content, subject, and identity tokens, which allow the model to interpret and compose multiple visual conditions in an interleaved text-image sequence. Through post-training on the Bagel unified backbone with 700K interleaved examples, SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment. Extensive experiments show that SIGMA improves controllability, cross-condition consistency, and visual quality across diverse editing and generation tasks, with substantial gains over Bagel on compositional tasks.

</details>


### [71] [Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025](https://arxiv.org/abs/2602.07565)
*Jingzhe Ma,Meng Zhang,Jianlong Yu,Kun Liu,Zunxiao Xu,Xue Cheng,Junjie Zhou,Yanfei Wang,Jiahang Li,Zepeng Wang,Kazuki Osamura,Rujie Liu,Narishige Abe,Jingjie Wang,Shunli Zhang,Haojun Xie,Jiajun Wu,Weiming Wu,Wenxiong Kang,Qingshuo Gao,Jiaming Xiong,Xianye Ben,Lei Chen,Lichen Song,Junjian Cui,Haijun Xiong,Junhao Lu,Bin Feng,Mengyuan Liu,Ji Zhou,Baoquan Zhao,Ke Xu,Yongzhen Huang,Liang Wang,Manuel J Marin-Jimenez,Md Atiqur Rahman Ahad,Shiqi Yu*

Main category: cs.CV

TL;DR: HID 2025竞赛在SUSTech-Competition数据集上取得了94.2%的准确率，创下新纪录，证明了步态识别算法在跨域泛化方面的持续进步。


<details>
  <summary>Details</summary>
Motivation: 解决远距离身份识别中传统生物特征难以获取的问题，步态识别提供了一种实用替代方案。竞赛旨在推动步态识别技术进步并提供公平评估平台。

Method: 采用SUSTech-Competition数据集，包含服装、携带物品和视角的显著变化。不提供专用训练数据，要求参与者使用外部数据集训练模型。每年使用不同随机种子生成评估分割以减少过拟合风险。

Result: 尽管难度增加，参与者仍实现了进一步改进，最佳方法达到94.2%的准确率，为该数据集设立了新基准。

Conclusion: HID 2025竞赛表明算法进步能够超越先前观察到的准确率限制，分析了关键技术趋势并概述了步态识别未来研究的潜在方向。

Abstract: Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.

</details>


### [72] [Cross-Camera Cow Identification via Disentangled Representation Learning](https://arxiv.org/abs/2602.07566)
*Runcheng Wang,Yaru Chen,Guiguo Zhang,Honghua Jiang,Yongliang Qiao*

Main category: cs.CV

TL;DR: 基于解耦表示学习的跨摄像头奶牛识别框架，通过子空间可识别性保证理论分解图像特征，实现86.0%的跨摄像头识别准确率，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有动物识别方法在跨摄像头场景下因光照、背景、视角和成像特性差异导致的性能严重下降问题，推动非接触式技术在动态真实农场环境中的大规模应用。

Method: 提出基于子空间可识别性保证理论的解耦表示学习框架，通过物理数据生成过程建模，设计特征解耦模块将观测图像分解为多个正交潜在子空间，分离出跨摄像头稳定的身份相关生物特征。

Result: 在五个不同摄像头节点构建的高质量数据集上，经过七个跨摄像头任务的广泛实验，平均准确率达到86.0%，显著优于源域基线(51.9%)和最强跨摄像头基线方法(79.8%)。

Conclusion: 建立了基于子空间理论的特征解耦框架，为无约束智能农场环境中的精确动物监测提供了新范式，有效解决了跨摄像头奶牛识别中的泛化问题。

Abstract: Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.

</details>


### [73] [Visualizing the Invisible: Enhancing Radiologist Performance in Breast Mammography via Task-Driven Chromatic Encoding](https://arxiv.org/abs/2602.07568)
*Hui Ye,Shilong Yang,Yexuan Xing,Juan Yu,Yaoqin Xie,Wei Zhang,Chulong Zhang*

Main category: cs.CV

TL;DR: MammoColor是一个端到端的乳腺X线摄影增强框架，通过任务驱动的色度编码（TDCE）将单通道乳腺图像转换为彩色增强视图，在乳腺密度较高的情况下显著提升检测性能，特别是在减少假阳性召回方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 乳腺X线摄影在致密乳腺组织中敏感性较低，组织重叠和细微发现增加了感知难度，需要开发能够增强视觉感知的新方法。

Method: 开发了轻量级TDCE模块与BI-RADS分类器耦合的端到端框架，在VinDr-Mammo数据集上训练，并在内部测试集、两个公共数据集和三个外部临床队列上进行评估，同时进行了多读者多病例的观察研究。

Result: 在VinDr-Mammo数据集上AUC从0.7669提升至0.8461（P=0.004），在致密乳腺中提升更大（AUC 0.749至0.835）。观察研究中特异性从0.90提升至0.96（P=0.052），敏感性保持相当。

Conclusion: TDCE提供了一种任务优化的色度表示方法，可以改善感知显著性并减少乳腺X线摄影分诊中的假阳性召回。

Abstract: Purpose:Mammography screening is less sensitive in dense breasts, where tissue overlap and subtle findings increase perceptual difficulty. We present MammoColor, an end-to-end framework with a Task-Driven Chromatic Encoding (TDCE) module that converts single-channel mammograms into TDCE-encoded views for visual augmentation. Materials and Methods:MammoColor couples a lightweight TDCE module with a BI-RADS triage classifier and was trained end-to-end on VinDr-Mammo. Performance was evaluated on an internal test set, two public datasets (CBIS-DDSM and INBreast), and three external clinical cohorts. We also conducted a multi-reader, multi-case (MRMC) observer study with a washout period, comparing (1) grayscale-only, (2) TDCE-only, and (3) side-by-side grayscale+TDCE. Results:On VinDr-Mammo, MammoColor improved AUC from 0.7669 to 0.8461 (P=0.004). Gains were larger in dense breasts (AUC 0.749 to 0.835). In the MRMC study, TDCE-encoded images improved specificity (0.90 to 0.96; P=0.052) with comparable sensitivity. Conclusion:TDCE provides a task-optimized chromatic representation that may improve perceptual salience and reduce false-positive recalls in mammography triage.

</details>


### [74] [ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention](https://arxiv.org/abs/2602.07574)
*Wenjie Liu,Hao Wu,Xin Qiu,Yingqi Fan,Yihan Zhang,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CV

TL;DR: ViCA是一种创新的多模态大语言模型架构，通过稀疏交叉注意力机制大幅减少视觉处理计算开销，在保持98%基线精度的同时将视觉侧计算降至4%，实现3.5-10倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在每个Transformer层统一处理视觉和文本token，造成巨大计算开销。研究发现视觉嵌入已与语言空间良好对齐，且有效的视觉语言交互仅发生在少数层中。

Method: 提出ViCA架构，让视觉token绕过所有自注意力和前馈层，仅在选定层通过稀疏交叉注意力与文本交互，实现最小化视觉处理。

Result: 在3个MLLM主干、9个多模态基准和26个剪枝基线测试中，ViCA保持98%基线精度，视觉计算降至4%，单批次推理加速3.5倍，多批次加速10倍，视觉定位开销接近零。

Conclusion: ViCA提供了规则化、硬件友好的推理流水线，可与token剪枝方法正交结合获得进一步效率提升，为MLLM的高效部署提供了有效解决方案。

Abstract: Modern multimodal large language models (MLLMs) adopt a unified self-attention design that processes visual and textual tokens at every Transformer layer, incurring substantial computational overhead. In this work, we revisit the necessity of such dense visual processing and show that projected visual embeddings are already well-aligned with the language space, while effective vision-language interaction occurs in only a small subset of layers. Based on these insights, we propose ViCA (Vision-only Cross-Attention), a minimal MLLM architecture in which visual tokens bypass all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. Extensive evaluations across three MLLM backbones, nine multimodal benchmarks, and 26 pruning-based baselines show that ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%, consistently achieving superior performance-efficiency trade-offs. Moreover, ViCA provides a regular, hardware-friendly inference pipeline that yields over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared with text-only LLMs. It is also orthogonal to token pruning methods and can be seamlessly combined for further efficiency gains. Our code is available at https://github.com/EIT-NLP/ViCA.

</details>


### [75] [Automated rock joint trace mapping using a supervised learning model trained on synthetic data generated by parametric modelling](https://arxiv.org/abs/2602.07590)
*Jessica Ka Yi Chiu,Tom Frode Hansen,Eivind Magnus Paulsen,Ole Jakob Mengshoel*

Main category: cs.CV

TL;DR: 地质驱动的机器学习方法，通过合成数据生成和迁移学习实现岩石节理迹线自动识别，解决了真实数据稀缺和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决岩石节理迹线识别中真实数据稀缺、类别不平衡以及地质特征保持的挑战，提高自动化识别的准确性和地质意义。

Method: 结合地质建模、合成数据生成和监督图像分割。首先使用离散裂缝网络模型生成具有地质相关性的合成图像，然后采用混合训练和预训练+微调策略在真实图像上进行模型训练。

Result: 合成数据能有效支持监督学习；混合训练在标签一致时表现良好，微调在标签噪声较大时更稳健；零预测能力有限但通过少量真实数据微调可获得良好泛化能力。

Conclusion: 该方法为可靠的节理迹线识别提供了有效途径，支持领域自适应和评估的进一步研究，定性分析显示比定量指标更能体现地质意义。

Abstract: This paper presents a geology-driven machine learning method for automated rock joint trace mapping from images. The approach combines geological modelling, synthetic data generation, and supervised image segmentation to address limited real data and class imbalance. First, discrete fracture network models are used to generate synthetic jointed rock images at field-relevant scales via parametric modelling, preserving joint persistence, connectivity, and node-type distributions. Second, segmentation models are trained using mixed training and pretraining followed by fine-tuning on real images. The method is tested in box and slope domains using several real datasets. The results show that synthetic data can support supervised joint trace detection when real data are scarce. Mixed training performs well when real labels are consistent (e.g. box-domain), while fine-tuning is more robust when labels are noisy (e.g. slope-domain where labels can be biased, incomplete, and inconsistent). Fully zero-shot prediction from synthetic model remains limited, but useful generalisation is achieved by fine-tuning with a small number of real data. Qualitative analysis shows clearer and more geologically meaningful joint traces than indicated by quantitative metrics alone. The proposed method supports reliable joint mapping and provides a basis for further work on domain adaptation and evaluation.

</details>


### [76] [TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation](https://arxiv.org/abs/2602.07595)
*Yuanzhi Liang,Xuan'er Wu,Yirui Liu,Yijie Fang,Yizhen Fan,Ke Hao,Rui Li,Ruiying Liu,Ziqi Ni,Peng Yu,Yanbo Wang,Haibin Huang,Qizhen Weng,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出一个系统化的视频生成器后训练框架，将监督策略塑造、奖励驱动的强化学习和基于偏好的优化整合为稳定性约束的优化堆栈，以提升感知保真度、时间一致性和提示遵循能力。


<details>
  <summary>Details</summary>
Motivation: 解决生产环境中视频生成器的后训练需求，包括指令跟随、可控性和长时间稳定性，同时应对高计算成本、时间累积故障模式以及异质不确定反馈等实际约束。

Method: 采用分阶段的诊断驱动优化方法，整合监督策略塑造、奖励驱动的强化学习和偏好优化，构建稳定性约束的优化堆栈来处理视频生成的特殊挑战。

Result: 开发出可扩展的后训练流程，能够在保持初始化可控性的同时，显著改善生成视频的感知质量、时间连贯性和提示遵循能力。

Conclusion: 该框架为构建稳定、可扩展且在实际部署中有效的视频生成后训练流程提供了清晰的蓝图，将优化过程从孤立技巧集合转变为系统化的诊断驱动方法。

Abstract: Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.

</details>


### [77] [Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.07605)
*Hulingxiao He,Zijun Geng,Yuxin Peng*

Main category: cs.CV

TL;DR: Fine-R1是一个专门为细粒度视觉识别设计的MLLM，通过CoT监督微调和三元组增强策略优化，仅需4-shot训练就能在seen和unseen子类别识别上超越现有MLLM和CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在粗粒度视觉任务表现良好，但在细粒度视觉识别中存在性能差距，需要大量标注数据且容易过拟合seen子类别，泛化到unseen子类别能力差。

Method: 1) Chain-of-Thought监督微调：构建高质量FGVR CoT数据集，包含视觉分析、候选子类别、比较和预测的推理链；2) 三元组增强策略优化：类内增强混合同类图像轨迹提升鲁棒性，类间增强最大化跨子类别响应差异增强判别能力。

Result: 仅用4-shot训练，Fine-R1在seen和unseen子类别识别上均优于现有通用MLLM、推理MLLM和对比CLIP模型。

Conclusion: Fine-R1在专家标注难以获取的知识密集型领域展现出潜力，为解决细粒度视觉识别中的数据和泛化问题提供了有效方案。

Abstract: Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of "visual analysis, candidate sub-categories, comparison, and prediction", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.

</details>


### [78] [HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology](https://arxiv.org/abs/2602.07608)
*Yixin Chen,Ziyu Su,Lingbin Meng,Elshad Hasanov,Wei Chen,Anil Parwani,M. Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 提出HistoMet框架，通过决策感知、概念对齐的MIL方法从原发肿瘤病理图像预测转移风险和转移部位，采用两阶段预测流程并整合语言定义的概念以提高临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有计算方法将转移状态或部位预测作为孤立任务，未能显式建模临床顺序决策过程（先评估转移风险再进行部位特异性评估）的研究空白。

Method: 开发两模块预测流程：先估计原发肿瘤转移进展可能性，再对高风险病例进行条件性转移部位预测；整合语言定义和数据自适应的转移概念，使用预训练病理视觉语言模型指导表示学习。

Result: 在多机构泛癌队列6504名患者上验证，在95%灵敏度筛查设置下显著减少下游工作量；对转移病例实现宏观F1分数74.6±1.3和宏观一对多AUC 92.1。

Conclusion: 显式建模临床决策结构能够直接从原发肿瘤病理学实现稳健且可部署的转移进展和部位趋向性预后预测。

Abstract: Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.

</details>


### [79] [AD-MIR: Bridging the Gap from Perception to Persuasion in Advertising Video Understanding via Structured Reasoning](https://arxiv.org/abs/2602.07625)
*Binxiao Xu,Junyu Feng,Xiaopeng Lin,Haodong Li,Zhiyuan Feng,Bohan Zeng,Shaolin Lu,Ming Lu,Qi She,Wentao Zhang*

Main category: cs.CV

TL;DR: AD-MIR是一个两阶段框架，通过结构化记忆构建和结构化推理代理，将广告视频的像素级感知与高层次营销逻辑连接起来，在AdsQA基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体在广告视频多模态理解方面存在认知鸿沟，难以将像素级感知与高层次营销策略联系起来，需要专门解决广告意图解码的挑战。

Method: 采用两阶段架构：1）结构感知记忆构建阶段，通过语义检索和精确关键词匹配将原始视频转换为结构化数据库；2）结构化推理代理，模拟营销专家通过迭代查询循环分解叙事并推断隐含说服策略，采用基于证据的自校正机制。

Result: 在AdsQA基准测试中，AD-MIR在严格准确率上超越最强通用智能体DVD 1.8%，在宽松准确率上超越9.5%，达到最先进性能。

Conclusion: 有效的广告理解需要将抽象营销策略明确地基于像素级证据进行接地，AD-MIR框架成功解决了这一需求并展示了优越性能。

Abstract: Multimodal understanding of advertising videos is essential for interpreting the intricate relationship between visual storytelling and abstract persuasion strategies. However, despite excelling at general search, existing agents often struggle to bridge the cognitive gap between pixel-level perception and high-level marketing logic. To address this challenge, we introduce AD-MIR, a framework designed to decode advertising intent via a two-stage architecture. First, in the Structure-Aware Memory Construction phase, the system converts raw video into a structured database by integrating semantic retrieval with exact keyword matching. This approach prioritizes fine-grained brand details (e.g., logos, on-screen text) while dynamically filtering out irrelevant background noise to isolate key protagonists. Second, the Structured Reasoning Agent mimics a marketing expert through an iterative inquiry loop, decomposing the narrative to deduce implicit persuasion tactics. Crucially, it employs an evidence-based self-correction mechanism that rigorously validates these insights against specific video frames, automatically backtracking when visual support is lacking. Evaluation on the AdsQA benchmark demonstrates that AD-MIR achieves state-of-the-art performance, surpassing the strongest general-purpose agent, DVD, by 1.8% in strict and 9.5% in relaxed accuracy. These results underscore that effective advertising understanding demands explicitly grounding abstract marketing strategies in pixel-level evidence. The code is available at https://github.com/Little-Fridge/AD-MIR.

</details>


### [80] [Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation](https://arxiv.org/abs/2602.07643)
*Yichi Zhang,Feiyang Xiao,Le Xue,Wenbo Zhang,Gang Feng,Chenguang Zheng,Yuan Qi,Yuan Cheng,Zixin Hu*

Main category: cs.CV

TL;DR: 该研究通过创建包含490个全身PET/CT和464个全身PET/MRI扫描的UMD数据集，对3D分割基础模型进行综合评估，发现从结构影像到功能影像领域转换时存在显著性能差距，揭示当前模型远未达到真正通用状态。


<details>
  <summary>Details</summary>
Motivation: 当前3D医学基础模型的验证主要局限于区域和结构影像，缺乏对模态差异的探索，需要提供严谨客观的评估来验证其在真实临床应用中的鲁棒性。

Method: 构建UMD数据集（约67.5万张2D图像，1.2万个3D器官标注），通过受试者内配对扫描的对照比较，将成像模态作为主要自变量，对代表性3D分割基础模型进行全面评估。

Result: 评估揭示了文献报道基准与真实世界效能之间的显著差异，特别是在从结构域转向功能域时出现系统性失败，表明当前模型在模态适应性方面存在严重不足。

Conclusion: 当前3D基础模型远未达到真正通用状态，需要向多模态训练和评估范式转变，以弥合理想化基准测试与全面临床实用性之间的差距。该数据集和分析为开发真正模态无关的医学基础模型奠定了基础。

Abstract: While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\sim$675k 2D images, $\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.

</details>


### [81] [From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding](https://arxiv.org/abs/2602.07645)
*Leonardo Gonzalez*

Main category: cs.CV

TL;DR: Images2Slides是一个API驱动的系统，可将静态信息图转换为可编辑的Google Slides幻灯片，通过视觉语言模型提取区域级规范并重建元素，实现高精度的文本和图像恢复。


<details>
  <summary>Details</summary>
Motivation: 信息图通常以静态图像形式导出，内容被锁定在像素中，导致更新、本地化和重用成本高昂，需要一种方法将其转换为可编辑格式。

Method: 使用视觉语言模型提取区域级规范，将像素几何映射到幻灯片坐标，并通过Google Slides批量更新API重建元素。系统支持多VLM后端，采用通用JSON区域模式和确定性后处理。

Result: 在29个程序生成的信息图基准测试中，整体元素恢复率达到0.989±0.057，文本转录错误CER=0.033±0.149，文本区域布局保真度IoU=0.364±0.161，图像区域IoU=0.644±0.131。

Conclusion: 系统成功实现了从静态信息图到可编辑幻灯片的转换，解决了文本大小校准和非均匀背景等工程挑战，为未来工作提供了指导方向。

Abstract: Infographics are widely used to communicate information with a combination of text, icons, and data visualizations, but once exported as images their content is locked into pixels, making updates, localization, and reuse expensive. We describe \textsc{Images2Slides}, an API-based pipeline that converts a static infographic (PNG/JPG) into a native, editable Google Slides slide by extracting a region-level specification with a vision-language model (VLM), mapping pixel geometry into slide coordinates, and recreating elements using the Google Slides batch update API. The system is model-agnostic and supports multiple VLM backends via a common JSON region schema and deterministic postprocessing. On a controlled benchmark of 29 programmatically generated infographic slides with known ground-truth regions, \textsc{Images2Slides} achieves an overall element recovery rate of $0.989\pm0.057$ (text: $0.985\pm0.083$, images: $1.000\pm0.000$), with mean text transcription error $\mathrm{CER}=0.033\pm0.149$ and mean layout fidelity $\mathrm{IoU}=0.364\pm0.161$ for text regions and $0.644\pm0.131$ for image regions. We also highlight practical engineering challenges in reconstruction, including text size calibration and non-uniform backgrounds, and describe failure modes that guide future work.

</details>


### [82] [Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation](https://arxiv.org/abs/2602.07658)
*Avinash Kumar K M,Samarth S. Raut*

Main category: cs.CV

TL;DR: 该研究评估了医学扫描3D重建流程中的误差，比较了不同分割算法和几何形状的体素与表面精度指标，发现Otsu方法最适用于各种几何形状，Jaccard指数比Dice更适合薄壁结构评估。


<details>
  <summary>Details</summary>
Motivation: 医学扫描创建3D模型的精度受多种因素影响，但几何类型、类别不平衡、体素和点云对齐对精度的影响尚未得到充分探索。

Method: 使用SLA技术打印球体、面罩和AAA模型，通过micro-CT扫描，采用GMM、Otsu和RG分割方法，使用KU算法对齐分割和参考模型，通过ICP配准表面网格，评估Dice、Jaccard、精确度、倒角距离和平均Hausdorff距离等指标。

Result: Otsu方法对所有几何形状最适用；AAA由于壁薄和对齐问题重叠分数低；类别不平衡对AAA的特异性影响最大；表面精度指标与体素指标趋势不同；RG方法在球体上表现最佳；面罩表面误差最大可能由于ICP配准问题。

Conclusion: 分割精度是重建过程各阶段误差的累积结果，高类别不平衡和对齐敏感性情况下体素精度指标可能误导；Jaccard指数比Dice更严格，更适合薄壁结构评估；必须确保体素和点云对齐才能可靠评估重建流程。

Abstract: The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.

</details>


### [83] [Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making](https://arxiv.org/abs/2602.07668)
*Ross Greer,Laura Fleig,Maitrayee Keskar,Erika Maquiling,Giovanni Tapia Lopez,Angel Martinez-Sanchez,Parthib Roy,Jake Rattigan,Mira Sur,Alejandra Vidrio,Thomas Marcotte,Mohan Trivedi*

Main category: cs.CV

TL;DR: L-LIO框架通过融合音频和视觉传感增强车辆安全，在驾驶员状态评估和环境理解方面提供多模态解决方案，特别适用于视觉信号不足的复杂场景。


<details>
  <summary>Details</summary>
Motivation: 现有LILO框架主要依赖视觉信息，但音频模态能提供额外的驾驶员状态和环境信息，特别是在视觉信号受限或需要语境理解的场景中。

Method: 扩展LILO框架为L-LIO，通过音频信号增强多模态传感器融合。评估三个应用案例：驾驶员语音分类（如醉酒状态）、乘客自然语言指令分析、以及音频辅助视觉系统解析外部行为。

Result: 试点研究表明音频能提供安全相关洞察，特别是在需要声音信息的复杂场景中。但存在环境噪声干扰、隐私问题和跨主体鲁棒性等挑战。

Conclusion: L-LIO通过音频-视觉多模态融合增强了驾驶员和场景理解，为安全干预提供了新途径，但需要在动态现实环境中进一步提高可靠性。

Abstract: The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention monitoring. In this research, we propose an augmentation to this framework, making a case for the audio modality as an additional source of information to understand the driver, and in the evolving autonomy landscape, also the passengers and those outside the vehicle. We expand LILO by incorporating audio signals, forming the looking-and-listening inside-and-outside (L-LIO) framework to enhance driver state assessment and environment understanding through multimodal sensor fusion. We evaluate three example cases where audio enhances vehicle safety: supervised learning on driver speech audio to classify potential impairment states (e.g., intoxication), collection and analysis of passenger natural language instructions (e.g., "turn after that red building") to motivate how spoken language can interface with planning systems through audio-aligned instruction data, and limitations of vision-only systems where audio may disambiguate the guidance and gestures of external agents. Datasets include custom-collected in-vehicle and external audio samples in real-world environments. Pilot findings show that audio yields safety-relevant insights, particularly in nuanced or context-rich scenarios where sound is critical to safe decision-making or visual signals alone are insufficient. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts. L-LIO augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention.

</details>


### [84] [Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning](https://arxiv.org/abs/2602.07680)
*Ross Greer,Maitrayee Keskar,Angel Martinez-Sanchez,Parthib Roy,Shashank Shriram,Mohan Trivedi*

Main category: cs.CV

TL;DR: 论文研究了视觉语言模型在自动驾驶安全评估和决策中的应用，提出了三种系统级用例：基于CLIP的轻量级危险筛查、场景级嵌入在轨迹规划中的集成、以及自然语言作为行为约束在运动规划中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言表示如何支持安全关键的自动驾驶场景，通过将语义概念与视觉观察对齐来改进感知、预测和规划管道的安全性能。

Method: 1) 使用CLIP图像-文本相似性进行类别无关的危险筛查；2) 在基于transformer的轨迹规划框架中集成场景级视觉语言嵌入；3) 利用自然语言作为运动规划的行为约束。

Result: 危险筛查方法能稳健检测分布外道路危险；全局嵌入直接用于规划器未能提高轨迹精度；自然语言约束能抑制严重规划失败并改善安全对齐行为。

Conclusion: 视觉语言表示在表达语义风险、意图和行为约束方面对自动驾驶安全具有重要潜力，但需要精心设计的系统架构和结构化基础，而非简单的特征注入。

Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.

</details>


### [85] [Process-of-Thought Reasoning for Videos](https://arxiv.org/abs/2602.07689)
*Jusheng Zhang,Kaitong Cai,Jian Wang,Yongsen Zheng,Kwok-Yan Lam,Keze Wang*

Main category: cs.CV

TL;DR: PoT是一个用于视频推理的思维过程框架，通过将推理分解为可验证的步骤序列来提高视频理解的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 视频理解需要对长而嘈杂的观测进行时序定位和多步推理，现有方法缺乏明确的推理过程和证据追溯能力。

Method: 提出PoT框架，包含三个核心组件：(1)时序证据选择；(2)逐步状态更新；(3)约束答案合成，支持模型无关的架构设计。

Result: 在标准视频推理任务上，PoT显著提高了事实准确性和时序定位能力，减少了幻觉解释，提供了可解释的推理轨迹。

Conclusion: PoT框架通过显式结构化推理过程，有效提升了视频推理的性能和可解释性，为复杂视频理解任务提供了新的解决方案。

Abstract: Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.

</details>


### [86] [Semantic-Deviation-Anchored Multi-Branch Fusion for Unsupervised Anomaly Detection and Localization in Unstructured Conveyor-Belt Coal Scenes](https://arxiv.org/abs/2602.07694)
*Wenping Jin,Yuyang Tang,Li Zhu*

Main category: cs.CV

TL;DR: 提出了一种用于煤矿传送带场景的无监督异物异常检测与像素级定位方法，通过多视角互补线索协作感知框架，在构建的CoalAD基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 煤矿传送带场景中异物检测面临高度非结构化环境挑战：煤矸石随机堆放、背景复杂多变、异物对比度低且易变形遮挡，导致传统异常检测方法性能下降。

Method: 提出互补线索协作感知框架，从三个角度提取和融合异常证据：对象级语义组合建模、基于语义属性的全局偏差分析、细粒度纹理匹配。

Result: 在CoalAD基准测试中，该方法在图像级和像素级指标上均优于广泛使用的基线方法，消融实验验证了各组成部分的有效贡献。

Conclusion: 该方法为煤矿场景中的异物检测提供了有效的无监督解决方案，通过多视角特征融合实现了鲁棒的异常检测和精确定位，代码已开源。

Abstract: Reliable foreign-object anomaly detection and pixel-level localization in conveyor-belt coal scenes are essential for safe and intelligent mining operations. This task is particularly challenging due to the highly unstructured environment: coal and gangue are randomly piled, backgrounds are complex and variable, and foreign objects often exhibit low contrast, deformation, occlusion, resulting in coupling with their surroundings. These characteristics weaken the stability and regularity assumptions that many anomaly detection methods rely on in structured industrial settings, leading to notable performance degradation. To support evaluation and comparison in this setting, we construct \textbf{CoalAD}, a benchmark for unsupervised foreign-object anomaly detection with pixel-level localization in coal-stream scenes. We further propose a complementary-cue collaborative perception framework that extracts and fuses complementary anomaly evidence from three perspectives: object-level semantic composition modeling, semantic-attribution-based global deviation analysis, and fine-grained texture matching. The fused outputs provide robust image-level anomaly scoring and accurate pixel-level localization. Experiments on CoalAD demonstrate that our method outperforms widely used baselines across the evaluated image-level and pixel-level metrics, and ablation studies validate the contribution of each component. The code is available at https://github.com/xjpp2016/USAD.

</details>


### [87] [A hybrid Kolmogorov-Arnold network for medical image segmentation](https://arxiv.org/abs/2602.07702)
*Deep Bhattacharyya,Ali Ayub,A. Ben Hamza*

Main category: cs.CV

TL;DR: U-KABS：一种结合Kolmogorov-Arnold网络和U型编码器-解码器架构的混合框架，通过Bernstein多项式和B样条可学习激活函数，提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在诊断和治疗规划中至关重要，但由于医学图像的固有复杂性和变异性，特别是捕捉数据中的非线性关系方面存在挑战。

Method: 提出U-KABS混合框架，整合卷积和squeeze-and-excitation阶段增强通道特征表示，以及KAN Bernstein Spline阶段使用基于Bernstein多项式和B样条的可学习激活函数。采用编码器-解码器架构和跳跃连接实现多尺度特征融合。

Result: 在多个医学影像基准数据集上评估，U-KABS相比强基线表现出优越性能，特别是在分割复杂解剖结构方面。

Conclusion: 该混合设计利用Bernstein多项式的全局平滑性和B样条的局部适应性，能够有效捕捉医学图像中复杂结构分割所需的大范围上下文趋势和细粒度模式。

Abstract: Medical image segmentation plays a vital role in diagnosis and treatment planning, but remains challenging due to the inherent complexity and variability of medical images, especially in capturing non-linear relationships within the data. We propose U-KABS, a novel hybrid framework that integrates the expressive power of Kolmogorov-Arnold Networks (KANs) with a U-shaped encoder-decoder architecture to enhance segmentation performance. The U-KABS model combines the convolutional and squeeze-and-excitation stage, which enhances channel-wise feature representations, and the KAN Bernstein Spline (KABS) stage, which employs learnable activation functions based on Bernstein polynomials and B-splines. This hybrid design leverages the global smoothness of Bernstein polynomials and the local adaptability of B-splines, enabling the model to effectively capture both broad contextual trends and fine-grained patterns critical for delineating complex structures in medical images. Skip connections between encoder and decoder layers support effective multi-scale feature fusion and preserve spatial details. Evaluated across diverse medical imaging benchmark datasets, U-KABS demonstrates superior performance compared to strong baselines, particularly in segmenting complex anatomical structures.

</details>


### [88] [All-Optical Segmentation via Diffractive Neural Networks for Autonomous Driving](https://arxiv.org/abs/2602.07717)
*Yingjie Li,Daniel Robinson,Cunxi Yu*

Main category: cs.CV

TL;DR: 提出基于衍射光学神经网络的全光计算框架，用于自动驾驶中的RGB图像分割和车道线检测，相比传统深度神经网络具有更高的能效和更低的延迟。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络在自动驾驶图像处理中能耗高、延迟大，衍射光学神经网络在能效方面具有优势，可通过全光处理减少模数转换开销。

Method: 采用衍射光学神经网络进行全光图像处理，利用光衍射实现高速计算，在CityScapes数据集上进行图像分割实验，并在定制室内轨道数据集和CARLA模拟环境中进行车道检测案例研究。

Result: 实验结果表明DONN系统在CityScapes数据集上的图像分割效果显著，并在多样化环境条件下验证了模型在车道检测任务中的泛化能力。

Conclusion: 提出的全光计算框架为自动驾驶视觉任务提供了高能效、低延迟的解决方案，衍射光学神经网络在实时图像处理应用中具有重要应用前景。

Abstract: Semantic segmentation and lane detection are crucial tasks in autonomous driving systems. Conventional approaches predominantly rely on deep neural networks (DNNs), which incur high energy costs due to extensive analog-to-digital conversions and large-scale image computations required for low-latency, real-time responses. Diffractive optical neural networks (DONNs) have shown promising advantages over conventional DNNs on digital or optoelectronic computing platforms in energy efficiency. By performing all-optical image processing via light diffraction at the speed of light, DONNs save computation energy costs while reducing the overhead associated with analog-to-digital conversions by all-optical encoding and computing. In this work, we propose a novel all-optical computing framework for RGB image segmentation and lane detection in autonomous driving applications. Our experimental results demonstrate the effectiveness of the DONN system for image segmentation on the CityScapes dataset. Additionally, we conduct case studies on lane detection using a customized indoor track dataset and simulated driving scenarios in CARLA, where we further evaluate the model's generalizability under diverse environmental conditions.

</details>


### [89] [PAND: Prompt-Aware Neighborhood Distillation for Lightweight Fine-Grained Visual Classification](https://arxiv.org/abs/2602.07768)
*Qiuming Luo,Yuebing Li,Feng Li,Chang Kong*

Main category: cs.CV

TL;DR: PAND是一种两阶段知识蒸馏框架，通过提示感知语义校准和邻域感知结构蒸馏，将大型视觉语言模型的知识转移到轻量级网络，在细粒度视觉分类任务上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决细粒度视觉分类中传统方法依赖固定提示和全局对齐的问题，实现更有效的知识蒸馏。

Method: 两阶段框架：1) 提示感知语义校准生成自适应语义锚点；2) 邻域感知结构蒸馏约束学生网络的局部决策结构。

Result: 在四个FGVC基准测试中均优于最先进方法，ResNet-18在CUB-200上达到76.09%准确率，比VL2Lite基线提升3.4%。

Conclusion: PAND通过解耦语义校准和结构转移，有效解决了细粒度视觉分类中的知识蒸馏挑战，为轻量化模型部署提供了有效方案。

Abstract: Distilling knowledge from large Vision-Language Models (VLMs) into lightweight networks is crucial yet challenging in Fine-Grained Visual Classification (FGVC), due to the reliance on fixed prompts and global alignment. To address this, we propose PAND (Prompt-Aware Neighborhood Distillation), a two-stage framework that decouples semantic calibration from structural transfer. First, we incorporate Prompt-Aware Semantic Calibration to generate adaptive semantic anchors. Second, we introduce a neighborhood-aware structural distillation strategy to constrain the student's local decision structure. PAND consistently outperforms state-of-the-art methods on four FGVC benchmarks. Notably, our ResNet-18 student achieves 76.09% accuracy on CUB-200, surpassing the strong baseline VL2Lite by 3.4%. Code is available at https://github.com/LLLVTA/PAND.

</details>


### [90] [Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion](https://arxiv.org/abs/2602.07775)
*Haodong Li,Shaoteng Liu,Zhe Lin,Manmohan Chandraker*

Main category: cs.CV

TL;DR: Rolling Sink是一种无需额外训练的方法，通过在推理时维护自回归缓存来解决视频扩散模型在超长时域合成中的训练-测试差距问题，实现了从5秒训练到5-30分钟生成的稳定视频合成。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在有限训练时长下存在训练-测试差距，特别是在测试时生成超出训练时长的视频时会出现视觉退化问题，而长视频训练计算成本高昂，因此需要寻找无需训练的解决方案。

Method: 基于Self Forcing框架（仅在5秒片段上训练），通过系统分析自回归缓存维护机制，提出了Rolling Sink方法，在推理时有效管理缓存状态以保持长期一致性。

Result: Rolling Sink能够将视频合成扩展到超长持续时间（5-30分钟，16FPS），保持一致的物体、稳定的颜色、连贯的结构和平滑的运动，在长时域视觉保真度和时间一致性方面优于现有最先进基线。

Conclusion: 该方法提供了一种计算高效的训练免费解决方案，成功解决了自回归视频模型在超长时域合成中的退化问题，为长视频生成提供了实用且有效的技术路径。

Abstract: Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/

</details>


### [91] [Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing](https://arxiv.org/abs/2602.07784)
*Jayawant Bodagala,Balaji Bodagala*

Main category: cs.CV

TL;DR: UCATSC是一个基于模型的交通信号控制系统，通过随机决策过程建模交通信号控制，考虑视觉感知的不确定性，使用硬约束确保安全和防止饥饿问题，相比强化学习方法提供更可解释的控制策略。


<details>
  <summary>Details</summary>
Motivation: 现实世界中自适应交通信号控制部署受限，主要原因是视觉感知的不确定性、隐含安全性问题以及主要在仿真中验证的非可解释控制策略。

Method: 使用具有约束和部分可观测性的随机决策过程建模交通信号控制，考虑视觉感知的不确定性；在信念空间中进行反事实推演时预测和执行与安全和防止饥饿相关的硬约束。

Result: 系统设计旨在改善交通延误和排放，同时防止安全关键错误，并基于显式模型提供可解释的控制策略输出。

Conclusion: UCATSC提供了一种新的模型化方法，通过硬约束确保安全性，解决了传统强化学习方法在安全性和可解释性方面的局限性，为实际部署提供了更可靠的解决方案。

Abstract: Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.

</details>


### [92] [VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos](https://arxiv.org/abs/2602.07801)
*Wenqi Liu,Yunxiao Wang,Shijie Ma,Meng Liu,Qile Su,Tianke Zhang,Haonan Fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Yinwei Wei,Xuemeng Song*

Main category: cs.CV

TL;DR: VideoTemp-o3是一个统一的视频理解框架，通过联合建模视频定位和问答任务，解决了长视频理解中均匀采样效率低、定位能力弱的问题，支持按需剪辑和定位优化。


<details>
  <summary>Details</summary>
Motivation: 传统均匀帧采样方法在长视频理解中无法捕捉关键视觉证据，导致性能下降和幻觉增加。现有的智能视频理解方法存在效率低下、定位能力弱和工作流程僵化的问题。

Method: 提出VideoTemp-o3统一框架，采用监督微调阶段的统一掩码机制和强化学习的专用奖励机制。开发高质量长视频定位问答数据构建流程，并建立相应基准进行评估。

Result: 实验结果表明，该方法在长视频理解和定位任务上都取得了显著性能提升。

Conclusion: VideoTemp-o3通过联合建模和专用机制设计，有效解决了长视频理解中的关键挑战，为智能视频分析提供了更高效的解决方案。

Abstract: In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which the model actively identifies relevant video segments, performs dense sampling within those clips, and then produces answers. However, existing methods remain inefficient, suffer from weak localization, and adhere to rigid workflows. To solve these issues, we propose VideoTemp-o3, a unified agentic thinking-with-videos framework that jointly models video grounding and question answering. VideoTemp-o3 exhibits strong localization capability, supports on-demand clipping, and can refine inaccurate localizations. Specifically, in the supervised fine-tuning stage, we design a unified masking mechanism that encourages exploration while preventing noise. For reinforcement learning, we introduce dedicated rewards to mitigate reward hacking. Besides, from the data perspective, we develop an effective pipeline to construct high-quality long video grounded QA data, along with a corresponding benchmark for systematic evaluation across various video durations. Experimental results demonstrate that our method achieves remarkable performance on both long video understanding and grounding.

</details>


### [93] [How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study](https://arxiv.org/abs/2602.07814)
*Simiao Ren,Yuchen Zhou,Xingyu Shen,Kidus Zewde,Tommy Duong,George Huang,Hatsanai,Tiangratanakul,Tsang,Ng,En Wei,Jiayu Xue*

Main category: cs.CV

TL;DR: 首次对16种最先进的AI生成图像检测方法进行全面的零样本评估，涵盖23个预训练检测器变体和12个数据集，揭示现有检测器在零样本场景下的性能局限性和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像在数字平台上的激增，可靠的检测方法对于打击错误信息和维护内容真实性至关重要。现有基准主要评估微调模型，而忽略了即开即用性能这一最常见部署场景。

Method: 对16种最先进的检测方法（23个预训练检测器变体）在12个多样化数据集上进行零样本评估，涵盖291个独特生成器和260万张图像样本，包括现代扩散模型。

Result: 研究发现：(1)不存在通用最佳检测器，排名极不稳定；(2)最佳与最差检测器性能差距达37个百分点；(3)训练数据对齐对泛化影响显著；(4)现代商业生成器可击败大多数检测器；(5)识别出三种系统性失败模式。统计分析确认检测器间存在显著性能差异。

Conclusion: 研究结果挑战了'一刀切'的检测器范式，表明从业者必须根据具体威胁环境仔细选择检测器，而不能依赖已发布的基准性能。

Abstract: As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$ρ$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\% mean accuracy) from the worst (37.5\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $χ^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.

</details>


### [94] [Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures](https://arxiv.org/abs/2602.07815)
*Simiao Ren*

Main category: cs.CV

TL;DR: 首个大规模跨范式面部年龄估计基准测试，比较34个模型（22个专用架构和12个通用视觉语言模型）在8个标准数据集上的表现，发现零样本VLMs显著超越大多数专用模型。


<details>
  <summary>Details</summary>
Motivation: 面部年龄估计在内容审核、年龄验证和深度伪造检测中至关重要，但缺乏对现代视觉语言模型与专用年龄估计架构的系统性比较。

Method: 评估34个模型（22个专用架构和12个VLMs）在8个标准数据集上的表现，总计每个模型1100张测试图像，使用MAE作为主要评估指标，并分析年龄验证阈值和年龄分层表现。

Result: 零样本VLMs平均MAE为5.65年，显著优于非LLM模型的9.88年。最佳VLM（Gemini 3 Flash Preview，MAE 4.32）比最佳非LLM模型（MiVOLO，MAE 5.10）提升15%。在18岁阈值验证中，VLMs的未成年人误判率仅为13-25%，而非LLM模型为60-100%。

Conclusion: 研究挑战了任务专用架构对年龄估计的必要性假设，建议领域应转向将VLM能力蒸馏到高效专用模型中。

Abstract: Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\% false adult rates on minors while VLMs achieve 13--25\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.

</details>


### [95] [Back to Physics: Operator-Guided Generative Paths for SMS MRI Reconstruction](https://arxiv.org/abs/2602.07820)
*Zhibo Chen,Yu Guan,Yajuan Huang,Chaoqi Chen,XiangJi,Qiuyun Fan,Dong Liang,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出基于操作符引导的双流交互网络(OCDI-Net)，通过确定性更新解决同时多层(SMS)MRI重建中的耦合逆问题，实现切片分离和平面内补全的两阶段推理。


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散模型的重建方法依赖高斯噪声假设，与SMS采集中的操作符主导退化不匹配，需要解决切片间干扰和k空间数据缺失的强耦合逆问题。

Method: 构建操作符引导的退化轨迹模型，使用OCDI-Net显式分离目标切片内容和切片间干扰，预测结构化退化以进行操作符对齐的反转，采用两阶段链式推理流程。

Result: 在fastMRI脑数据和前瞻性扩散MRI数据上验证，相比传统和基于学习的SMS重建方法，显示出更高的保真度和更低的切片泄漏。

Conclusion: 操作符引导的框架能有效处理SMS MRI中的确定性退化过程，OCDI-Net的双流设计和两阶段推理策略显著改善了重建质量。

Abstract: Simultaneous multi-slice (SMS) imaging with in-plane undersampling enables highly accelerated MRI but yields a strongly coupled inverse problem with deterministic inter-slice interference and missing k-space data. Most diffusion-based reconstructions are formulated around Gaussian-noise corruption and rely on additional consistency steps to incorporate SMS physics, which can be mismatched to the operator-governed degradations in SMS acquisition. We propose an operator-guided framework that models the degradation trajectory using known acquisition operators and inverts this process via deterministic updates. Within this framework, we introduce an operator-conditional dual-stream interaction network (OCDI-Net) that explicitly disentangles target-slice content from inter-slice interference and predicts structured degradations for operator-aligned inversion, and we instantiate reconstruction as a two-stage chained inference procedure that performs SMS slice separation followed by in-plane completion. Experiments on fastMRI brain data and prospectively acquired in vivo diffusion MRI data demonstrate improved fidelity and reduced slice leakage over conventional and learning-based SMS reconstructions.

</details>


### [96] [Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection](https://arxiv.org/abs/2602.07827)
*Guoting Wei,Xia Yuan,Yang Zhou,Haizhao Jing,Yu Liu,Xianbiao Qi,Chunxia Zhao,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: OTA-Det是首个统一开放词汇航空检测(OVAD)和遥感视觉定位(RSVG)的框架，通过任务重构和密集语义对齐策略，实现细粒度语义理解和多目标检测，在保持实时推理速度的同时在六个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有OVAD方法仅限于粗粒度类别语义，而RSVG结构上只能进行单目标定位，无法同时支持丰富语义理解和多目标检测，需要统一框架解决这些局限。

Method: 提出任务重构策略统一任务目标和监督机制，实现跨数据集联合训练；设计密集语义对齐策略建立从整体表达到个体属性的多粒度对应关系；基于RT-DETR架构引入高效模块，从闭集检测扩展到开放文本检测。

Result: 在六个OVAD和RSVG基准测试中达到最先进性能，同时保持34 FPS的实时推理速度。

Conclusion: OTA-Det成功统一了两个关键航空场景理解范式，解决了各自孤立操作的固有局限性，为同时实现细粒度语义理解和多目标检测提供了有效解决方案。

Abstract: Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.

</details>


### [97] [SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models](https://arxiv.org/abs/2602.07833)
*Weijiang Lv,Yaoxuan Feng,Xiaobo Xia,Jiayu Wang,Yan Jing,Wenchao Chen,Bo Chen*

Main category: cs.CV

TL;DR: 提出了SPD-Faith Bench基准来评估多模态大语言模型的推理忠实度，发现感知盲区和感知-推理分离两种系统失败模式，并提出无需训练的SAGE框架来改善视觉证据校准。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注感知幻觉，而对推理层面的不忠实性研究不足，需要从语言先验中分离出忠实度问题。

Method: 基于细粒度图像差异推理构建诊断基准SPD-Faith Bench，强制进行显式视觉比较，分析视觉注意力和表示偏移，并提出SAGE框架进行视觉证据校准。

Result: 评估发现MLLMs存在感知盲区和感知-推理分离两种系统失败模式，SAGE框架能有效改善视觉路由和推理-感知对齐。

Conclusion: 需要超越回答正确性来显式评估忠实度，提出的基准和方法为理解和完善多模态推理的忠实性提供了重要工具。

Abstract: Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.

</details>


### [98] [FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging](https://arxiv.org/abs/2602.08024)
*Ziyang Fan,Keyu Chen,Ruilong Xing,Yulin Li,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: FlashVID是一个无需训练的视频大语言模型推理加速框架，通过注意力与多样性令牌选择和树状时空令牌合并技术，在保留仅10%视觉令牌的情况下实现99.1%的性能保持，计算效率提升10倍。


<details>
  <summary>Details</summary>
Motivation: 现有VLLM加速框架独立压缩空间和时间冗余，忽略了时空关系，导致次优压缩效果。视频动态特性导致视觉特征在空间位置、尺度、方向等属性上随时间变化，需要更好的时空压缩方法。

Method: 使用注意力与多样性令牌选择(ADTS)选择最具代表性的基础视频表示令牌，然后应用树状时空令牌合并(TSTM)进行细粒度时空冗余消除。

Result: 在5个视频理解基准测试中，FlashVID仅保留10%视觉令牌即可保持LLaVA-OneVision 99.1%的性能，使Qwen2.5-VL视频帧输入能力提升10倍，相同计算预算下相对性能提升8.6%。

Conclusion: FlashVID作为无需训练即插即用模块，有效解决了VLLM计算效率问题，实现了优异的时空压缩效果和性能保持能力。

Abstract: Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.

</details>


### [99] [VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping](https://arxiv.org/abs/2602.07835)
*Sanoojan Baliah,Yohan Abeysinghe,Rusiru Thushara,Khan Muhammad,Abhinav Dhall,Karthik Nandakumar,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: VFace是一个无需训练、即插即用的视频人脸交换方法，通过频率谱注意力插值、目标结构引导和流引导注意力时间平滑三大技术，显著提升视频中人脸交换的时间一致性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的图像人脸交换方法在视频应用中存在时间不一致性问题，需要一种无需额外训练或微调的视频人脸交换解决方案。

Method: 1. 频率谱注意力插值技术保持关键身份特征；2. 目标结构引导通过注意力注入对齐结构特征；3. 流引导注意力时间平滑机制确保时空一致性。

Result: 实验表明该方法显著提升了时间一致性和视觉保真度，无需修改底层扩散模型或进行视频特定微调。

Conclusion: VFace提供了一个实用且模块化的视频人脸交换解决方案，可与现有的基于扩散模型的图像人脸交换方法无缝集成。

Abstract: We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.

</details>


### [100] [When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning](https://arxiv.org/abs/2602.08236)
*Shoubin Yu,Yue Zhang,Zun Wang,Jaehong Yoon,Huaxiu Yao,Mingyu Ding,Mohit Bansal*

Main category: cs.CV

TL;DR: 本研究深入分析了测试时视觉想象作为可控资源在空间推理中的作用，提出了自适应框架AVIC来选择性调用世界模型进行视觉想象，在多个基准测试中验证了选择性控制比固定想象策略更高效可靠。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型快速发展，但在需要理解不同视角下场景外观的空间推理任务中仍不可靠。现有方法通过世界模型增强视觉想象，但何时需要想象、想象的最佳程度以及何时想象有害等问题尚未得到充分理解。

Method: 提出AVIC自适应测试时框架，通过明确推理当前视觉证据的充分性，选择性调用和缩放视觉想象。在空间推理基准（SAT、MMSI）和具身导航基准（R2R）上进行实验验证。

Result: 研究揭示了想象关键、边际或有害的明确场景，显示选择性控制可以匹配或超越固定想象策略，同时显著减少世界模型调用和语言标记使用。

Conclusion: 研究结果强调了分析和控制测试时想象对于高效可靠空间推理的重要性，为视觉空间推理提供了新的可控资源管理方法。

Abstract: Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.

</details>


### [101] [Geometry-Aware Rotary Position Embedding for Consistent Video World Model](https://arxiv.org/abs/2602.07854)
*Chendong Xiang,Jiajun Liu,Jintao Zhang,Xiao Yang,Zhengwei Fang,Shizun Wang,Zijun Wang,Yingtian Zou,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: ViewRope：一种几何感知的视频变换器编码方法，通过将相机射线方向直接注入自注意力层来解决3D世界模型中的空间持久性问题，显著改善长期一致性并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前预测性世界模型缺乏空间持久性，在长轨迹中无法维持稳定的场景结构，当相机重新访问先前观察位置时经常出现细节幻觉。几何漂移问题源于对屏幕空间位置嵌入的依赖，这与3D一致性所需的投影几何相冲突。

Method: 提出ViewRope几何感知编码，将相机射线方向直接注入视频变换器自注意力层；使用相对射线几何而非像素局部性参数化注意力；提出几何感知帧稀疏注意力，利用几何线索选择性地关注相关历史帧；开发ViewBench诊断套件测量循环闭合保真度和几何漂移。

Result: ViewRope显著改善了长期一致性，同时降低了计算成本。该方法通过几何感知的注意力机制有效解决了3D世界模型中的空间持久性问题。

Conclusion: 通过引入几何感知的编码和注意力机制，ViewRope成功解决了预测性世界模型中的空间持久性问题，为构建更稳定和一致的3D模拟环境提供了有效解决方案。

Abstract: Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.

</details>


### [102] [Learning Self-Correction in Vision-Language Models via Rollout Augmentation](https://arxiv.org/abs/2602.08503)
*Yi Ding,Ziliang Qiu,Bolian Li,Ruqi Zhang*

Main category: cs.CV

TL;DR: Octopus框架通过rollout重组合成密集自校正样本，解决VLMs中自校正学习信号稀疏问题，结合响应掩码策略实现自校正与推理的有效学习，在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以学习视觉语言模型中的自校正行为，因为有效的自校正行为出现频率低，导致学习信号极其稀疏。

Method: 提出correction-specific rollouts (Octopus)框架：1）通过重组现有rollout合成密集自校正样本；2）引入响应掩码策略将自校正与直接推理解耦；3）基于此开发Octopus-8B模型。

Result: 在7个基准测试中达到开源VLMs的SOTA性能，比最佳RLVR基线提高1.0分，同时每步训练时间仅需0.72倍。

Conclusion: Octopus框架通过rollout重组和响应掩码策略有效解决了VLMs自校正学习的稀疏信号问题，显著提升了模型性能和训练效率。

Abstract: Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.

</details>


### [103] [Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images](https://arxiv.org/abs/2602.07860)
*Fei Yu,Shudan Guo,Shiqing Xin,Beibei Wang,Haisen Zhao,Wenzheng Chen*

Main category: cs.CV

TL;DR: 提出一种从超高速运动模糊图像中恢复3D形状的逆渲染方法，通过快速重心坐标求解器显著提升计算效率，实现了对高速平移和旋转物体的有效3D重建。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建方法（如MVS）在极端运动模糊场景下失效，而高速运动物体在体育和工业场景中普遍存在，需要新的解决方案。

Method: 采用逆渲染方法，开发快速重心坐标求解器减少计算开销（速度提升4.57倍），实现完全可微分的高光真实高速运动模拟。

Result: 方法在高速平移和旋转两种典型运动类型上验证有效，能够高效模拟超高速运动物体并成功从2D图像恢复3D形状。

Conclusion: 该方法突破了基于视觉的3D重建边界，为极端运动条件下的形状恢复提供了有效解决方案。

Abstract: We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.
  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.
  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/

</details>


### [104] [Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds](https://arxiv.org/abs/2602.07864)
*Chen Yang,Guanxin Lin,Youquan He,Peiyao Chen,Guanghe Liu,Yufan Mo,Zhouyuan Xu,Linhao Wang,Guohui Zhang,Zihang Zhang,Shenxiang Zeng,Chen Wang,Jiansheng Fan*

Main category: cs.CV

TL;DR: SSI-Bench是一个针对视觉语言模型空间推理能力的基准测试，专注于受约束流形上的空间推理，包含1000个排序问题，评估几何和拓扑推理能力。现有VLMs表现远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型基准测试主要评估无约束场景，模型可能利用2D捷径，缺乏对真实物理世界中几何、拓扑和物理约束的空间推理能力评估。

Method: 通过完全人工驱动的流程构建基准：10名研究人员花费400+小时策划图像、标注结构组件、设计问题以最小化像素级线索，包含几何推理、拓扑推理、心理旋转、截面推断、遮挡推理和力路径推理等空间操作。

Result: 评估31个广泛使用的VLMs显示与人类存在巨大差距：最佳开源模型准确率22.2%，最强闭源模型33.6%，而人类达到91.6%。鼓励模型思考仅带来边际收益，错误分析显示结构基础和约束一致的3D推理存在失败。

Conclusion: SSI-Bench揭示了当前VLMs在空间推理方面的显著局限性，特别是在处理受约束的3D结构时，需要改进结构基础能力和约束一致的3D推理方法。

Abstract: Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.

</details>


### [105] [WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning](https://arxiv.org/abs/2602.07872)
*Mert Sonmezer,Serge Vasylechko,Duygu Atasoy,Seyda Ertekin,Sila Kurugol*

Main category: cs.CV

TL;DR: WristMIR是一个区域感知的儿科腕部X射线检索框架，通过结合全局和局部对比编码器以及两阶段检索过程，显著提高了骨折模式检索性能，无需手动图像标注。


<details>
  <summary>Details</summary>
Motivation: 腕部X射线骨折模式检索具有挑战性，因为临床重要线索细微、高度局部化，且常被重叠解剖结构或不同成像视角遮挡。大型标注数据集的稀缺进一步限制了基于案例的医学图像检索的发展。

Method: 利用MedGemma结构化报告挖掘生成全局和区域级描述，结合预处理腕部图像和特定骨骼裁剪（远端桡骨、远端尺骨、尺骨茎突），联合训练全局和局部对比编码器，采用两阶段检索：粗粒度全局匹配候选检查，然后进行区域条件重排序。

Result: 将图像到文本Recall@5从0.82%提升至9.35%；嵌入表示在骨折分类中表现更强（AUROC 0.949，AUPRC 0.953）；区域感知评估中两阶段设计显著改善基于检索的骨折诊断，平均F1分数从0.568提高到0.753；放射科医生评价其检索病例临床相关性更高，平均评分从3.36升至4.35。

Conclusion: 解剖学引导的检索在增强儿科肌肉骨骼成像中的诊断推理和支持临床决策方面具有巨大潜力，WristMIR框架为医学图像检索提供了有效的区域感知解决方案。

Abstract: Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.

</details>


### [106] [Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video](https://arxiv.org/abs/2602.07891)
*Zihui Gao,Ke Liu,Donny Y. Chen,Duochao Shi,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: SAGE是一个通过互联网视频流对几何基础模型进行可扩展适应的框架，使用层次化挖掘流程将视频转换为训练轨迹，结合稀疏几何锚点和密集可微分一致性监督，显著提升零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 几何基础模型在3D重建方面具有潜力，但受到大规模多样化3D标注数据稀缺的限制。互联网视频提供了近乎无限的原始数据，但由于缺乏真实几何真值和存在观测噪声，难以直接用于几何学习。

Method: 1) 信息丰富的训练轨迹选择；2) 通过SfM点云进行稀疏几何锚点提供全局结构指导；3) 通过3D高斯渲染实现密集可微分一致性提供多视角约束；4) 使用锚点数据正则化策略防止灾难性遗忘。

Result: 在未见过的基准测试集（7Scenes、TUM-RGBD、Matterport3D）上，Chamfer距离相比最先进基线降低了20-42%，显著增强了零样本泛化能力。

Conclusion: SAGE开创了通过互联网视频适应几何基础模型的方法，为通用3D学习建立了可扩展的范式，解决了大规模3D标注数据稀缺的问题。

Abstract: Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.

</details>


### [107] [Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models](https://arxiv.org/abs/2602.07899)
*Zhenhao Shang,Haizhao Jing,Guoting Wei,Haokui Zhang,Rong Xiao,Jianqing Gao,Peng Wang*

Main category: cs.CV

TL;DR: TLQ是一个针对视觉语言模型的token级重要性感知分层量化框架，通过梯度引导的token重要性整合机制和多GPU分层校准方案，在降低硬件依赖的同时显著提升量化性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中视觉和文本token在激活分布和量化误差敏感性方面存在显著差异，传统PTQ校准方法难以有效处理这些差异，导致量化性能下降。

Method: 提出token级重要性整合机制，利用梯度信息构建token级校准集；设计多GPU量化暴露分层校准方案，保持校准过程与真实量化推理路径一致，并将计算负载分配到多个RTX3090 GPU上。

Result: 在两个模型、三种模型规模和两种量化设置下均获得一致的性能提升，表现出强大的量化稳定性。

Conclusion: TLQ框架通过细粒度token级校准和分布式分层校准，有效解决了视觉语言模型PTQ中的校准挑战，为大规模模型部署提供了高效且硬件友好的解决方案。

Abstract: Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.

</details>


### [108] [Which private attributes do VLMs agree on and predict well?](https://arxiv.org/abs/2602.07931)
*Olena Hrynenko,Darya Baranouskaya,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CV

TL;DR: 零样本评估开源视觉语言模型在隐私相关属性识别中的表现，发现VLM比人工标注更倾向于预测隐私属性存在，且在VLM间高一致性情况下可补充人工标注遗漏


<details>
  <summary>Details</summary>
Motivation: 评估视觉语言模型在隐私相关属性识别任务中的零样本性能，探索VLM与人工标注的一致性和差异性

Method: 对开源VLM进行零-shot隐私属性识别评估，分析VLM间的标注一致性以及与人工标注的差异情况

Result: VLM相比人工标注更频繁预测隐私属性存在；当VLM间一致性高时，能够识别人工标注遗漏的属性

Conclusion: VLM具有支持大规模图像数据集隐私标注的潜力，可作为人工标注的补充工具

Abstract: Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.

</details>


### [109] [Integrating Specialized and Generic Agent Motion Prediction with Dynamic Occupancy Grid Maps](https://arxiv.org/abs/2602.07938)
*Rabbia Asghar,Lukas Rummelhard,Wenqian Liu,Anne Spalanzani,Christian Laugier*

Main category: cs.CV

TL;DR: 提出统一框架，通过动态占用网格地图和时序解码管道同时预测未来占用状态网格、车辆网格和场景流网格，解决现有方法在行为复杂性和泛化能力方面的局限


<details>
  <summary>Details</summary>
Motivation: 现有预测方法存在局限性：智能体无关模型难以捕捉动态行为复杂性，智能体特定方法对感知不良或未识别智能体泛化能力差。结合两者可实现更鲁棒安全的运动预测

Method: 基于轻量级时空骨干网络，使用定制化的相互依赖损失函数捕捉网格间依赖关系并实现多样化未来预测。通过占用状态信息强制执行流引导转换，损失函数作为正则化器指导占用演化

Result: 在nuScenes和Woven Planet数据集上评估，相比基线方法在动态车辆和通用动态场景元素预测方面表现出优越性能

Conclusion: 该框架能够同时预测车辆智能体的特定行为和其他动态实体的演化，有效处理复杂场景中的障碍物和遮挡问题，为自动驾驶提供更全面的场景理解

Abstract: Accurate prediction of driving scene is a challenging task due to uncertainty in sensor data, the complex behaviors of agents, and the possibility of multiple feasible futures. Existing prediction methods using occupancy grid maps primarily focus on agent-agnostic scene predictions, while agent-specific predictions provide specialized behavior insights with the help of semantic information. However, both paradigms face distinct limitations: agent-agnostic models struggle to capture the behavioral complexities of dynamic actors, whereas agent-specific approaches fail to generalize to poorly perceived or unrecognized agents; combining both enables robust and safer motion forecasting. To address this, we propose a unified framework by leveraging Dynamic Occupancy Grid Maps within a streamlined temporal decoding pipeline to simultaneously predict future occupancy state grids, vehicle grids, and scene flow grids. Relying on a lightweight spatiotemporal backbone, our approach is centered on a tailored, interdependent loss function that captures inter-grid dependencies and enables diverse future predictions. By using occupancy state information to enforce flow-guided transitions, the loss function acts as a regularizer that directs occupancy evolution while accounting for obstacles and occlusions. Consequently, the model not only predicts the specific behaviors of vehicle agents, but also identifies other dynamic entities and anticipates their evolution within the complex scene. Evaluations on real-world nuScenes and Woven Planet datasets demonstrate superior prediction performances for dynamic vehicles and generic dynamic scene elements compared to baseline methods.

</details>


### [110] [One-Shot Crowd Counting With Density Guidance For Scene Adaptaion](https://arxiv.org/abs/2602.07955)
*Jiwei Chen,Qi Wang,Junyu Gao,Jing Zhang,Dingyi Li,Jing-Jia Luo*

Main category: cs.CV

TL;DR: 提出一种基于少样本学习的跨场景人群计数方法，通过局部和全局密度特征指导模型适应未见监控场景，在三个数据集上超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 不同监控摄像头捕获的人群场景差异巨大，现有模型对未见监控场景的泛化能力有限。

Method: 将不同监控场景视为不同类别，引入少样本学习。提出多局部密度学习器学习支持场景中不同密度分布的原型，编码局部密度相似性矩阵进行局部指导；同时提取全局密度特征进行全局指导。

Result: 在三个监控数据集上的实验表明，该方法能有效适应未见监控场景，在少样本人群计数任务中优于现有最先进方法。

Conclusion: 通过结合局部和全局密度特征的少样本学习方法，显著提升了人群计数模型对未见监控场景的泛化能力。

Abstract: Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.

</details>


### [111] [D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning](https://arxiv.org/abs/2602.07960)
*Changli Tang,Tianyi Wang,Fengyun Rao,Jing Lyu,Chao Zhang*

Main category: cs.CV

TL;DR: D-ORCA是一个对话中心的全模态大语言模型，专门用于鲁棒的视听字幕生成，在说话人识别、语音识别和时间定位方面显著优于现有开源模型，参数量仅80亿但性能可与Qwen3-Omni竞争。


<details>
  <summary>Details</summary>
Motivation: 视频中的口语对话是重要信息源，需要准确识别谁在何时说了什么来进行深度视频理解，但目前开源生态中缺乏高质量的多方对话视频数据集和相应的鲁棒视听字幕模型。

Method: 构建了包含近4万个多语言多方对话视频的大规模数据集DVD，采用群体相对策略优化和三个新颖的奖励函数（说话人归属准确性、全局语音内容准确性、句子级时间边界对齐）来确保细粒度字幕准确性。

Result: D-ORCA在说话人识别、语音识别和时间定位方面大幅超越现有开源模型，在多个通用视听理解基准测试中与Qwen3-Omni性能相当，尽管参数量仅为80亿。

Conclusion: D-ORCA通过创新的奖励函数和高质量数据集，实现了对视频对话内容的准确理解和字幕生成，为深度视频理解提供了有效的解决方案，相关代码、数据和检查点将开源发布。

Abstract: Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \textbf{d}ialogue-centric \textbf{o}mni-modal large language model optimized for \textbf{r}obust audio-visual \textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.

</details>


### [112] [EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation](https://arxiv.org/abs/2602.07967)
*Xiaofeng Tan,Wanjiang Weng,Haodong Lei,Hongsong Wang*

Main category: cs.CV

TL;DR: EasyTune提出了一种针对扩散模型的高效对齐方法，通过分步微调解决传统方法的内存消耗大和优化效率低的问题，并引入自精化偏好学习机制处理运动偏好数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于可微分奖励的扩散模型对齐方法存在两个主要问题：(1) 低效和粗粒度的优化过程；(2) 高内存消耗。研究发现这些限制的关键原因是去噪轨迹中不同步骤之间的递归依赖关系。

Method: 提出EasyTune方法：1) 在去噪过程的每个步骤分别进行微调，而非整个轨迹，从而解耦递归依赖；2) 引入自精化偏好学习(SPL)机制，动态识别偏好对并进行偏好学习，解决运动偏好数据稀缺问题。

Result: 实验表明，EasyTune在MM-Dist对齐指标上比DRaFT-50提升8.2%，同时仅需要其31.16%的额外内存开销，并实现了7.3倍的训练加速。

Conclusion: EasyTune通过解耦去噪步骤间的递归依赖，实现了更密集、细粒度和内存高效的优化，同时通过自精化偏好学习机制有效解决了运动偏好数据稀缺问题，为扩散模型的对齐提供了高效解决方案。

Abstract: In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.

</details>


### [113] [FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction](https://arxiv.org/abs/2602.07979)
*Peng Peng,Xinrui Zhang,Junlin Wang,Lei Li,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: FSP-Diff是一个用于超低剂量能谱CT重建的全光谱先验增强双域潜在扩散框架，通过互补特征构建、全光谱先验集成和高效潜在扩散合成三大策略，显著提升图像质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 超低剂量条件下能谱CT的能量特定投影信噪比急剧下降，导致重建图像出现严重伪影和结构细节丢失，需要解决这一技术挑战。

Method: 1) 互补特征构建：整合直接图像重建和投影域去噪结果；2) 全光谱先验集成：融合多能量投影为高信噪比全光谱图像；3) 高效潜在扩散合成：将多路径特征嵌入紧凑潜在空间进行扩散过程。

Result: 在模拟和真实数据集上的广泛实验表明，FSP-Diff在图像质量和计算效率方面均显著优于现有最先进方法。

Conclusion: 该框架展示了在临床可行超低剂量能谱CT成像方面的巨大潜力，成功实现了细节保真度与噪声抑制的平衡，同时加速了重建过程。

Abstract: Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.

</details>


### [114] [Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2602.07980)
*Junlin Wang,Jiancheng Fang,Peng Peng,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出CSDN方法用于超稀疏视角CBCT重建，通过神经先验编码连续三维衰减表示，结合正弦图扩散和数字放射影像扩散双路径协同优化，有效抑制伪影并恢复细节纹理


<details>
  <summary>Details</summary>
Motivation: CBCT临床应用受限于辐射剂量与图像质量的权衡，超稀疏角度采样导致严重欠采样伪影和层间不一致性，现有方法难以平衡角度连续性和空间细节保真度

Method: CSDN方法包含：1）神经先验作为结构基础编码连续三维衰减表示；2）正弦图精炼扩散(Sino-RD)恢复角度连续性；3）数字放射影像精炼扩散(DR-RD)从投影图像角度增强层间一致性；4）双投影重建融合(DPRF)模块自适应融合双路径输出

Result: 实验表明CSDN在超稀疏视角条件下有效抑制伪影并恢复精细纹理，性能优于现有最先进技术

Conclusion: CSDN通过神经先验和协同扩散策略成功解决了超稀疏视角CBCT重建的挑战，实现了角度连续性和层间一致性的平衡，提高了诊断可靠性

Abstract: The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.

</details>


### [115] [Deepfake Synthesis vs. Detection: An Uneven Contest](https://arxiv.org/abs/2602.07986)
*Md. Tarek Hasan,Sanjay Saha,Shaojing Fan,Swakkhar Shatabda,Terence Sim*

Main category: cs.CV

TL;DR: 实证分析显示当前深度伪造检测模型在应对现代合成技术生成的深度伪造内容时表现显著不佳，包括人类评估者面对高质量伪造内容时也表现较差，突显了检测技术发展滞后于生成技术进步的紧迫问题。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的快速发展（如扩散模型、NeRF和增强的GAN）使得合成媒体更加真实和易得，而检测方法虽有进步但仍需评估其实际有效性，需要实证研究来揭示检测模型与现代合成技术之间的性能差距。

Method: 采用全面的实证分析方法，包括对最先进深度伪造检测技术的系统评估，以及针对尖端合成方法进行人类评估实验，通过大量实验验证检测模型的性能表现。

Result: 研究发现许多最先进的检测模型在面对现代合成技术生成的深度伪造内容时表现显著不佳，人类参与者在面对最佳质量的深度伪造时也表现较差，表明检测能力严重落后于生成技术。

Conclusion: 研究强调了当前检测方法与新一代深度伪造生成技术成熟度之间的关键差距，迫切需要持续改进检测模型以跟上深度伪造生成技术的演进能力，呼吁加强这一关键研究领域的努力。

Abstract: The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.

</details>


### [116] [MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance](https://arxiv.org/abs/2602.07993)
*Xuehai Bai,Xiaoling Gu,Akide Liu,Hangjie Yuan,YiFan Zhang,Jack Ma*

Main category: cs.CV

TL;DR: MCIE-E1提出了一种基于多模态大语言模型的复杂指令图像编辑方法，通过空间感知交叉注意力和背景一致性交叉注意力模块解决指令遵循不足和背景不一致问题，在CIE-Bench基准上相比现有方法取得23.96%的指令遵循度提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法仅限于简单编辑操作，无法满足现实应用中复杂组合指令的需求，存在指令遵循不足和背景不一致两个关键挑战。

Method: 提出MCIE-E1方法，包含两个核心模块：空间感知交叉注意力模块（通过空间引导在去噪过程中显式对齐语义指令与空间区域）和背景一致性交叉注意力模块（保持未编辑区域特征以确保背景一致性）。构建专门的数据管道，结合MLLM的细粒度自动过滤和人工验证来解决复杂指令编辑数据集稀缺问题。

Result: 在CIE-Bench基准测试中，MCIE-E1在定量和定性评估中均优于先前最先进方法，指令遵循度提升23.96%。

Conclusion: MCIE-E1通过创新的架构设计、数据管道和评估协议，有效解决了复杂指令图像编辑中的关键挑战，为现实应用提供了更强大的图像编辑能力。

Abstract: Recent advances in instruction-based image editing have shown remarkable progress. However, existing methods remain limited to relatively simple editing operations, hindering real-world applications that require complex and compositional instructions. In this work, we address these limitations from the perspectives of architectural design, data, and evaluation protocols. Specifically, we identify two key challenges in current models: insufficient instruction compliance and background inconsistency. To this end, we propose MCIE-E1, a Multimodal Large Language Model-Driven Complex Instruction Image Editing method that integrates two key modules: a spatial-aware cross-attention module and a background-consistent cross-attention module. The former enhances instruction-following capability by explicitly aligning semantic instructions with spatial regions through spatial guidance during the denoising process, while the latter preserves features in unedited regions to maintain background consistency. To enable effective training, we construct a dedicated data pipeline to mitigate the scarcity of complex instruction-based image editing datasets, combining fine-grained automatic filtering via a powerful MLLM with rigorous human validation. Finally, to comprehensively evaluate complex instruction-based image editing, we introduce CIE-Bench, a new benchmark with two new evaluation metrics. Experimental results on CIE-Bench demonstrate that MCIE-E1 consistently outperforms previous state-of-the-art methods in both quantitative and qualitative assessments, achieving a 23.96% improvement in instruction compliance.

</details>


### [117] [ForecastOcc: Vision-based Semantic Occupancy Forecasting](https://arxiv.org/abs/2602.08006)
*Riya Mohan,Juana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: ForecastOcc是首个基于视觉的语义占用预测框架，直接从相机图像联合预测未来占用状态和语义类别，无需外部地图，在Occ3D-nuScenes和SemanticKITTI数据集上均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉占用预测方法主要关注运动相关类别而缺乏语义信息，语义占用预测方法依赖单独网络获取历史占用预测，导致误差累积且无法直接从图像学习时空特征。

Method: 提出包含时序交叉注意力预测模块、2D到3D视图变换器、3D编码器和语义占用头的架构，实现多时间跨度的体素级预测，直接从历史相机图像生成语义占用预测。

Result: 在Occ3D-nuScenes多视角预测和SemanticKITTI单目预测任务中均超越基线方法，提供语义丰富、未来感知的预测，捕捉自动驾驶关键场景动态和语义信息。

Conclusion: ForecastOcc框架成功解决了现有方法的局限性，首次实现直接从图像进行语义占用预测，为自动驾驶提供了更全面的未来环境状态理解能力。

Abstract: Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.

</details>


### [118] [PhysDrape: Learning Explicit Forces and Collision Constraints for Physically Realistic Garment Draping](https://arxiv.org/abs/2602.08020)
*Minghai Chen,Mingyuan Liu,Yuxiang Huan*

Main category: cs.CV

TL;DR: PhysDrape是一种混合神经-物理求解器，通过可微分两阶段求解器（力求解器和碰撞投影）结合显式约束，解决了基于深度学习的服装悬垂中碰撞处理与物理真实性的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习服装悬垂方法使用软惩罚处理碰撞，导致几何可行性与物理合理性之间的内在权衡：惩罚碰撞会扭曲网格结构，而保持形状会导致穿透。

Method: 提出物理信息图神经网络，基于物理增强图（编码材料参数和身体接近度）预测残差位移；集成可微分两阶段求解器：可学习力求解器迭代求解StVK模型的不平衡力确保准静态平衡，可微分投影严格强制执行身体表面碰撞约束。

Result: PhysDrape实现了最先进性能，确保可忽略的穿透和显著更低的应变能，在实时应用中具有优越的物理保真度和鲁棒性。

Conclusion: 该方法通过显式约束保证物理有效性，同时支持端到端学习优化网络，为物理一致的预测提供了有效解决方案。

Abstract: Deep learning-based garment draping has emerged as a promising alternative to traditional Physics-Based Simulation (PBS), yet robust collision handling remains a critical bottleneck. Most existing methods enforce physical validity through soft penalties, creating an intrinsic trade-off between geometric feasibility and physical plausibility: penalizing collisions often distorts mesh structure, while preserving shape leads to interpenetration. To resolve this conflict, we present PhysDrape, a hybrid neural-physical solver for physically realistic garment draping driven by explicit forces and constraints. Unlike soft-constrained frameworks, PhysDrape integrates neural inference with explicit geometric solvers in a fully differentiable pipeline. Specifically, we propose a Physics-Informed Graph Neural Network conditioned on a physics-enriched graph -- encoding material parameters and body proximity -- to predict residual displacements. Crucially, we integrate a differentiable two-stage solver: first, a learnable Force Solver iteratively resolves unbalanced forces derived from the Saint Venant-Kirchhoff (StVK) model to ensure quasi-static equilibrium; second, a Differentiable Projection strictly enforces collision constraints against the body surface. This differentiable design guarantees physical validity through explicit constraints, while enabling end-to-end learning to optimize the network for physically consistent predictions. Extensive experiments demonstrate that PhysDrape achieves state-of-the-art performance, ensuring negligible interpenetration with significantly lower strain energy compared to existing baselines, achieving superior physical fidelity and robustness in real-time.

</details>


### [119] [MIND: Benchmarking Memory Consistency and Action Control in World Models](https://arxiv.org/abs/2602.08025)
*Yixuan Ye,Xuanyu Lu,Yuxin Jiang,Yuchao Gu,Rui Zhao,Qiwei Liang,Jiachun Pan,Fengda Zhang,Weijia Wu,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: MIND是一个开域闭环重访基准，用于评估世界模型的记忆一致性和动作控制能力，包含250个高质量视频和多样化场景，并提出了MIND-World基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估世界模型在动态视觉环境中基本能力的统一基准，特别是记忆一致性和动作控制能力的系统性评估。

Method: 构建包含250个1080p视频的数据集，设计高效评估框架测量记忆一致性和动作控制能力，引入不同动作空间（角色移动速度和相机旋转角度）来评估动作泛化能力。

Result: 实验验证了MIND基准的完整性，揭示了当前世界模型在保持长期记忆一致性和跨动作空间泛化方面存在的主要挑战。

Conclusion: MIND为世界模型研究提供了首个开域闭环重访基准，填补了该领域评估标准的空白，为未来研究提供了重要基础。

Abstract: World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/

</details>


### [120] [Enhanced Mixture 3D CGAN for Completion and Generation of 3D Objects](https://arxiv.org/abs/2602.08046)
*Yahia Hamdi,Nicolas Andrialovanirina,Kélig Mahé,Emilie Poisson Caillault*

Main category: cs.CV

TL;DR: 提出MoE-DCGAN架构，将深度3D卷积GAN与专家混合模型结合，用于高质量3D模型生成和缺损物体重建，通过动态容量约束机制实现专家网络的平衡选择。


<details>
  <summary>Details</summary>
Motivation: 传统GAN在3D生成和补全任务中难以处理复杂多样的数据分布，特别是面对不完整输入和大面积缺损区域时存在计算需求高、建模困难等问题。

Method: 集成深度3D卷积GAN与MoE框架，使用多个专门化生成器捕获数据的不同模态，引入无辅助损失的动态容量约束机制指导分类生成器选择。

Result: 在具有不同大小缺损区域的形状生成和补全任务中，模型表现优于现有先进方法，定量和定性结果均证实了有效性。

Conclusion: MoE-DCGAN能够有效处理复杂3D数据，在保持训练稳定性和计算效率的同时，实现了高质量的3D模型生成和缺损重建。

Abstract: The generation and completion of 3D objects represent a transformative challenge in computer vision. Generative Adversarial Networks (GANs) have recently demonstrated strong potential in synthesizing realistic visual data. However, they often struggle to capture complex and diverse data distributions, particularly in scenarios involving incomplete inputs or significant missing regions. These challenges arise mainly from the high computational requirements and the difficulty of modeling heterogeneous and structurally intricate data, which restrict their applicability in real-world settings. Mixture of Experts (MoE) models have emerged as a promising solution to these limitations. By dynamically selecting and activating the most relevant expert sub-networks for a given input, MoEs improve both performance and efficiency. In this paper, we investigate the integration of Deep 3D Convolutional GANs (CGANs) with a MoE framework to generate high-quality 3D models and reconstruct incomplete or damaged objects. The proposed architecture incorporates multiple generators, each specialized to capture distinct modalities within the dataset. Furthermore, an auxiliary loss-free dynamic capacity constraint (DCC) mechanism is introduced to guide the selection of categorical generators, ensuring a balance between specialization, training stability, and computational efficiency, which is critical for 3D voxel processing. We evaluated the model's ability to generate and complete shapes with missing regions of varying sizes and compared its performance with state-of-the-art approaches. Both quantitative and qualitative results confirm the effectiveness of the proposed MoE-DCGAN in handling complex 3D data.

</details>


### [121] [Vanilla Group Equivariant Vision Transformer: Simple and Effective](https://arxiv.org/abs/2602.08047)
*Jiahong Fu,Qi Xie,Deyu Meng,Zongben Xu*

Main category: cs.CV

TL;DR: 提出一个系统性的等变Vision Transformer框架，通过使ViT的关键组件（包括补丁嵌入、自注意力、位置编码和上下采样）等变化，构建具有保证等变性的ViT架构，作为即插即用的替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有等变ViT在性能与等变性之间难以平衡，特别是在协调自注意力机制与补丁嵌入模块方面存在挑战，需要实现ViT中多样化模块的整体等变修改。

Method: 开发一个简单框架，系统性地使ViT的关键组件等变化，包括：补丁嵌入、自注意力机制、位置编码以及上下采样模块，确保整个架构的理论等变性，并可扩展到Swin Transformer等变体。

Result: 大量实验表明，所提出的等变ViT在广泛的视觉任务中持续提升性能和数据效率，证明了该框架的有效性和实用性。

Conclusion: 该研究提供了一个理论严谨且实践通用的即插即用等变ViT框架，成功解决了现有方法在平衡性能与等变性方面的挑战，为构建更高效的视觉Transformer模型提供了系统解决方案。

Abstract: Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.

</details>


### [122] [Weak to Strong: VLM-Based Pseudo-Labeling as a Weakly Supervised Training Strategy in Multimodal Video-based Hidden Emotion Understanding Tasks](https://arxiv.org/abs/2602.08057)
*Yufei Wang,Haixu Liu,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

TL;DR: 提出多模态弱监督框架用于视频中隐藏情绪的自动识别，在iMiGUE网球采访数据集上取得SOTA结果。通过YOLO检测、DINOv2特征提取、Gemini生成伪标签、OpenPose关键点分析，简化GNN为MLP处理时空关系，使用Transformer编码多模态信息，最终准确率从0.6提升至0.69以上。


<details>
  <summary>Details</summary>
Motivation: 解决视频中隐藏情绪自动识别的问题，特别是在严重类别不平衡的情况下提升识别准确率，并探索简化模型架构的可能性。

Method: 1) YOLO 11x进行人像检测和裁剪，DINOv2-Base提取视觉特征；2) Gemini 2.5 Pro通过CoT+Reflection提示生成伪标签和推理文本；3) OpenPose生成137维关键点序列并添加帧间偏移特征；4) 将GNN简化为MLP处理关键点时空关系；5) 超长序列Transformer独立编码图像和关键点序列；6) 多模态预训练后联合微调。

Result: 在iMiGUE数据集上，准确率从之前工作的0.6以下提升至0.69以上，建立了新的公开基准。验证了MLP化的关键点骨干网络可以匹配甚至超越基于GCN的方法。

Conclusion: 提出的多模态弱监督框架有效提升了隐藏情绪识别性能，证明了简化模型架构在特定任务中的有效性，为相关研究提供了新的技术路线和基准结果。

Abstract: To tackle the automatic recognition of "concealed emotions" in videos, this paper proposes a multimodal weak-supervision framework and achieves state-of-the-art results on the iMiGUE tennis-interview dataset. First, YOLO 11x detects and crops human portraits frame-by-frame, and DINOv2-Base extracts visual features from the cropped regions. Next, by integrating Chain-of-Thought and Reflection prompting (CoT + Reflection), Gemini 2.5 Pro automatically generates pseudo-labels and reasoning texts that serve as weak supervision for downstream models. Subsequently, OpenPose produces 137-dimensional key-point sequences, augmented with inter-frame offset features; the usual graph neural network backbone is simplified to an MLP to efficiently model the spatiotemporal relationships of the three key-point streams. An ultra-long-sequence Transformer independently encodes both the image and key-point sequences, and their representations are concatenated with BERT-encoded interview transcripts. Each modality is first pre-trained in isolation, then fine-tuned jointly, with pseudo-labeled samples merged into the training set for further gains. Experiments demonstrate that, despite severe class imbalance, the proposed approach lifts accuracy from under 0.6 in prior work to over 0.69, establishing a new public benchmark. The study also validates that an "MLP-ified" key-point backbone can match - or even surpass - GCN-based counterparts in this task.

</details>


### [123] [Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling](https://arxiv.org/abs/2602.08058)
*Xihang Yu,Rajat Talak,Lorenzo Shaikewitz,Luca Carlone*

Main category: cs.CV

TL;DR: Picasso是一个物理约束的多物体场景重建管道，通过考虑几何、非穿透性和物理原理来构建场景重建，在遮挡和噪声环境下提供物理上合理的重建结果。


<details>
  <summary>Details</summary>
Motivation: 在遮挡和测量噪声存在时，几何上准确但物理上不合理的场景重建会导致物体相互穿透或不稳定平衡，影响数字孪生中动态行为预测的准确性。

Method: 提出Picasso物理约束重建管道，使用快速拒绝采样方法推理多物体交互，利用推断的物体接触图指导采样；同时构建Picasso数据集用于评估。

Result: 在新提出的Picasso数据集和YCB-V数据集上评估显示，Picasso大幅优于现有技术，提供既物理合理又更符合人类直觉的重建结果。

Conclusion: 物体姿态和形状估计需要对场景进行整体推理，考虑物体交互和物理合理性，Picasso通过物理约束方法成功解决了这一问题，为基于仿真的规划和接触丰富行为控制提供了重要基础。

Abstract: In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.

</details>


### [124] [DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models](https://arxiv.org/abs/2602.08059)
*Tong Zhang,Ru Zhang,Jianyi Liu*

Main category: cs.CV

TL;DR: DICE是一种无需训练的实时艺术家风格擦除框架，通过对比子空间分解将风格与内容分离，有效防止扩散模型的风格模仿侵权问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型使风格模仿变得容易，引发版权和知识产权风险，现有对策要么需要昂贵的权重编辑，要么依赖明确指定的编辑风格，缺乏实用性。

Method: 构建对比三元组迫使模型在潜在空间区分风格和非风格特征，将解耦过程形式化为可解的广义特征值问题，识别风格子空间；采用自适应注意力解耦编辑策略动态评估每个token的风格浓度并执行差异抑制和内容增强。

Result: DICE在风格擦除彻底性和内容完整性保护之间实现了优越的平衡，仅需额外3秒时间即可解耦风格。

Conclusion: DICE提供了一种实用高效的技术来遏制风格模仿，无需训练即可实现实时艺术家风格擦除。

Abstract: The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.

</details>


### [125] [ReRoPE: Repurposing RoPE for Relative Camera Control](https://arxiv.org/abs/2602.08068)
*Chunyang Li,Yuanbo Yang,Jiahao Shao,Hongyu Zhou,Katja Schwarz,Yiyi Liao*

Main category: cs.CV

TL;DR: ReRoPE是一种即插即用的框架，通过将相对相机位姿信息注入预训练视频扩散模型中未充分利用的RoPE低频频谱带，实现精确的相机视角控制，同时保持强大的生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用相对于固定参考帧的相机位姿编码缺乏平移不变性，导致泛化能力差和累积漂移问题，而相对相机位姿嵌入虽然更鲁棒，但难以在不增加训练成本或改变架构的情况下集成到预训练模型中。

Method: 利用Rotary Positional Embeddings (RoPE)在现有模型中未充分利用低频频谱带的特点，将相对相机位姿信息无缝注入这些频带，实现精确的相机控制。

Result: 在图像到视频(I2V)和视频到视频(V2V)任务中，ReRoPE在相机控制精度和视觉保真度方面表现出色，提供了训练效率高的可控高保真视频生成方案。

Conclusion: ReRoPE为预训练视频扩散模型提供了一种无需大量训练成本或架构修改的相机控制解决方案，通过智能利用RoPE的频谱特性实现了精确的视角控制。

Abstract: Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/

</details>


### [126] [ViT-5: Vision Transformers for The Mid-2020s](https://arxiv.org/abs/2602.08071)
*Feng Wang,Sucheng Ren,Tiezheng Zhang,Predrag Neskovic,Anand Bhattad,Cihang Xie,Alan Yuille*

Main category: cs.CV

TL;DR: ViT-5是对Vision Transformer架构的现代化升级，通过整合过去五年的架构创新，在保持Attention-FFN基本结构的同时，在归一化、激活函数、位置编码、门控机制和可学习token等方面进行组件级优化，显著提升了视觉理解与生成任务的性能。


<details>
  <summary>Details</summary>
Motivation: 随着Vision Transformer架构在过去五年中的快速发展，许多新的架构改进尚未被系统地整合到基础模型中。本研究旨在通过系统地整合这些创新，构建一个现代化的Vision Transformer主干网络，为2020年代中期提供简单易用的升级方案。

Method: 采用组件级精细化策略，在保持经典Attention-FFN结构的基础上，对归一化层、激活函数、位置编码方案、门控机制和可学习token等多个关键组件进行现代化更新，形成新一代ViT-5架构。

Result: 在ImageNet-1K分类任务上，ViT-5-Base达到84.2%的top-1准确率，超越DeiT-III-Base的83.8%；在生成任务中，作为SiT扩散框架的主干网络，ViT-5实现1.84 FID，优于原始ViT主干的2.06 FID。同时展现出改进的表征学习能力和更好的空间推理行为。

Conclusion: ViT-5通过系统整合近期的架构创新，提供了一个简单易用的现代化Vision Transformer升级方案，在理解和生成任务上均表现出色，具有良好的任务迁移能力，符合当代基础模型的实践标准。

Abstract: This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.

</details>


### [127] [VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval](https://arxiv.org/abs/2602.08099)
*Issar Tzachor,Dvir Samuel,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 无需视觉监督，通过中间层嵌入提取和文本对齐策略，MLLM在视频检索任务中实现零样本SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有生成式多模态大语言模型在视频任务中表现不及视频基础模型，需要探索如何有效利用MLLM进行视频-文本嵌入和检索

Method: 1. 进行层间分析发现中间层已编码任务相关信息；2. 结合中间层嵌入和校准的MLLM头部实现零样本检索；3. 提出轻量级文本对齐策略，将密集视频描述映射到简短摘要

Result: 无需视觉微调，仅通过文本处理就在多个视频检索基准测试中大幅超越现有方法，达到最先进水平

Conclusion: MLLM的中间层蕴含丰富视频理解能力，通过合理的嵌入提取和文本对齐策略可以实现卓越的零样本视频检索性能

Abstract: Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.

</details>


### [128] [MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.08112)
*Sidike Paheding,Abel Reyes-Angulo,Leo Thomas Ramos,Angel D. Sappa,Rajaneesh A.,Hiral P. B.,Sajin Kumar K. S.,Thomas Oommen*

Main category: cs.CV

TL;DR: MMLSv2是一个用于火星表面滑坡分割的多模态数据集，包含664张七波段图像和276张地理隔离测试图像，支持模型训练但在地理泛化方面仍存在挑战


<details>
  <summary>Details</summary>
Motivation: 为解决火星滑坡检测中缺乏多模态数据集和地理泛化评估的问题，创建包含多种遥感波段和地理隔离测试集的数据集

Method: 构建包含RGB、数字高程模型、坡度、热惯性和灰度通道的七波段多模态图像数据集，分为训练/验证/测试集和地理隔离测试集

Result: 实验显示数据集支持稳定训练并达到竞争性性能，但在碎片化、细长和小规模滑坡区域仍有挑战，地理隔离测试集导致性能显著下降

Conclusion: MMLSv2为火星滑坡分割提供了有价值的基准数据集，特别在地理泛化评估方面具有重要作用，揭示了模型在分布外数据上的鲁棒性挑战

Abstract: We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2

</details>


### [129] [Building Damage Detection using Satellite Images and Patch-Based Transformer Methods](https://arxiv.org/abs/2602.08117)
*Smriti Siva,Jan Cross-Zamirski*

Main category: cs.CV

TL;DR: 评估Vision Transformer模型在xBD卫星数据集上的建筑损伤分类性能，提出基于补丁的预处理流程和冻结头微调策略，在噪声和类别不平衡数据上取得与CNN基准相当的F1分数


<details>
  <summary>Details</summary>
Motivation: 快速建筑损伤评估对灾后响应至关重要，但卫星图像数据存在标签噪声和严重类别不平衡问题，需要研究ViT模型在此类数据上的表现

Method: 使用DINOv2-small和DeiT模型，提出针对性补丁预处理流程隔离结构特征并减少背景噪声，采用冻结头微调策略降低计算需求，通过准确率、精确率、召回率和宏观平均F1分数评估性能

Result: 小型ViT架构配合新颖训练方法在灾害分类任务中取得了与先前CNN基准相当的竞争性宏观平均F1分数

Conclusion: Vision Transformer模型通过适当的预处理和训练策略，能够在噪声和类别不平衡的卫星图像数据上有效进行建筑损伤分类，为灾后快速评估提供可行的技术方案

Abstract: Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.
  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.

</details>


### [130] [MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection](https://arxiv.org/abs/2602.08126)
*Venkatraman Narayanan,Bala Sai,Rahul Ahuja,Pratik Likhar,Varun Ravi Kumar,Senthil Yogamani*

Main category: cs.CV

TL;DR: MambaFusion是一种新型多模态3D目标检测框架，通过选择性状态空间模型与窗口化Transformer的交替结构实现线性时间复杂度的全局上下文建模，结合多模态令牌对齐和可靠性感知融合机制，在nuScenes基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中多模态融合的挑战：相机提供密集视觉线索但深度信息不准确，LiDAR提供精确3D结构但覆盖稀疏。现有BEV融合框架存在上下文建模效率低、空间不变融合和不确定性推理困难等问题。

Method: 1. 交替使用选择性状态空间模型(SSMs)和窗口化Transformer进行全局上下文传播；2. 多模态令牌对齐(MTA)模块基于空间置信度和标定一致性动态重加权相机-LiDAR特征；3. 结构条件扩散头集成基于图的推理和不确定性感知去噪，确保物理合理性和校准置信度。

Result: 在nuScenes基准测试中建立了新的最先进性能，同时保持线性时间复杂度，为实际自动驾驶系统提供鲁棒、时间稳定且可解释的3D感知能力。

Conclusion: 将SSM的高效性与可靠性驱动的融合相结合，能够为现实世界自动驾驶系统提供强大的多模态3D感知解决方案，证明了该框架在效率、适应性和物理基础方面的优势。

Abstract: Reliable 3D object detection is fundamental to autonomous driving, and multimodal fusion algorithms using cameras and LiDAR remain a persistent challenge. Cameras provide dense visual cues but ill posed depth; LiDAR provides a precise 3D structure but sparse coverage. Existing BEV-based fusion frameworks have made good progress, but they have difficulties including inefficient context modeling, spatially invariant fusion, and reasoning under uncertainty. We introduce MambaFusion, a unified multi-modal detection framework that achieves efficient, adaptive, and physically grounded 3D perception. MambaFusion interleaves selective state-space models (SSMs) with windowed transformers to propagate the global context in linear time while preserving local geometric fidelity. A multi-modal token alignment (MTA) module and reliability-aware fusion gates dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency. Finally, a structure-conditioned diffusion head integrates graph-based reasoning with uncertainty-aware denoising, enforcing physical plausibility, and calibrated confidence. MambaFusion establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity. The framework demonstrates that coupling SSM-based efficiency with reliability-driven fusion yields robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.

</details>


### [131] [Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries](https://arxiv.org/abs/2602.08131)
*Isaac Corley,Hannah Kerner,Caleb Robinson,Jennifer Marcus*

Main category: cs.CV

TL;DR: FTW生态系统提供160万个农田边界多边形基准、预训练分割模型和命令行工具，支持农田边界提取和作物分类，在有限标签下实现0.65-0.75的F1分数，覆盖24个国家476万平方公里区域。


<details>
  <summary>Details</summary>
Motivation: 农田边界地图是农业数据产品的基础，对作物监测、产量估算和病害评估至关重要，需要标准化工具和基准数据集来支持大规模农业分析。

Method: 使用MOSAIKS随机卷积特征和FTW导出的农田边界，通过预训练分割模型进行农田边界提取，结合两个notebook分别处理局部尺度的作物分类和森林损失归因，以及国家尺度的云优化数据推理。

Result: 在有限标签条件下实现作物类型分类的宏观F1分数0.65-0.75，在五个国家476万平方公里区域生成预计算预测，中位预测农田面积从卢旺达的0.06公顷到瑞士的0.28公顷不等。

Conclusion: FTW生态系统为全球农业监测提供了可扩展的工具和基准数据集，证明了在有限监督下实现大规模农田边界提取和作物分类的可行性，为精准农业应用奠定了基础。

Abstract: Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).

</details>


### [132] [Robustness of Vision Language Models Against Split-Image Harmful Input Attacks](https://arxiv.org/abs/2602.08136)
*Md Rafi Ur Rashid,MD Sadik Hossain Shanto,Vishnu Asutosh Dasu,Shagufta Mehnaz*

Main category: cs.CV

TL;DR: SIVA攻击方法利用VLMs在分割图像输入上的安全对齐漏洞，通过渐进式攻击策略和对抗知识蒸馏实现高转移成功率，揭示了当前VLM安全对齐的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉越狱攻击主要针对完整图像，而VLMs经过广泛安全对齐后对此类攻击具有强鲁棒性。研究发现VLMs的安全对齐未考虑跨多个图像片段分布的有害语义，导致对分割图像输入的安全检测失效。

Method: 提出分割图像视觉越狱攻击(SIVA)，包含渐进式攻击策略：从朴素分割到自适应白盒攻击，最终发展为黑盒转移攻击。最强策略采用新型对抗知识蒸馏(Adv-KD)算法提升跨模型转移能力。

Result: 在三个最先进VLMs和三个越狱数据集上的评估显示，最强攻击相比现有基线实现了高达60%的转移成功率提升。

Conclusion: 当前VLM安全对齐存在关键漏洞，需要针对分割图像输入开发更全面的安全对齐方法。研究提出了解决此漏洞的有效途径。

Abstract: Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.

</details>


### [133] [DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation](https://arxiv.org/abs/2602.08168)
*Mei Ling Chee,Thangarajah Akilan,Aparna Ravindra Phalke,Kanchan Keisham*

Main category: cs.CV

TL;DR: DAS-SK是一个轻量级语义分割架构，通过将选择性核卷积整合到双空洞可分离卷积模块中，在农业高分辨率图像分割中实现了精度与计算效率的平衡，显著减少了参数和计算量。


<details>
  <summary>Details</summary>
Motivation: 农业高分辨率图像语义分割需要平衡精度和计算效率的模型，以便在实际系统中部署，特别是无人机和边缘设备等资源受限环境。

Method: 提出DAS-SK架构，将选择性核卷积(SK-Conv)整合到双空洞可分离卷积(DAS-Conv)模块中，增强多尺度特征学习；改进空洞空间金字塔池化(ASPP)模块，同时捕获细粒度局部结构和全局上下文信息；基于改进的DeepLabV3框架，使用MobileNetV3-Large和EfficientNet-B3两个互补主干网络。

Result: 在LandCover.ai、VDD和PhenoBench三个基准测试中，DAS-SK持续达到最先进性能，比CNN、transformer和混合模型更高效；相比顶级transformer模型，参数减少21倍，GFLOPs减少19倍。

Conclusion: DAS-SK是农业机器人和高分辨率遥感实时应用的稳健、高效、可扩展解决方案，在其他视觉领域也具有广泛部署潜力。

Abstract: Semantic segmentation in high-resolution agricultural imagery demands models that strike a careful balance between accuracy and computational efficiency to enable deployment in practical systems. In this work, we propose DAS-SK, a novel lightweight architecture that retrofits selective kernel convolution (SK-Conv) into the dual atrous separable convolution (DAS-Conv) module to strengthen multi-scale feature learning. The model further enhances the atrous spatial pyramid pooling (ASPP) module, enabling the capture of fine-grained local structures alongside global contextual information. Built upon a modified DeepLabV3 framework with two complementary backbones - MobileNetV3-Large and EfficientNet-B3, the DAS-SK model mitigates limitations associated with large dataset requirements, limited spectral generalization, and the high computational cost that typically restricts deployment on UAVs and other edge devices. Comprehensive experiments across three benchmarks: LandCover.ai, VDD, and PhenoBench, demonstrate that DAS-SK consistently achieves state-of-the-art performance, while being more efficient than CNN-, transformer-, and hybrid-based competitors. Notably, DAS-SK requires up to 21x fewer parameters and 19x fewer GFLOPs than top-performing transformer models. These findings establish DAS-SK as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains.

</details>


### [134] [PEGAsus: 3D Personalization of Geometry and Appearance](https://arxiv.org/abs/2602.08198)
*Jingyu Hu,Bin Hu,Ka-Hei Hui,Haipeng Li,Zhengzhe Liu,Daniel Cohen-Or,Chi-Wing Fu*

Main category: cs.CV

TL;DR: PEGAsus是一个新的3D形状个性化生成框架，能够在几何和外观两个层面学习形状概念，通过提取可重用的类别无关属性并与文本结合来生成新颖形状。


<details>
  <summary>Details</summary>
Motivation: 解决3D形状个性化生成问题，需要从参考形状中提取可重用的几何和外观属性，并与文本描述结合来创造新颖的形状，实现细粒度的形状生成控制。

Method: 1) 将3D形状个性化定义为提取可重用的类别无关几何和外观属性；2) 设计渐进式优化策略在几何和外观层面学习形状概念；3) 扩展到区域级概念学习，使用上下文感知和无上下文损失。

Result: 实验结果表明PEGAsus能够有效从各种参考形状中提取属性，并与文本灵活组合生成新形状，在跨类别场景下也能产生多样化的个性化结果，定量和定性实验均优于现有最先进方法。

Conclusion: PEGAsus框架通过几何和外观层面的概念学习，实现了有效的3D形状个性化生成，提供了细粒度的控制能力，在跨类别场景下表现出色，超越了现有技术水平。

Abstract: We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.

</details>


### [135] [Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video](https://arxiv.org/abs/2602.08202)
*Jinrong Lv,Xun Gong,Zhaohuan Li,Weili Jiang*

Main category: cs.CV

TL;DR: 提出MCSDR方法，使用条件分数扩散模型从超声心动图视频中生成左心室射血分数的连续后验分布，解决传统回归方法在处理多模态分布时的局限性，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 左心室射血分数估计是一个不适定逆问题，单一视频序列可能对应多个合理的生理值分布。传统深度学习方法采用MSE损失回归条件期望，但在后验分布多模态或重尾时会产生误导性预测。

Method: 提出多模态条件分数扩散回归模型(MCSDR)，基于生成式回归范式，建模超声心动图视频和患者人口统计学属性先验条件下的连续后验分布。

Result: 在EchoNet-Dynamic、EchoNet-Pediatric和CAMUS数据集上的广泛实验表明MCSDR达到最先进性能。定性分析显示模型在高噪声或显著生理变异病例中表现出不同的生成轨迹。

Conclusion: 从确定性回归向生成式回归的范式转变为LVEF估计提供了更准确的概率建模，并为AI辅助诊断提供了新的可解释性层面。

Abstract: Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.

</details>


### [136] [Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2602.08206)
*Chufeng Zhou,Jian Wang,Xinyuan Liu,Xiaokang Zhang*

Main category: cs.CV

TL;DR: 提出GR-CoT框架，通过地理空间推理链增强MLLM的场景理解能力，解决遥感图像中光谱相似但语义不同的地物分类歧义问题


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇语义分割方法依赖视觉-文本特征被动映射，缺乏地理空间上下文感知，导致语义歧义和误分类

Method: 包含离线知识蒸馏流和在线实例推理流。离线流建立细粒度类别解释标准，在线流执行宏观场景锚定、视觉特征解耦和知识驱动决策合成的顺序推理过程

Result: 在LoveDA和GID5基准测试中表现出优越性能

Conclusion: GR-CoT框架通过地理空间推理有效解决了遥感图像开放词汇分割中的语义歧义问题，实现了精确的地理语义对齐

Abstract: Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.

</details>


### [137] [Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension](https://arxiv.org/abs/2602.08211)
*Yik Lung Pang,Changjae Oh*

Main category: cs.CV

TL;DR: 提出了一种名为Chain-of-Caption的训练免费框架，通过结合多种视觉和文本上下文来提升多模态大语言模型在指代表达理解任务中的性能，在多个数据集上实现了5%到30%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型虽然通过扩大模型规模和训练数据在REC任务上取得了高准确率，但通过Chain-of-Thought和工具使用等技术提供额外上下文可以进一步提升性能，需要系统分析不同上下文提供方式的效果。

Method: 提出Chain-of-Caption训练免费框架，分析通过工具使用提供额外视觉和文本上下文的方法，在RefCOCO/RefCOCOg/RefCOCO+和Ref-L4数据集上进行实验，评估不同上下文组合对REC性能的影响。

Result: 实验表明单独的文本或视觉上下文都能在不进行微调的情况下提升REC性能，通过组合多种上下文，该框架在不同IoU阈值下的准确率比基线模型提升了5%到30%。

Conclusion: Chain-of-Caption框架有效证明了通过工具使用提供额外上下文可以显著提升MLLMs在REC任务上的性能，为训练免费的模型优化提供了新思路。

Abstract: Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.

</details>


### [138] [Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval](https://arxiv.org/abs/2602.08224)
*Jing Zhang,Zhikai Li,Xuewen Liu,Qingyi Gu*

Main category: cs.CV

TL;DR: Efficient-SAM2通过对象感知稀疏窗口路由(SWR)和稀疏记忆检索(SMR)技术，在保持SAM2分割精度的同时显著提升推理效率，实现1.68倍加速且仅损失1.0%准确率。


<details>
  <summary>Details</summary>
Motivation: SAM2在视频对象分割中表现出色但计算负担重，现有优化方法主要关注重新训练轻量级主干网络，缺乏对训练后加速的探索。研究发现SAM2具有类似生物视觉的稀疏感知模式，存在消除冗余计算的机会。

Method: 提出对象感知稀疏窗口路由(SWR)：利用前一帧解码器的一致性线索将背景区域路由到轻量级快捷分支；提出对象感知稀疏记忆检索(SMR)：仅让每帧中的显著记忆标记参与计算，并重用首次识别的显著性模式。

Result: Efficient-SAM2在SAM2.1-L模型上实现1.68倍加速，在SA-V测试集上仅损失1.0%的准确率，额外参数量可忽略不计，训练开销最小。

Conclusion: 通过利用SAM2的稀疏感知特性，提出的Efficient-SAM2方法有效消除了任务无关计算，显著提升了推理效率，为实时视频处理应用提供了可行解决方案。

Abstract: Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.

</details>


### [139] [Generating Adversarial Events: A Motion-Aware Point Cloud Framework](https://arxiv.org/abs/2602.08230)
*Hongwei Ren,Youxin Jiang,Qifei Gu,Xiangqian Wu*

Main category: cs.CV

TL;DR: MA-ADV是首个利用点云表示生成对抗事件的方法，通过扩散平滑扰动、Adam优化和二分搜索实现最小成本扰动，在事件感知系统中达到100%攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 事件相机在安全关键领域广泛应用，但深度神经网络对对抗样本的脆弱性威胁事件系统的可靠性。由于主流事件表示的非可微性，基于梯度的攻击方法难以扩展，事件对抗攻击研究稀缺。

Method: 提出MA-ADV框架：利用点云表示事件，考虑事件中的高频噪声，采用基于扩散的方法平滑扰动，充分利用事件的空间和时间关系，通过样本级Adam优化、迭代精化和二分搜索寻找最小成本扰动。

Result: 实验验证MA-ADV实现100%攻击成功率且扰动成本最小，对抗防御方法表现出更强的鲁棒性。

Conclusion: 该工作揭示了事件感知系统面临的关键安全挑战，MA-ADV为事件对抗攻击研究提供了有效解决方案。

Abstract: Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \textbf{M}otion-\textbf{A}ware \textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.

</details>


### [140] [Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2602.08262)
*Guoqi Yu,Xiaowei Hu,Angelica I. Aviles-Rivero,Anqi Qiu,Shujun Wang*

Main category: cs.CV

TL;DR: DeCI框架通过循环-漂移分解和通道独立性处理原始BOLD信号，在fMRI脑疾病分类中优于传统功能连接方法和现有时间序列模型


<details>
  <summary>Details</summary>
Motivation: 传统基于Pearson相关性的功能连接方法将4D BOLD信号降维为静态2D矩阵，丢失了时间动态信息且仅捕获线性关系，需要直接建模时间信息

Method: 提出DeCI框架：1）循环-漂移分解分离每个ROI中的周期性振荡和慢基线趋势；2）通道独立性分别建模每个ROI以提高鲁棒性和减少过拟合

Result: 在五个公共数据集上，DeCI在分类准确率和泛化能力方面均优于基于功能连接的方法和现有时间序列基线模型

Conclusion: 研究支持在fMRI分析中转向端到端时间建模，以更好地捕获复杂脑动力学，DeCI框架为此提供了有效解决方案

Abstract: Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.

</details>


### [141] [PISCO: Precise Video Instance Insertion with Sparse Control](https://arxiv.org/abs/2602.08277)
*Xiangbo Gao,Renjie Li,Xinghao Chen,Yuheng Wu,Suofei Feng,Qing Yin,Zhengzhong Tu*

Main category: cs.CV

TL;DR: PISCO是一个用于精确视频实例插入的视频扩散模型，支持任意稀疏关键帧控制，能够自动传播物体外观、运动和交互，在稀疏控制条件下优于现有修复和视频编辑基线方法。


<details>
  <summary>Details</summary>
Motivation: 专业AI辅助电影制作需要精确的目标修改，视频实例插入需要在保持场景完整性的同时将特定实例插入现有素材，这要求精确的时空定位、物理一致的场景交互和原始动态的忠实保持。

Method: 提出PISCO视频扩散模型，采用可变信息引导进行鲁棒条件化，分布保持时间掩码稳定时间生成，以及几何感知条件化实现真实场景适应，解决预训练视频扩散模型中稀疏条件化引起的分布偏移问题。

Result: 实验显示PISCO在稀疏控制下持续优于强基线方法，随着控制信号增加表现出清晰、单调的性能提升，并构建了PISCO-Bench基准数据集进行评估。

Conclusion: PISCO代表了AI视频生成从通用生成向精细可控生成和高保真后处理的关键转变，为专业视频编辑提供了有效的解决方案。

Abstract: The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.

</details>


### [142] [Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning](https://arxiv.org/abs/2602.08282)
*Haixu Liu,Yufei Wang,Tianxiang Xu,Chuancheng Shi,Hongsheng Xing*

Main category: cs.CV

TL;DR: 提出多模态融合框架，结合PA和PO数据优势，通过地理对齐策略和混合专家范式解决数据稀疏性、标签噪声和分布偏移问题，在GeoLifeCLEF 2025数据集上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 解决大规模跨物种植物分布预测中PA数据稀缺昂贵、PO数据标签噪声严重的问题，以及训练测试样本间地理分布偏移的挑战

Method: 采用Swin Transformer Base处理卫星图像，TabM网络提取表格特征，Temporal Swin Transformer建模时间序列，使用可堆叠三模态交叉注意力机制融合异质模态；基于卫星影像地理覆盖的伪标签聚合策略实现标签空间与遥感特征空间对齐；混合专家范式按空间邻近度分区处理测试样本

Result: 在GeoLifeCLEF 2025数据集上，特别是在PA覆盖有限和分布偏移显著的情况下，表现出优越的预测性能

Conclusion: 多模态融合框架和混合专家范式有效解决了植物分布预测中的数据挑战，为生物多样性保护提供了可靠的技术方案

Abstract: Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.

</details>


### [143] [CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment](https://arxiv.org/abs/2602.08309)
*Yunzuo Hu,Wen Li,Jing Zhang*

Main category: cs.CV

TL;DR: CAE-AV框架通过两个互补模块CASTE和CASE解决音频-视觉学习中的模态不对齐问题，利用跨模态一致性引导和字幕对齐的显著性增强来提升表示质量，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 音频-视觉学习面临由屏幕外源和背景杂波引起的模态不对齐问题，现有方法通常放大不相关区域或时刻，导致训练不稳定和表示质量下降。

Method: 提出CAE-AV框架，包含两个模块：1) CASTE通过评估帧级音频-视觉一致性动态平衡时空关系；2) CASE在选定的时空位置注入跨模态语义引导。还设计了轻量级目标函数：字幕-模态InfoNCE、视觉-音频一致性和熵正则化。

Result: 使用冻结骨干网络，在AVE、AVVP、AVS和AVQA基准测试中达到最先进性能，定性分析验证了其对音频-视觉不对齐的鲁棒性。

Conclusion: CAE-AV框架通过跨模态一致性引导和语义对齐有效缓解了音频-视觉不对齐问题，显著提升了多模态学习的表示质量和性能。

Abstract: Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, and current methods usually amplify irrelevant regions or moments, leading to unstable training and degraded representation quality. To address this challenge, we proposed a novel Caption-aligned and Agreement-guided Enhancement framework (CAE-AV) for audio-visual learning, which used two complementary modules: Cross-modal Agreement-guided Spatio-Temporal Enrichment (CASTE) and Caption-Aligned Saliency-guided Enrichment (CASE) to relieve audio-visual misalignment. CASTE dynamically balances spatial and temporal relations by evaluating frame-level audio-visual agreement, ensuring that key information is captured from both preceding and subsequent frames under misalignment. CASE injects cross-modal semantic guidance into selected spatio-temporal positions, leveraging high-level semantic cues to further alleviate misalignment. In addition, we design lightweight objectives, caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization to guide token selection and strengthen cross-modal semantic alignment. With frozen backbones, CAE-AV achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks, and qualitative analyses further validate its robustness against audio-visual misalignment.

</details>


### [144] [Language-Guided Transformer Tokenizer for Human Motion Generation](https://arxiv.org/abs/2602.08337)
*Sheng Yan,Yong Wang,Xin Du,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: LG-Tok是一种语言引导的运动离散化方法，通过Transformer架构实现语言与运动的高效对齐，在减少token数量的同时保持高质量重建，并在HumanML3D和Motion-X基准测试中超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 传统运动离散化方法通过增加token数量来提高重建质量，但这会增加生成模型的学习难度。需要一种既能保持高质量重建又能降低生成复杂度的解决方案。

Method: 提出语言引导的token化方法(LG-Tok)，使用基于Transformer的tokenizer实现语言与运动的全局对齐，并设计了语言丢弃方案使detokenizer支持无语言条件生成。

Result: 在HumanML3D和Motion-X基准测试中，LG-Tok获得Top-1分数0.542和0.582（优于MARDM的0.500和0.528），FID分数0.057和0.088（优于0.114和0.147）。LG-Tok-mini仅用一半token仍保持竞争性能。

Conclusion: LG-Tok通过语言引导实现了高效的运动语义表示，显著提升了运动生成的质量和效率，证明了语言对齐在运动token化中的重要性。

Abstract: In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.

</details>


### [145] [UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science](https://arxiv.org/abs/2602.08342)
*Jie Zhang,Xingtong Yu,Yuan Fang,Rudi Stouffs,Zdravko Trivic*

Main category: cs.CV

TL;DR: UGData数据集通过空间图对齐街景图像，UGE方法结合指令对比学习和图空间编码实现图像、文本与空间结构对齐，UGBench基准测试显示在多个VLM骨干网上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 解决城市环境多模态嵌入学习中缺乏显式空间对齐的问题，现有数据集和基准测试无法有效捕捉城市理解的空间特性

Method: 两阶段训练策略：1) 指令引导的对比学习 2) 基于图的空间编码；使用LoRA调优在Qwen2-VL、Phi-3-Vision等VLM骨干网上训练固定维度空间嵌入

Result: 在Qwen2.5-VL-7B上实现图像检索44%提升和地理位置排名30%提升，在未见城市上分别获得30%和22%的增益

Conclusion: 显式空间接地对于空间密集型城市任务具有显著有效性，UGData和UGE方法为城市多模态理解提供了有效的空间对齐解决方案

Abstract: Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.

</details>


### [146] [What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning](https://arxiv.org/abs/2602.08346)
*Yujin Zhou,Pengcheng Wen,Jiale Chen,Boqin Yin,Han Zhu,Jiaming Ji,Juntao Dai,Chi-Min Chan,Sirui Han*

Main category: cs.CV

TL;DR: 本文提出了首个专门针对图像思维范式下过程奖励模型（PRMs）的综合性基准测试，包含1206条人工标注的推理轨迹，定义了7种细粒度错误类型，并验证了当前大视觉语言模型作为PRMs的局限性。


<details>
  <summary>Details</summary>
Motivation: 图像思维范式虽然提升了视觉推理能力，但在推理过程中会产生多样化的错误，而现有的PRMs基准主要面向文本，缺乏对这一范式的全面评估，因此需要专门的基准来评估和改进PRMs。

Method: 通过分析推理轨迹和PRMs引导搜索实验，定义了7种细粒度错误类型；构建了包含1206条人工标注推理轨迹的基准，涵盖4个类别和16个子类别；对当前LVLMs作为PRMs的能力进行了实验分析。

Result: 实验表明当前LVLMs作为PRMs表现不足：在视觉推理过程评估中能力有限，在不同错误类型上性能差异显著，存在正向评估偏差，且对推理步骤位置敏感。

Conclusion: 该基准有效验证了PRMs评估的必要性，为提升LVLMs中PRMs的能力奠定了重要基础，揭示了专门开发PRMs的迫切需求和改进潜力。

Abstract: The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.

</details>


### [147] [E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs](https://arxiv.org/abs/2602.08355)
*Xianjie Liu,Yiman Hu,Liang Wu,Ping Hu,Yixiong Zou,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 提出了一个电商短视频理解基准E-VAds，包含3961个视频和19785个问答对，并开发了基于强化学习的E-VAds-R1模型，在商业意图推理任务上取得109.2%的性能提升


<details>
  <summary>Details</summary>
Motivation: 电商短视频具有目标驱动格式和密集多模态信号的特点，现有模型难以处理此类内容，且现有基准主要关注通用任务而忽略了商业意图推理

Method: 首先提出多模态信息密度评估框架量化领域复杂度，然后构建E-VAds基准数据集，使用多智能体系统生成问答对，最后开发基于强化学习的E-VAds-R1模型，采用多粒度奖励设计MG-GRPO策略

Result: 评估显示电商内容在视觉、音频和文本模态上的信息密度显著高于主流数据集，E-VAds-R1模型仅用数百个训练样本就在商业意图推理上实现了109.2%的性能增益

Conclusion: 电商短视频理解是一个具有挑战性的前沿领域，E-VAds基准和E-VAds-R1模型为解决这一领域的复杂推理任务提供了有效解决方案

Abstract: E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a \textbf{multi-modal information density assessment framework} to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce \textbf{E-commerce Video Ads Benchmark (E-VAds)}, which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop \textbf{E-VAds-R1}, an RL-based reasoning model featuring a multi-grained reward design called \textbf{MG-GRPO}. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.

</details>


### [148] [Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers](https://arxiv.org/abs/2602.08388)
*Shuo Zhang,Wenzhuo Wu,Huayu Zhang,Jiarong Cheng,Xianghao Zang,Chao Ban,Hao Sun,Zhongjiang He,Tianwei Cao,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: GeoEdit是一个基于扩散变换器的图像编辑框架，通过上下文生成实现精确的几何变换编辑，并引入效果敏感注意力机制来增强光照和阴影效果的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在处理几何变换（平移、旋转、缩放）时存在困难，特别是在复杂场景中，且对复杂光照和阴影效果建模不足，导致结果不真实。

Method: 提出GeoEdit框架，利用扩散变换器模块进行上下文生成，集成几何变换实现精确对象编辑；引入效果敏感注意力机制增强光照和阴影建模；构建包含12万张高质量图像对的RS-Objects数据集进行训练。

Result: 在公共基准测试中，GeoEdit在视觉质量、几何精度和真实感方面持续优于最先进的方法。

Conclusion: GeoEdit通过创新的几何变换集成和效果敏感注意力机制，成功解决了复杂场景中几何编辑和光照效果建模的挑战，显著提升了图像编辑的真实性和准确性。

Abstract: Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.

</details>


### [149] [D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy](https://arxiv.org/abs/2602.08395)
*Jianfeng Liang,Shaocheng Shen,Botao Xu,Qiang Hu,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: D²-VR：一种基于单图像扩散的低步数推理视频修复框架，通过退化鲁棒流对齐模块和对抗蒸馏实现12倍加速，在保持感知质量的同时确保时间一致性


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散先验和时间对齐的视频修复方法虽然感知质量优异，但面临推理延迟高和时间不稳定的问题，限制了实际部署

Method: 1. 设计退化鲁棒流对齐(DRFA)模块，利用置信感知注意力过滤不可靠运动线索；2. 采用对抗蒸馏范式压缩扩散采样轨迹；3. 设计协同优化策略平衡感知质量与时间一致性

Result: 实验表明D²-VR达到最先进性能，同时将采样过程加速12倍

Conclusion: 该方法有效解决了扩散基视频修复框架的延迟和不稳定性问题，为实际应用提供了可行解决方案

Abstract: The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}

</details>


### [150] [RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications](https://arxiv.org/abs/2602.08397)
*Chiara Lena,Davide Milesi,Alessandro Casella,Luca Carlini,Joseph C. Norton,James Martin,Bruno Scaglioni,Keith L. Obstein,Roberto De Sire,Marco Spadaccini,Cesare Hassan,Pietro Valdastri,Elena De Momi*

Main category: cs.CV

TL;DR: RealSynCol是一个高度逼真的合成结肠镜数据集，通过CT扫描构建结肠几何结构并在虚拟环境中渲染，包含28,130帧图像及对应的深度图、光流、3D网格和相机轨迹，显著提升了深度学习算法在临床图像上的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 开发结肠镜3D重建和病变识别深度学习方法的瓶颈在于缺乏大规模真实标注数据，需要创建逼真的合成数据集来支持算法开发。

Method: 从10个CT扫描中提取结肠几何结构，导入模拟术中条件的虚拟环境，使用真实血管纹理进行渲染，生成包含多种标注信息的合成数据集。

Result: 基准测试显示RealSynCol的高真实性和变异性显著提高了在临床图像上的深度和姿态估计性能，证明了其作为内窥镜诊断算法开发工具的有效性。

Conclusion: RealSynCol合成数据集为解决结肠镜深度学习中的数据稀缺问题提供了有效解决方案，能够显著提升算法在真实临床场景中的泛化能力。

Abstract: Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.

</details>


### [151] [Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features](https://arxiv.org/abs/2602.08430)
*Qiang Wang*

Main category: cs.CV

TL;DR: 本文重新审视了基于注意力的稀疏图像匹配模型训练问题，发现了LightGlue模型中被忽视的关键设计选择，分析了检测器和描述子在transformer匹配框架中的作用，并提出了一种使用多样化检测器关键点微调现有模型的通用方法。


<details>
  <summary>Details</summary>
Motivation: 重新审视基于注意力的稀疏图像匹配模型训练问题，识别影响LightGlue模型性能的关键设计选择，并探索检测器和描述子在transformer匹配框架中的相对重要性。

Method: 首先识别关键设计选择，然后分析检测器和描述子的作用，最后提出使用多种检测器的关键点来微调现有图像匹配模型的新方法，构建检测器无关的通用模型。

Result: 当作为零样本匹配器用于新检测器时，所得到的模型达到或超过了专门为这些特征训练的模型的准确率。检测器而非描述子通常是性能差异的主要原因。

Conclusion: 研究结果为基于transformer的匹配模型的部署和局部特征的未来设计提供了有价值的见解，证明了构建检测器无关的通用匹配模型的可行性。

Abstract: We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.

</details>


### [152] [Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition](https://arxiv.org/abs/2602.08439)
*Yuhao Dong,Shulin Tian,Shuai Liu,Shuangrui Ding,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: 提出了Demo-driven Video In-Context Learning任务和Demo-ICL-Bench基准，用于评估多模态大语言模型从视频演示中学习的能力，并开发了Demo-ICL模型通过两阶段训练策略有效解决这一挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准主要评估模型基于静态内部知识的能力，而非从动态新颖上下文中学习和适应的能力，需要填补这一研究空白。

Method: 构建包含1200个教学视频的Demo-ICL-Bench基准，提出文本和视频两种演示形式，开发Demo-ICL模型采用视频监督微调和信息辅助直接偏好优化的两阶段训练策略。

Result: 实验证明Demo-ICL-Bench具有挑战性，Demo-ICL模型在该基准上表现出色，验证了所提方法的有效性。

Conclusion: 该研究为视频上下文学习开辟了新方向，展示了从演示中学习视频内容的重要性，并为未来研究提供了基准和方法基础。

Abstract: Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.

</details>


### [153] [Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries](https://arxiv.org/abs/2602.08448)
*Haocheng Lu,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: Vista是一个用于流式视频问答的创新型框架，通过场景感知的分割、压缩和召回机制，解决了MLLMs在处理连续视频流时的内存和效率问题，在StreamingBench上实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 流式视频问答面临视频帧顺序到达和用户查询任意时间点发出的挑战，现有基于固定大小内存或简单压缩的方法存在上下文丢失或内存溢出问题，限制了在长时、实时场景中的有效性。

Method: 1) 场景感知分割：动态将输入帧聚类为时空和视觉连贯的场景单元；2) 场景感知压缩：将每个场景压缩为紧凑的token表示存储在GPU内存中，全分辨率帧卸载到CPU内存；3) 场景感知召回：收到查询时选择性召回相关场景并重新整合到模型输入中。

Result: 在StreamingBench上的大量实验表明，Vista实现了最先进的性能，为现实世界的流式视频理解建立了强大的基线。

Conclusion: Vista是一个模型无关的框架，可与多种视觉语言骨干无缝集成，在不影响延迟或内存效率的情况下实现长上下文推理，为流式视频问答提供了高效可扩展的解决方案。

Abstract: Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.

</details>


### [154] [TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation](https://arxiv.org/abs/2602.08462)
*Yiyang Cao,Yunze Deng,Ziyu Lin,Bin Feng,Xinggang Wang,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: TriC-Motion是一个基于扩散模型的文本到运动生成框架，通过空间-时间-频率三域联合建模和因果干预，解决了现有方法无法同时利用多域信息的问题，在HumanML3D数据集上取得了0.612的R@1最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本到运动生成方法主要关注空间-时间建模或独立的频率域分析，缺乏跨空间、时间和频率域的统一联合优化框架，导致无法同时利用所有域的信息，生成质量受限。此外，运动生成框架中常存在运动无关噪声与有用特征的纠缠问题。

Method: 提出TriC-Motion框架，包含三个核心建模模块：时间运动编码、空间拓扑建模和混合频率分析。通过分数引导的三域融合模块整合多域信息，并设计基于因果关系的反事实运动解耦器来消除运动无关噪声。

Result: 在HumanML3D数据集上取得了0.612的R@1分数，优于现有最先进方法，能够生成高保真、连贯、多样且与文本对齐的运动序列。

Conclusion: TriC-Motion通过三域联合建模和因果干预机制，有效解决了多域信息利用和噪声解耦问题，显著提升了文本到运动生成的质量和性能。

Abstract: Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.

</details>


### [155] [Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation](https://arxiv.org/abs/2602.08479)
*Alif Rizqullah Mahdi,Mahdi Rezaei,Natasha Merat*

Main category: cs.CV

TL;DR: 基于WIVW数据集开发手势分类框架，使用2D姿态估计提取76个静态动态特征，将行人手势分为四类，分类准确率达87%，显著提升自动驾驶系统对行人手势的感知能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆难以理解行人手势这一非语言交通沟通方式，需要提升对行人手势的识别能力以改善人车交互。

Method: 采用2D姿态估计技术处理真实世界视频序列，提取归一化关键点的76个静态和动态特征，建立四分类（停止、通行、感谢问候、无手势）手势识别系统。

Result: 手势分类准确率达到87%，研究发现手部位置和移动速度是区分不同手势类别的关键判别特征。

Conclusion: 该研究不仅提升了自动驾驶系统的感知能力，还为理解交通场景中行人行为提供了重要见解，证明了姿态估计在行人手势识别中的有效性。

Abstract: Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.

</details>


### [156] [Enhanced Food Category Recognition under Illumination-Induced Domain Shift](https://arxiv.org/abs/2602.08491)
*Keonvin Park,Aditya Pal,Jin Hong Mok*

Main category: cs.CV

TL;DR: 该研究探讨了光照变化对多类别食品识别的影响，通过合成光照增强数据集和跨数据集评估，揭示了光照引起的域偏移问题，并提出光照感知增强方法显著提升了识别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实环境中的视觉食品识别系统（如自动传送带检测）对光照变化引起的域偏移高度敏感。现有研究通常局限于单一食品类别或受控环境，且大多数公开食品数据集缺乏明确的光照标注，因此需要系统研究光照变化对多类别食品识别的影响。

Method: 使用Food-101和Fruits-360两个广泛采用的数据集，通过系统变化光温和光强构建合成光照增强数据集，进行跨数据集评估和域泛化研究，特别关注光照敏感目标类别（如苹果类食品）。

Result: 实验结果显示，跨数据集评估中由于视觉条件不匹配导致准确率显著下降，而光照感知增强方法显著提高了在域偏移下的识别鲁棒性，同时保持了实时性能。

Conclusion: 光照鲁棒性对于在现实世界检测场景中部署可靠的食品识别系统至关重要，光照感知增强为解决光照引起的域偏移问题提供了实用解决方案。

Abstract: Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.
  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.
  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.

</details>


### [157] [Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?](https://arxiv.org/abs/2602.08505)
*Caterina Fuster-Barceló,Virginie Uhlmann*

Main category: cs.CV

TL;DR: Vision foundation models (VFMs) 在生物医学图像分析中表现良好，但在跨不同电子显微镜图像数据集进行迁移时存在显著的领域不匹配问题，即使使用参数高效微调 (LoRA) 也难以实现单一模型的稳健跨域性能。


<details>
  <summary>Details</summary>
Motivation: 研究视觉基础模型在异质电子显微镜图像数据集上的潜在表示是否足够通用，以支持有效的跨域迁移和重用，特别是在线粒体分割任务中。

Method: 使用两个公共EM数据集(Lucchi++和VNC)和三个VFM模型(DINOv2、DINOv3、OpenCLIP)，评估冻结主干网络和LoRA微调两种适应策略，并通过PCA、Fréchet Dinov2距离和线性探针分析潜在表示空间。

Result: 单数据集训练表现良好且LoRA能提升域内性能，但多数据集训练导致性能显著下降，PEFT仅带来边际改善。分析显示两个EM数据集间存在明显的领域不匹配。

Conclusion: VFM在轻量适应下可在单一域内实现竞争性EM分割性能，但当前PEFT策略不足以获得跨异质EM数据集的单一稳健模型，需要额外的领域对齐机制。

Abstract: Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets. Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP). We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset. Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT. Exploration of the latent representation space through various techniques (PCA, Fréchet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training. These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.

</details>


### [158] [GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving](https://arxiv.org/abs/2602.08524)
*Linger Deng,Yuliang Liu,Wenwen Yu,Zujia Zhang,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: GeoFocus是一个新颖的几何问题解决框架，通过Critical Local Perceptor模块增强局部结构感知，以及VertexLang语言优化全局拓扑编码，在多个几何数据集上显著提升性能


<details>
  <summary>Details</summary>
Motivation: 解决大型多模态模型在几何问题解决中的挑战，特别是需要同时关注全局形状识别和基于几何理论的精细局部关系

Method: 提出两模块框架：1) Critical Local Perceptor - 通过13个理论感知模板自动识别关键局部结构；2) VertexLang - 紧凑的拓扑形式语言，用顶点坐标和连接关系编码全局图形

Result: 在Geo3K、GeoQA和FormalGeo7K数据集上比领先专业模型准确率提升4.7%，局部特征覆盖率提高61%，全局感知训练时间减少20%，在MATHVERSE中展现优越鲁棒性

Conclusion: GeoFocus通过结合局部结构感知和全局拓扑编码，有效提升了多模态模型在几何问题解决中的性能，为几何推理提供了新的解决方案

Abstract: Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus

</details>


### [159] [Automatic regularization parameter choice for tomography using a double model approach](https://arxiv.org/abs/2602.08528)
*Chuyang Wu,Samuli Siltanen*

Main category: cs.CV

TL;DR: 提出一种基于双网格离散化的X射线断层扫描自动正则化参数选择方法，通过反馈控制算法动态调整正则化强度，确保重建结果在不同网格上的一致性。


<details>
  <summary>Details</summary>
Motivation: X射线断层扫描重建是一个病态逆问题，特别是在数据有限的情况下。正则化至关重要，但其效果依赖于正则化参数的选择，需要在数据保真度和先验信息之间取得平衡。

Method: 使用同一问题的两种不同计算离散化网格，通过反馈控制算法动态调整正则化强度，使迭代重建达到两个网格上重建结果充分相似的最小参数值。

Result: 该方法在真实断层扫描数据上证明了其有效性，能够自动选择适当的正则化参数。

Conclusion: 所提出的双网格方法提供了一种有效的自动参数选择策略，解决了X射线断层扫描重建中正则化参数选择的难题，特别适用于数据有限的情况。

Abstract: Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.

</details>


### [160] [Thegra: Graph-based SLAM for Thermal Imagery](https://arxiv.org/abs/2602.08531)
*Anastasiia Kornilova,Ivan Moskalenko,Arabella Gromova,Gonzalo Ferrer,Alexander Menshchikov*

Main category: cs.CV

TL;DR: 提出了一种基于稀疏单目图SLAM系统，使用在可见光谱数据上训练的SuperPoint检测器和LightGlue匹配器来处理热成像数据，通过预处理管道和置信度加权因子图提升在低纹理热图像中的SLAM性能。


<details>
  <summary>Details</summary>
Motivation: 热成像在视觉退化环境（如低光照、烟雾、恶劣天气）中具有实用价值，但热图像通常具有低纹理、低对比度和高噪声的特点，这使得基于特征的SLAM变得复杂。

Method: 采用在可见光谱大规模数据上训练的SuperPoint特征检测器和LightGlue匹配器；引入预处理管道增强输入适用性；修改核心SLAM模块处理稀疏和异常值特征匹配；将SuperPoint的关键点置信度分数整合到置信度加权因子图中。

Result: 在公开热成像数据集上的评估表明，该系统实现了可靠的性能，无需数据集特定的训练或微调特征检测器。

Conclusion: 该方法利用通用学习特征实现了跨域泛化，为热成像SLAM提供了一种有效的解决方案，特别是在缺乏高质量热成像数据的情况下。

Abstract: Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.

</details>


### [161] [TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation](https://arxiv.org/abs/2602.08540)
*He Wu,Xia Yan,Yanghui Xu,Liegang Xia,Jiazhou Chen*

Main category: cs.CV

TL;DR: 提出TIBR4D框架，通过两阶段迭代边界精化实现动态4D高斯场景的高效无学习对象分割，显著提升边界精度和分割质量


<details>
  <summary>Details</summary>
Motivation: 动态4D高斯场景中的对象级分割面临复杂运动、遮挡和模糊边界的挑战，现有基于阈值的一次性方法难以处理遮挡和保持对象结构完整性

Method: 采用两阶段迭代边界精化：1) IGIT阶段在时间片段级进行迭代高斯实例追踪，逐步优化高斯到实例概率；2) RCC阶段通过抑制边界附近高不确定性高斯实现帧级渲染范围控制，并提出时间分割合并策略平衡身份一致性与动态感知

Result: 在HyperNeRF和Neu3D数据集上实验表明，相比SOTA方法能产生边界更清晰、更准确的对象高斯点云，且效率更高

Conclusion: TIBR4D框架有效解决了4D高斯场景分割中的边界模糊和遮挡问题，为动态场景分析提供了高效可靠的解决方案

Abstract: Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.

</details>


### [162] [GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing](https://arxiv.org/abs/2602.08550)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: GOT-Edit是一种在线跨模态模型编辑方法，通过将几何感知线索集成到通用目标跟踪器中，结合2D语义和3D几何推理来提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 人类感知利用3D知识和语义推理进行有效目标跟踪，而传统通用目标跟踪方法主要依赖2D特征，忽略了3D几何线索，容易受到部分遮挡、干扰物以及几何和外观变化的影响。

Method: 提出GOT-Edit方法，利用预训练的视觉几何基础Transformer从少量2D图像推断几何线索，通过零空间约束更新进行在线模型编辑，无缝结合几何信息和语义判别能力。

Result: 在多个通用目标跟踪基准测试中，GOT-Edit展现出卓越的鲁棒性和准确性，特别是在遮挡和杂乱场景下表现优异。

Conclusion: GOT-Edit为通用目标跟踪建立了结合2D语义和3D几何推理的新范式，显著提升了在复杂场景下的跟踪性能。

Abstract: Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>


### [163] [FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction](https://arxiv.org/abs/2602.08558)
*Guan Yuan Tan,Ngoc Tuan Vu,Arghya Pal,Sailaja Rajanala,Raphael Phan C. -W.,Mettu Srinivas,Chee-Ming Ting*

Main category: cs.CV

TL;DR: FLAG-4D是一个创新的动态场景新视角生成框架，通过双变形网络重建3D高斯原语在时空中的演化过程，解决了现有方法在稀疏视角下难以捕捉复杂点运动和精细动态细节的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用单一MLP建模时间变形，难以一致地捕捉复杂点运动和精细动态细节，特别是在稀疏输入视角下表现不佳。

Method: 采用双变形网络：瞬时变形网络(IDN)建模细粒度局部变形，全局运动网络(GMN)捕捉长程动态，通过相互学习进行精炼。融合预训练光流骨干网络的密集运动特征，使用变形引导注意力机制对齐流信息与3D高斯状态。

Result: 实验表明FLAG-4D相比最先进方法实现了更高保真度、更时间一致的重建结果，具有更好的细节保留能力。

Conclusion: FLAG-4D通过双变形网络架构和运动特征融合，有效解决了动态场景重建中的复杂运动建模和时间一致性挑战，为动态新视角生成提供了更优的解决方案。

Abstract: We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning. To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian. Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.

</details>


### [164] [SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning](https://arxiv.org/abs/2602.08582)
*Melany Yang,Yuhang Yu,Diwang Weng,Jinwei Chen,Wei Dong*

Main category: cs.CV

TL;DR: SemiNFT是一个基于扩散Transformer的智能调色框架，通过模仿人类艺术学习过程（从模仿到创造）实现照片级色彩调整，结合配对学习和强化学习，在标准基准测试和零样本任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 传统手动调色需要专业知识，现有参考图像方法仅进行像素级统计匹配，缺乏语义理解和美学感知能力

Method: 采用Diffusion Transformer架构，分两阶段训练：1）使用配对三元组学习基本结构保持和色彩映射；2）通过强化学习在未配对数据上培养细致美学感知，采用混合在线-离线奖励机制防止灾难性遗忘

Result: 在标准预设迁移基准测试中超越现有最优方法，在黑白照片着色和跨域（动漫到照片）预设迁移等零样本任务中表现出色

Conclusion: SemiNFT超越了简单的统计匹配，实现了高级美学理解水平，为非专家用户提供了专业级的色彩调整能力

Abstract: Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.

</details>


### [165] [Overview and Comparison of AVS Point Cloud Compression Standard](https://arxiv.org/abs/2602.08613)
*Wei Gao,Wenxu Gao,Xingming Mu,Changhao Peng,Ge Li*

Main category: cs.CV

TL;DR: 本文综述了中国AVS工作组开发的第一代点云压缩标准AVS PCC，从技术特点和性能比较两个角度进行分析，重点介绍了该标准采用的新编码工具与技术，以及与MPEG标准(G-PCC和V-PCC)的差异。


<details>
  <summary>Details</summary>
Motivation: 点云数据在沉浸式媒体、自动驾驶、数字遗产保护等领域应用广泛，但大数据量对传输和存储带来挑战。为优化人机感知性能，需要高效的压缩技术。中国AVS工作组开发了与MPEG标准不同的点云压缩标准AVS PCC。

Method: 从两个视角进行综述：1）相关技术分析，详细介绍AVS PCC标准采用的新编码工具和技术；2）性能比较，与MPEG的G-PCC和V-PCC标准进行对比分析。

Result: AVS PCC标准采用了多项创新的编码工具和技术，在技术方案上与现有的MPEG标准存在显著差异，为点云压缩提供了新的解决方案。

Conclusion: AVS PCC作为中国自主制定的点云压缩标准，通过引入新的编码技术，为点云压缩领域提供了多样化的技术选择，有助于推动点云技术在实际应用中的广泛部署。

Abstract: Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.

</details>


### [166] [Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration](https://arxiv.org/abs/2602.08615)
*Kfir Goldberg,Elad Richardson,Yael Vinker*

Main category: cs.CV

TL;DR: Inspiration Seeds是一个无需文本提示的生成框架，通过两幅输入图像生成多样化视觉连贯的组合，支持创意工作早期的视觉构思探索


<details>
  <summary>Details</summary>
Motivation: 传统生成模型主要针对精心设计的文本提示进行优化，缺乏对创意形成前期开放式视觉探索的支持，而设计师通常从松散连接的视觉参考中寻找灵感

Method: 使用CLIP稀疏自编码器提取CLIP潜在空间中的编辑方向并分离概念对，在合成三元组上进行前馈训练，完全通过视觉方式分解视觉方面

Result: 模型能够产生揭示输入图像间潜在关系的多样化视觉连贯组合，不依赖用户指定的文本提示

Conclusion: 该方法通过消除对语言的依赖并支持快速直观的重组合，为创意工作的早期模糊阶段提供了有效的视觉构思支持

Abstract: While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.

</details>


### [167] [Improving Reconstruction of Representation Autoencoder](https://arxiv.org/abs/2602.08620)
*Siyu Liu,Chujie Qin,Hubery Yin,Qixin Yan,Zheng-Peng Duan,Chen Li,Jing Lyu,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: LV-RAE是一个表示自编码器，通过增强语义特征的低级信息实现高保真重建，同时提高生成质量


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型作为图像编码器时缺乏低级信息（颜色、纹理等），导致重建保真度下降，成为潜在扩散模型扩展的主要瓶颈

Method: 提出LV-RAE表示自编码器，通过增强语义特征的低级信息；分析发现高维信息丰富的潜在表示使解码器对扰动敏感，提出通过微调解码器提高鲁棒性和通过受控噪声注入平滑生成潜在表示

Result: 实验证明LV-RAE显著提高了重建保真度，同时保持了语义抽象能力并实现了强生成质量

Conclusion: LV-RAE有效解决了语义特征缺乏低级信息的问题，通过增强鲁棒性和平滑潜在表示，在保持语义分布对齐的同时实现了高质量重建和生成

Abstract: Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.

</details>


### [168] [Revisiting [CLS] and Patch Token Interaction in Vision Transformers](https://arxiv.org/abs/2602.08626)
*Alexis Marouani,Oriane Siméoni,Hervé Jégou,Piotr Bojanowski,Huy V. Vo*

Main category: cs.CV

TL;DR: 该论文提出通过专门化处理路径分离类别标记和图像块标记的计算流程，特别是在归一化层和早期QKV投影中，显著提升了密集预测任务的性能，在分割任务上获得超过2 mIoU的性能提升，同时保持分类精度，仅增加8%参数量且无额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 标准Vision Transformer中类别标记和图像块标记虽然性质不同但被统一处理，研究发现标准归一化层在这两类标记间引入了隐式区分，阻碍了全局和局部特征的协同学习。

Method: 提出专门化处理路径，选择性地分离类别标记和图像块标记的计算流程，特别针对归一化层和早期查询-键-值投影进行改进，实现两类标记的显式差异化处理。

Result: 在标准基准测试中，分割性能提升超过2 mIoU点，同时保持强大的分类准确性；仅增加8%的参数，无额外计算开销；通过消融研究验证了架构组件专门化的有效性。

Conclusion: 针对Vision Transformer中不同类型标记的专门化处理能够显著提升模型性能，特别是在密集预测任务中，这种改进具有参数效率高、计算开销小的优点，且适用于不同模型规模和训练框架。

Abstract: Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.

</details>


### [169] [Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology](https://arxiv.org/abs/2602.08652)
*Oskar Thaeter,Tanja Niedermair,Johannes Raffler,Ralf Huss,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 深度学习模型使用低分辨率缩略图预测病理切片固定类型，在TCGA数据集上AUROC达0.88，比现有方法快400倍，实现高通量质量控制。


<details>
  <summary>Details</summary>
Motivation: 病理切片固定类型标注对下游分析和诊断准确性至关重要，但人工标注易出错。现有方法需要高分辨率全玻片图像，限制了高通量质量控制的扩展性。

Method: 提出基于深度学习的模型，使用低分辨率预扫描缩略图预测FFPE和冷冻切片固定类型。在TUM研究所的1200张WSI上训练，在TCGA、Augsburg和Regensburg数据集上进行评估。

Result: 在TCGA数据集上AUROC为0.88，比可比方法提升4.8%；在Regensburg和Augsburg数据集上AUROC为0.72。处理每张切片仅需21毫秒，比高倍率全分辨率方法快400倍。

Conclusion: 该方法为高通量病理工作流程提供了不依赖高倍率扫描的有效质量控制工具，未来将扩展到更多扫描仪类型和其他低分辨率标注任务。

Abstract: Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to
  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen
  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.
  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from
  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,
  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).
  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and
  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\times$
  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.
  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for
  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner
  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other
  low-resolution slide annotations.

</details>


### [170] [WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling](https://arxiv.org/abs/2602.08661)
*Yi Dao,Lankai Zhang,Hao Liu,Haiwei Zhang,Wenbo Wang*

Main category: cs.CV

TL;DR: WiFlow：基于WiFi信号的连续人体姿态估计新框架，采用编码器-解码器架构处理CSI数据，在保持信号时序结构的同时降低计算复杂度，在PCK指标上达到97.00%@20和99.48%@50的优异性能


<details>
  <summary>Details</summary>
Motivation: 解决传统WiFi姿态估计方法在连续运动场景下的性能限制和高计算开销问题，为物联网智能感知应用提供更实用的解决方案

Method: 使用编码器-解码器架构：编码器采用时间和非对称卷积捕捉CSI时空特征，通过轴向注意力机制精炼关键点特征；解码器将高维特征映射为关键点坐标

Result: 在包含360,000个样本的自建数据集上，PCK@20达到97.00%，PCK@50达到99.48%，平均关节位置误差0.008米，仅需4.82M参数

Conclusion: WiFlow在显著降低模型复杂度和计算成本的同时，为基于WiFi的实用人体姿态估计建立了新的性能基准，代码和数据集已开源

Abstract: Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.

</details>


### [171] [A Machine Learning accelerated geophysical fluid solver](https://arxiv.org/abs/2602.08670)
*Yang Bai*

Main category: cs.CV

TL;DR: 该论文研究如何将机器学习应用于具有数学约束的偏微分方程求解领域，特别是通过数据驱动的离散化方法改进结构化网格上的PDE求解器，在浅水方程和欧拉方程上实现了比传统Pyclaw求解器性能更好的经典求解器，并提出了四种深度神经网络方法，其中两种能够输出满意的解。


<details>
  <summary>Details</summary>
Motivation: 机器学习在图像分类和自然语言处理等领域已取得成功，但在具有数学约束的偏微分方程求解领域的应用仍有待探索。数据驱动的离散化方法为加速和改进现有PDE求解器提供了有前景的途径。

Method: 实现了浅水方程和欧拉方程的经典求解器，并在不同框架下进行比较。提出了四种深度神经网络方法用于ML-based求解器，这些网络预测准线性模板的系数来计算函数在给定位置的值或导数。

Result: 实验表明，经典求解器性能显著优于Pyclaw求解器。四种深度神经网络方法中有两种能够输出满意的解，证明了ML方法在PDE求解中的有效性。

Conclusion: 数据驱动的离散化方法能够提高低分辨率模拟的精度和稳定性，同时受益于传统数值格式（如通过有限体积型公式实现守恒律），为机器学习在偏微分方程求解领域的应用提供了有效途径。

Abstract: Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.

</details>


### [172] [ALIVE: Animate Your World with Lifelike Audio-Video Generation](https://arxiv.org/abs/2602.08682)
*Ying Guo,Qijun Gan,Yifu Zhang,Jinlai Liu,Yifei Hu,Pan Xie,Dongjun Qian,Yu Zhang,Ruiqi Li,Yuqi Zhang,Ruibiao Lu,Xiaofeng Mei,Bo Han,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: ALIVE是一个基于预训练文本到视频模型的音频-视频生成模型，通过联合音频-视频分支架构实现Sora风格的音视频生成和动画功能，在百万级高质量数据上训练后性能超越开源模型并媲美商业解决方案。


<details>
  <summary>Details</summary>
Motivation: 视频生成技术正朝着统一的音频-视频生成方向发展，现有文本到视频基础模型缺乏音频生成和参考动画能力，需要开发能够同时处理音视频同步和动画生成的新模型。

Method: 采用MMDiT架构增强，添加联合音频-视频分支，包括时间对齐跨模态融合(TA-CrossAttn)和统一时间RoPE(UniTemp-RoPE)实现精确音视频对齐；设计包含音频-视频字幕和质量控制的综合数据流水线收集高质量微调数据。

Result: 模型在百万级高质量数据上持续预训练和微调后表现出色，在音频-视频生成和动画任务上持续超越开源模型，性能匹配或超越最先进的商业解决方案。

Conclusion: ALIVE为社区开发音频-视频生成模型提供了详细的实现方案和基准测试，有望帮助更高效地发展音视频生成技术。

Abstract: Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.

</details>


### [173] [OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence](https://arxiv.org/abs/2602.08683)
*Feilong Tang,Xiang An,Yunyao Yan,Yin Xie,Bin Qin,Kaicheng Yang,Yifei Shen,Yuanhan Zhang,Chunyuan Li,Shikun Feng,Changrui Chen,Huajie Tan,Ming Hu,Manyuan Zhang,Bo Li,Ziyong Feng,Ziwei Liu,Zongyuan Ge,Jiankang Deng*

Main category: cs.CV

TL;DR: OneVision-Encoder (OV-Encoder) is a novel视觉编码器，通过采用编解码器对齐的稀疏处理方法，专注于视频中信息熵丰富的区域（3.1%-25%），实现了效率与精度的正相关提升，在多项视觉理解任务中超越现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉架构存在计算浪费问题，均匀处理冗余的像素网格而非专注于包含预测残差的稀疏信息区域。研究认为视觉通用智能本质上是压缩问题，需要架构与视频信息论原理对齐。

Method: 采用Codec Patchification技术，专注于信号熵丰富的稀疏区域；使用共享3D RoPE统一空间和时间推理；通过大规模聚类判别目标在百万级语义概念上进行训练，联合捕捉物体持久性和运动动态。

Result: 在16个图像、视频和文档理解基准测试中 consistently 超越Qwen3-ViT和SigLIP2等强基线模型，视频理解任务平均提升4.1%，同时使用更少的视觉token和预训练数据。

Conclusion: 编解码器对齐的补丁级稀疏性是基础原则，使OV-Encoder成为下一代视觉通用智能的可扩展引擎，证明了效率与精度并非权衡关系而是正相关。

Abstract: Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.
  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.
  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.

</details>


### [174] [Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm](https://arxiv.org/abs/2602.08699)
*Xiaogang Xu,Kun Zhou,Tao Hu,Jiafei Wu,Ruixing Wang,Hao Peng,Bei Yu*

Main category: cs.CV

TL;DR: VLLVE++：一种基于视频分解的低光视频增强框架，通过视角无关和视角相关分量分解，结合残差项和双向学习机制，有效处理动态场景和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 解决低光视频中严重的不可见性和噪声问题，传统方法难以处理动态场景和复杂退化情况。

Method: 提出视图感知视频分解策略：1) 视角无关分量捕捉内在外观；2) 视角相关分量描述光照条件；3) 引入残差项模拟场景自适应退化；4) 双结构增强网络和跨帧交互机制确保分解一致性；5) 双向学习实现增强和退化感知对应关系优化。

Result: 在广泛认可的低光视频增强基准测试中进行了大量实验，证明VLLVE++在处理真实世界场景和高动态视频方面表现出色。

Conclusion: VLLVE++通过创新的视频分解策略和残差建模，显著提升了低光视频增强的性能，特别是在处理挑战性场景方面表现出强大能力。

Abstract: Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE). We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++. This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones. Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.

</details>


### [175] [TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions](https://arxiv.org/abs/2602.08711)
*Linli Yao,Yuancheng Wei,Yaojie Zhang,Lei Li,Xinlong Chen,Feifan Song,Ziyue Wang,Kun Ouyang,Yuanxin Liu,Lingpeng Kong,Qi Liu,Pengfei Wan,Kun Gai,Yuanxing Zhang,Xu Sun*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create "script-like" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.

</details>


### [176] [Towards Understanding Multimodal Fine-Tuning: Spatial Features](https://arxiv.org/abs/2602.08713)
*Lachin Naghashyar,Hunar Batra,Ashkan Khakzar,Philip Torr,Ronald Clark,Christian Schroeder de Witt,Constantin Venhoff*

Main category: cs.CV

TL;DR: 该论文通过阶段性模型差异分析技术，首次对视觉语言模型的多模态微调过程进行机制性分析，揭示了语言模型如何学习视觉能力，识别了空间关系编码特征及其在注意力头中的因果激活机制。


<details>
  <summary>Details</summary>
Motivation: 尽管现代视觉语言模型在多种任务上表现优异，但语言主干网络在多模态训练中的适应过程以及视觉特定能力如何产生仍不清楚，需要机制性分析来理解这一过程。

Method: 采用阶段性模型差异分析技术，该技术能够隔离多模态微调过程中引入的表征变化，通过控制空间提示的移位来识别视觉偏好特征和空间关系编码特征。

Result: 研究发现：1）识别了在微调过程中出现或重新定向的视觉偏好特征；2）这些特征的一个选择性子集可靠地编码空间关系；3）这些特征的因果激活可追溯到一小群注意力头。

Conclusion: 阶段性模型差异分析能够揭示时空基础多模态特征的出现时机和位置，为理解预训练语言模型如何获得视觉基础能力提供了方法论基础，增强了多模态训练的可解释性。

Abstract: Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to "see". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.

</details>


### [177] [Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images](https://arxiv.org/abs/2602.08717)
*Farnaz Khun Jush,Grit Werner,Mark Klemens,Matthias Lenga*

Main category: cs.CV

TL;DR: 本文研究了在零样本条件下使用预训练基础模型进行CT和MR影像解剖区域检测的方法，提出了三种无需训练的方法流程，其中基于分割的规则系统在887个异质扫描上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有解剖区域识别方法严重依赖不可靠的DICOM元数据且主要使用监督学习，限制了在现实场景中的适用性，因此需要探索零样本解决方案。

Method: 提出了三种无需训练的流程：(1)基于预训练多器官分割模型的规则系统，(2)放射科医生规则指导的多模态大语言模型，(3)结合视觉输入和解剖证据的分割感知MLLM。

Result: 基于分割的规则方法表现最佳，加权F1分数CT为0.947，MR为0.914；MLLM在视觉特征明显区域表现良好；分割感知MLLM存在根本性限制。

Conclusion: 预训练分割模型驱动的规则系统在零样本解剖区域检测中展现出强大且一致的性能，为医疗影像自动化工作流程提供了可靠的解决方案。

Abstract: Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.

</details>


### [178] [Rotated Lights for Consistent and Efficient 2D Gaussians Inverse Rendering](https://arxiv.org/abs/2602.08724)
*Geng Lin,Matthias Zwicker*

Main category: cs.CV

TL;DR: RotLight：通过简单旋转拍摄对象来解决逆渲染中的材质反照率估计模糊性问题，结合代理网格实现精确光照追踪和全局光照处理


<details>
  <summary>Details</summary>
Motivation: 传统逆渲染方法在材质和光照分解中存在高度模糊性，导致反照率估计出现颜色不准确和烘焙阴影问题，尽管有正则化处理但仍存在缺陷

Method: 提出RotLight简单拍摄设置（物体多次旋转）+ 基于2DGS的代理网格方法，实现精确入射光追踪、残差约束和改进的全局光照处理

Result: 在合成和真实数据集上验证，仅需两次旋转即可有效减少伪影，实现优异的反照率估计并保持计算效率

Conclusion: RotLight通过简单旋转捕捉设置成功解决了逆渲染中的模糊性问题，结合代理网格技术显著提升了材质分解的准确性和渲染质量

Abstract: Inverse rendering aims to decompose a scene into its geometry, material properties and light conditions under a certain rendering model. It has wide applications like view synthesis, relighting, and scene editing. In recent years, inverse rendering methods have been inspired by view synthesis approaches like neural radiance fields and Gaussian splatting, which are capable of efficiently decomposing a scene into its geometry and radiance. They then further estimate the material and lighting that lead to the observed scene radiance. However, the latter step is highly ambiguous and prior works suffer from inaccurate color and baked shadows in their albedo estimation albeit their regularization. To this end, we propose RotLight, a simple capturing setup, to address the ambiguity. Compared to a usual capture, RotLight only requires the object to be rotated several times during the process. We show that as few as two rotations is effective in reducing artifacts. To further improve 2DGS-based inverse rendering, we additionally introduce a proxy mesh that not only allows accurate incident light tracing, but also enables a residual constraint and improves global illumination handling. We demonstrate with both synthetic and real world datasets that our method achieves superior albedo estimation while keeping efficient computation.

</details>


### [179] [FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing](https://arxiv.org/abs/2602.08725)
*Yongwen Lai,Chaoqun Wang,Shaobo Min*

Main category: cs.CV

TL;DR: FusionEdit是一个无需训练的文本引导图像编辑框架，通过语义差异自动识别编辑区域，采用距离感知潜在融合和总变差损失减少边界伪影，利用AdaIN调制实现统计注意力融合，在保持源图像一致性的同时提升编辑精确度。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用显式二值掩码进行约束编辑，但硬掩码边界会引入伪影并降低可编辑性，需要解决这些边界问题以实现更精确自然的图像编辑。

Method: 1. 通过测量源提示和目标提示之间的语义差异自动识别编辑和保留区域；2. 沿区域边界进行距离感知潜在融合生成软掩码，使用总变差损失确保平滑过渡；3. 在DiT注意力层中采用AdaIN调制进行统计注意力融合。

Result: 大量实验表明FusionEdit在性能上显著优于现有最先进方法，能够实现精确可控的编辑效果。

Conclusion: FusionEdit通过软掩码生成和统计注意力融合技术，有效解决了边界伪影问题，提升了文本引导图像编辑的精确性和自然度，是一个无需训练的高效编辑框架。

Abstract: Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.

</details>


### [180] [SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training](https://arxiv.org/abs/2602.08726)
*Khadija Iddrisu,Waseem Shariff,Suzanne Little,Noel OConnor*

Main category: cs.CV

TL;DR: 提出基于Blender生成的合成眼动数据集SynSacc，使用脉冲神经网络(SNN)进行眼动分类，在合成和真实事件数据上达到0.83准确率，并展示出优于人工神经网络的计算效率


<details>
  <summary>Details</summary>
Motivation: 传统帧相机存在运动模糊问题，事件相机(DVS)具有异步记录和高时间分辨率优势，但缺乏高质量眼动数据集。需要开发合成数据方法来提升事件相机在眼动分类中的性能

Method: 使用Blender生成模拟扫视和注视的合成数据集，训练两种SNN架构并在真实事件数据上进行微调，评估模型在不同时间分辨率下的鲁棒性

Result: 模型达到0.83分类准确率，在不同时间分辨率下保持稳定性能，SNN相比ANN获得显著计算效率提升

Conclusion: 合成数据增强在事件视觉中具有实用价值，SNN结合合成事件流为眼动分类提供了高效准确的解决方案，数据集和代码已开源

Abstract: The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.

</details>


### [181] [Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework](https://arxiv.org/abs/2602.08727)
*Johannes Thalhammer,Tina Dorosti,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

TL;DR: 提出一种计算高效的混合深度学习框架，结合2D和3D模型优势，通过2D U-Net提取切片特征，再由3D解码器利用跨切片上下文信息重建无伪影3D CT体积


<details>
  <summary>Details</summary>
Motivation: 欠采样CT体积可减少采集时间和辐射暴露，但会引入伪影降低图像质量和诊断价值，需要高效方法减少这些伪影

Method: 两阶段方法：首先使用2D U-Net处理单个切片提取特征图，然后将切片特征图堆叠作为输入，通过3D解码器利用体积上下文信息预测无伪影3D CT体积

Result: 在冠状面和矢状面方向上显著改善了切片间一致性，计算开销低

Conclusion: 该混合框架为高质量3D CT图像后处理提供了稳健且高效的解决方案

Abstract: Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.

</details>


### [182] [Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation](https://arxiv.org/abs/2602.08730)
*Shanshan Wang,Ziying Feng,Xiaozheng Shen,Xun Yang,Pichao Wang,Zhenwei He,Xingyi Zhang*

Main category: cs.CV

TL;DR: CLIP-Guided Alignment (CGA) 是一个新的无源域自适应框架，通过检测类别混淆对、构建混淆感知文本提示和特征对齐，有效解决了细粒度场景中源模型对视觉相似类别的不对称动态混淆问题。


<details>
  <summary>Details</summary>
Motivation: 解决无源域自适应中存在的类别不对称动态混淆问题，特别是在细粒度场景中，由于类别间视觉相似性导致的伪标签噪声和目标判别能力下降。

Method: 提出三部分方法：MCA检测定向混淆对，MCC利用CLIP构建混淆感知文本提示实现上下文敏感的伪标签，FAM通过对比学习对齐CLIP和源模型的特征表示以减少表征空间模糊性。

Result: 在多个数据集上的实验表明，CGA在混淆易发和细粒度场景中显著优于现有最先进的无源域自适应方法。

Conclusion: 显式建模类别间混淆对于有效的无源域自适应至关重要，CLIP的视觉-语言能力为解决类别混淆问题提供了有效途径。

Abstract: Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA

</details>


### [183] [From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2602.08735)
*Masanari Oi,Koki Maeda,Ryuto Koike,Daisuke Oba,Nakamasa Inoue,Naoaki Okazaki*

Main category: cs.CV

TL;DR: HATCH是一个针对多模态大语言模型的多图像空间推理训练框架，通过显式的跨视角对应和逐步视角变换机制，提升模型在多视角空间推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在单图像空间推理方面取得进展，但在需要整合多个视角信息的多图像空间推理任务中仍面临挑战。人类通过跨视角对应和逐步视角变换两种机制解决此类任务，但现有研究仅部分且隐式地整合这些机制。

Method: 提出HATCH训练框架，包含两个互补目标：(1)补丁级空间对齐：鼓励不同视角中空间对应区域的补丁表示对齐；(2)先行动后回答推理：要求模型在预测最终答案前生成显式的视角转换动作。

Result: 在三个基准测试中，HATCH始终以明显优势超越同等规模的基线模型，并与更大规模模型取得竞争性结果，同时保持单图像推理能力。

Conclusion: HATCH通过显式监督跨视角对应和视角变换机制，有效提升了多模态大语言模型在多图像空间推理任务中的性能，证明了人类认知机制在模型设计中的重要性。

Abstract: While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.

</details>


### [184] [Shifting the Breaking Point of Flow Matching for Multi-Instance Editing](https://arxiv.org/abs/2602.08749)
*Carmine Zaccagnino,Fabio Quattrini,Enis Simsar,Marta Tintoré Gazulla,Rita Cucchiara,Alessio Tonioni,Silvia Cascianelli*

Main category: cs.CV

TL;DR: 提出Instance-Disentangled Attention机制，解决流匹配模型在多实例编辑中语义干扰问题，实现单次处理的多区域独立编辑


<details>
  <summary>Details</summary>
Motivation: 现有流基编辑器主要支持全局或单指令编辑，难以处理多实例场景中多个部分需要独立编辑且避免语义干扰的问题

Method: 引入实例解耦注意力机制，分割联合注意力操作，在速度场估计中强制绑定实例特定文本指令与空间区域

Result: 在自然图像编辑和文本密集信息图编辑任务上验证，方法促进编辑解耦和局部性，同时保持全局输出一致性

Conclusion: 提出的方法有效解决了多实例编辑中的语义纠缠问题，实现了单次处理的实例级编辑能力

Abstract: Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.

</details>


### [185] [MVAnimate: Enhancing Character Animation with Multi-View Optimization](https://arxiv.org/abs/2602.08753)
*Tianyu Sun,Zhoujie Fu,Bang Zhang,Guosheng Lin*

Main category: cs.CV

TL;DR: MVAnimate是一个新颖的动画生成框架，利用多视角先验信息合成动态人物的2D和3D信息，提升生成视频质量，解决现有方法输出质量低和训练数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于2D或3D人体姿态建模的动画生成算法面临输出质量低和训练数据不足的问题，无法生成高质量动画视频，需要新的解决方案。

Method: 引入MVAnimate框架，基于多视角先验信息合成动态人物的2D和3D信息，利用多视角先验产生时间一致和空间连贯的动画输出，并优化目标角色的多视角视频质量。

Result: 实验结果表明该方法在不同数据集上表现出鲁棒性，能够处理各种运动模式和外观，相比现有动画方法有显著改进。

Conclusion: MVAnimate通过多视角信息融合有效提升了动画生成质量，为高质量角色动画生成提供了新的技术路径。

Abstract: The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.

</details>


### [186] [VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars](https://arxiv.org/abs/2602.08775)
*Vineet Kumar Rakesh,Ahana Bhattacharjee,Soumya Mazumdar,Tapas Samanta,Hemendra Kumar Pandey,Amitabha Das,Sarbajit Pal*

Main category: cs.CV

TL;DR: 提出了一种基于符号计算和吠陀数学的轻量级说话头像生成框架，可在CPU上实现实时合成，适用于资源受限的教育环境。


<details>
  <summary>Details</summary>
Motivation: 现有说话头像生成方法依赖GPU渲染、大量训练数据或高容量扩散模型，难以在离线或资源受限的学习环境中部署。

Method: 采用符号吠陀计算方法，将语音转换为时间对齐的音素流，映射到紧凑的视素库，通过Urdhva Tiryakbhyam吠陀经启发的符号协同发音生成平滑视素轨迹，使用轻量级2D渲染器进行ROI变形和嘴部合成。

Result: 在仅CPU执行下实现了同步精度、时间稳定性和身份一致性，相比其他CPU可行基线显著降低了计算负载和延迟。

Conclusion: 该方法可在低端硬件上实现可接受的唇同步质量，支持实用的教育用说话头像部署。

Abstract: Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg

</details>


### [187] [Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems](https://arxiv.org/abs/2602.08792)
*Hao Dong,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: 提出多模态深度学习框架MultiDeepSAD，结合高分辨率图像和力测量数据，用于铁路受电弓-接触网接口的电弧检测，通过合成数据增强和新的损失函数显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 受电弓-接触网接口的电弧现象对铁路供电系统造成严重风险，但由于瞬态特性、噪声环境、数据稀缺以及与其他瞬态现象难以区分等挑战，传统检测方法效果有限。

Method: 构建包含视觉和力测量数据的双模态数据集；提出MultiDeepSAD多模态异常检测算法；针对图像和力数据分别设计伪异常生成技术进行数据增强；使用新的损失函数进行训练。

Result: 实验表明该框架显著优于基线方法，在域迁移和真实电弧观测数据有限的情况下仍能保持高灵敏度。

Conclusion: 多模态融合方法能有效解决电弧检测的挑战，合成数据增强技术可缓解数据稀缺问题，为铁路系统安全监控提供了更可靠的解决方案。

Abstract: The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.

</details>


### [188] [MOVA: Towards Scalable and Synchronized Video-Audio Generation](https://arxiv.org/abs/2602.08794)
*SII-OpenMOSS Team,:,Donghua Yu,Mingshu Chen,Qi Chen,Qi Luo,Qianyi Wu,Qinyuan Cheng,Ruixiao Li,Tianyi Liang,Wenbo Zhang,Wenming Tu,Xiangyu Peng,Yang Gao,Yanru Huo,Ying Zhu,Yinze Luo,Yiyang Zhang,Yuerong Song,Zhe Xu,Zhiyu Zhang,Chenchen Yang,Cheng Chang,Chushu Zhou,Hanfu Chen,Hongnan Ma,Jiaxi Li,Jingqi Tong,Junxi Liu,Ke Chen,Shimin Li,Songlin Wang,Wei Jiang,Zhaoye Fei,Zhiyuan Ning,Chunguo Li,Chenhui Li,Ziwei He,Zengfeng Huang,Xie Chen,Xipeng Qiu*

Main category: cs.CV

TL;DR: MOVA是一个开源的32B参数音频-视频生成模型，采用混合专家架构，支持图像-文本到视频-音频的联合生成，能够产生高质量的同步音频内容（包括唇语同步、环境音效和音乐）。


<details>
  <summary>Details</summary>
Motivation: 当前音频-视频内容生成主要依赖级联流水线，导致成本高、错误累积和质量下降。现有系统多为闭源，限制了领域发展。需要开发能够同时生成高质量同步音频视频的开源解决方案。

Method: 采用混合专家（MoE）架构，总参数量32B，推理时激活18B参数。支持IT2VA（图像-文本到视频-音频）生成任务。

Result: 开发出能够生成高质量同步音频视频内容的模型，包括逼真的唇语同步、环境感知音效和内容对齐的音乐生成。

Conclusion: MOVA作为开源模型，通过释放模型权重和代码，旨在推动音频-视频联合生成研究，促进创作者社区发展，并提供高效推理、LoRA微调和提示增强等全面支持。

Abstract: Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.

</details>


### [189] [Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework](https://arxiv.org/abs/2602.08797)
*Jiaming Liu,Cheng Ding,Daoqiang Zhang*

Main category: cs.CV

TL;DR: 提出一种半监督师生框架，结合不确定性感知伪标签生成和基于置信度的渐进课程学习，在有限标注数据下实现高效的脑肿瘤MRI分割


<details>
  <summary>Details</summary>
Motivation: 解决MRI脑肿瘤分割中标注成本高昂和数据异质性（不同扫描仪和站点）带来的挑战，通过半监督学习减少对大量标注数据的依赖

Method: 使用教师模型生成概率掩码和逐像素不确定性；基于图像级置信度对未标注扫描进行排序并分阶段引入；采用双重损失目标训练学生模型学习高置信区域并遗忘低置信区域；通过一致性优化进一步提升伪标签质量

Result: 在BraTS 2021数据集上，验证集DSC从10%数据时的0.393提升到100%数据时的0.872；教师模型达到0.922 DSC，学生模型在肿瘤子区域（NCR/NET 0.797，水肿0.980）超越教师，特别是在增强类别（DSC 0.620）上表现优异

Conclusion: 置信度驱动的课程学习和选择性遗忘机制能够在有限监督和噪声伪标签条件下提供鲁棒的分割性能，展示了该方法在数据效率和模型性能方面的优势

Abstract: Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.

</details>


### [190] [Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing](https://arxiv.org/abs/2602.08820)
*Hao Yang,Zhiyu Tan,Jia Gong,Luozheng Qin,Hesen Chen,Xiaomeng Yang,Yuqing Sun,Yuetan Lin,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: Omni-Video 2是一个可扩展的高效视频生成编辑模型，通过连接多模态大语言模型和视频扩散模型，利用MLLM的理解推理能力生成显式目标描述来指导视频生成过程，实现高质量文本到视频生成和复杂编辑任务。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂组合式视频编辑中指令理解与生成过程的有效衔接问题，充分利用预训练多模态模型的理解能力和视频扩散模型的生成先验，实现高性能的视频生成与编辑。

Method: 1. 利用MLLM的理解推理能力生成显式目标描述解释用户指令；2. 开发轻量级适配器将多模态条件令牌注入预训练文本到视频扩散模型；3. 在精心策划的训练数据上扩展到14B参数规模。

Result: 在FiVE细粒度视频编辑基准和VBench文本到视频生成基准上表现优异，展现出在复杂组合指令跟随方面的卓越能力，在视频生成任务中达到竞争性或更优的质量。

Conclusion: Omni-Video 2通过有效整合MLLM的理解能力和视频扩散模型的生成能力，实现了参数高效的高质量视频生成和复杂编辑，为统一视频生成编辑系统提供了可行的技术路径。

Abstract: We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.

</details>


### [191] [Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications](https://arxiv.org/abs/2602.08822)
*Yao Pu,Yiming Shi,Zhenxi Zhang,Peixin Yu,Yitao Zhuang,Xiang Wang,Hongzhao Chen,Jing Cai,Ge Ren*

Main category: cs.CV

TL;DR: 开发了一个统一的基础模型，通过对比视觉表示学习和视觉语言对齐技术，实现任意到全的MRI合成，解决鼻咽癌放疗中MRI模态不全的问题，在多个验证站点表现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 鼻咽癌放疗中MRI检查存在患者不适、扫描时间长、成本高等实际问题，导致临床实践中模态不全，影响放疗计划准确性。传统MRI合成方法缺乏解剖适应性和临床可解释性，无法满足放疗需求。

Method: 采用对比编码器获取模态不变表示，结合CLIP基础的文本信息解码器实现语义一致的合成，通过一个统一的基础模型支持任意到全的MRI合成。模型在13个机构的40,825张图像上进行训练。

Result: 在26个内外验证站点（15,748张图像）上获得一致的高性能（平均SSIM 0.90，PSNR 27），具有优异的合成保真度以及对噪声和域转移的鲁棒性。统一表示还增强了放疗相关下游任务（如分割）的性能。

Conclusion: 该工作通过利用基础模型弥合技术合成与临床实用性之间的差距，推进了鼻咽癌护理的数字医学解决方案，为临床实践提供了有效的MRI合成工具。

Abstract: Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.

</details>


### [192] [VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning](https://arxiv.org/abs/2602.08828)
*Hao Tan,Jun Lan,Senyuan Shi,Zichang Tan,Zijian Yu,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: VideoVeritas框架通过联合偏好对齐和感知前置强化学习，结合细粒度感知与事实推理，有效检测AI生成视频，并在新构建的MintVid数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 视频生成技术快速发展带来安全风险，现有多模态大语言模型虽具备强推理能力，但细粒度感知能力有限，需要更可靠的检测方法。

Method: 提出Joint Preference Alignment和Perception Pretext Reinforcement Learning (PPRL)，通过时空定位和自监督目标计数等感知前置任务增强检测性能，而非直接优化检测任务。

Result: 实验结果表明，现有方法偏向表面推理或机械分析，而VideoVeritas在多样化基准测试中实现了更平衡的性能表现。

Conclusion: VideoVeritas框架通过强化感知能力与推理能力的结合，为AI生成视频检测提供了更有效的解决方案，并通过MintVid数据集促进了该领域的稳健评估。

Abstract: The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.

</details>


### [193] [FlattenGPT: Depth Compression for Transformer with Layer Flattening](https://arxiv.org/abs/2602.08858)
*Ruihan Xu,Qingpei Guo,Yao Zhu,Xiangyang Ji,Ming Yang,Shiliang Zhang*

Main category: cs.CV

TL;DR: FlattenGPT是一种新颖的Transformer深度压缩方法，通过将相邻块展平为一个块来减少网络深度，同时保持原始架构和学到的知识，在性能和效率之间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有整块剪枝方法会丢弃重要信息导致性能下降，通道剪枝无法减少模型深度且存在层间剪枝比例不一致问题，需要更好的深度压缩方案。

Method: 提出FlattenGPT方法，检测并展平相邻的Transformer块，压缩网络深度同时更有效地检测和移除参数冗余，保持原始架构一致性。

Result: 在LLaMA-2/3和Qwen-1.5等模型上，FlattenGPT以20%压缩比保留90-96%的零样本性能，在零样本准确率和WikiText-2困惑度上优于现有剪枝方法，推理加速效果显著。

Conclusion: FlattenGPT通过展平相邻块的方式有效减少了Transformer的深度冗余，在保持性能的同时显著提升模型效率，为Transformer效率优化提供了有前景的解决方案。

Abstract: Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\% of zero-shot performance with a compression ratio of 20\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.

</details>


### [194] [TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models](https://arxiv.org/abs/2602.08861)
*Xiangtian Zheng,Zishuo Wang,Yuxin Peng*

Main category: cs.CV

TL;DR: TiFRe框架通过文本引导的帧采样和帧匹配融合机制，在减少视频输入帧数的同时保持关键信息，有效降低计算成本并提升视频多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 视频多模态大语言模型在处理大量视频帧时面临高计算成本问题，简单的固定帧率关键帧选择方法会导致重要信息丢失和性能下降。

Method: 提出文本引导视频帧减少框架(TiFRe)，包含：1)文本引导帧采样策略(TFS)，利用LLM生成CLIP提示词并根据语义相似度选择关键帧；2)帧匹配融合机制(FMM)，将非关键帧信息整合到关键帧中减少信息损失。

Result: 实验表明TiFRe能有效降低计算成本，同时在视频语言任务上提升性能表现。

Conclusion: TiFRe通过智能帧选择和语义保留机制，为视频MLLMs提供了一种高效的计算成本优化方案，在保持性能的同时显著减少计算开销。

Abstract: With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.

</details>


### [195] [Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit](https://arxiv.org/abs/2602.08909)
*Zhendong Wang,Cihan Ruan,Jingchuan Xiao,Chuqing Shi,Wei Jiang,Wei Wang,Wenjie Liu,Nam Ling*

Main category: cs.CV

TL;DR: 该论文分析了3D高斯泼溅(3DGS)优化中出现的Rendering-Optimal References (RORs)结构特性，揭示了尺度混合结构和辐射双模态的稳定模式，并发现了密度分层现象：密集区域参数可预测，稀疏区域需要多视角约束。


<details>
  <summary>Details</summary>
Motivation: 研究标准多视角优化下3D高斯泼溅解的结构特征，理解参数决定的机制，为改进训练鲁棒性和架构设计提供理论基础。

Method: 通过可学习性探针训练预测器从点云重建RORs，应用方差分解分析几何与外观参数的耦合关系，提出密度感知策略。

Result: 发现密度分层现象：密集区域参数与几何相关可预测，稀疏区域因可见性异质性导致参数耦合而预测失败。RORs具有双重特性：几何基元和多视角合成基元。

Conclusion: RORs具有双重特性，需要密度感知策略来平衡前馈预测和基于渲染的细化，这对自适应系统的架构设计具有重要意义。

Abstract: We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.

</details>


### [196] [Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields](https://arxiv.org/abs/2602.08958)
*Weihan Luo,Lily Goli,Sherwin Bahmani,Felix Taubner,Andrea Tagliasacchi,David B. Lindell*

Main category: cs.CV

TL;DR: 提出一种3D高斯流场表示方法，用于建模植物生长过程中的时变3D外观，通过反向生长模拟初始化高斯基元，在植物生长时序数据集上实现了优于现有方法的图像质量和几何精度。


<details>
  <summary>Details</summary>
Motivation: 植物生长过程中会产生新的几何结构，现有动态建模技术（如变形场和4D高斯溅射）无法有效处理几何生成和非线性生长动态的问题。

Method: 引入3D高斯流场表示，将植物生长建模为高斯参数（位置、尺度、方向、颜色、不透明度）的时变导数；通过重建成熟植物并学习反向生长过程来初始化高斯基元。

Result: 在多视角植物生长时序数据集上，该方法在图像质量和几何精度方面均优于现有方法。

Conclusion: 该方法为生长中的3D结构的外观建模提供了一种新方法，能够有效处理植物生长过程中的非线性连续时间动态和几何生成问题。

Abstract: Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.

</details>


### [197] [MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE](https://arxiv.org/abs/2602.08961)
*Ruijie Zhu,Jiahao Lu,Wenbo Hu,Xiaoguang Han,Jianfei Cai,Ying Shan,Chuanxia Zheng*

Main category: cs.CV

TL;DR: MotionCrafter是一个基于视频扩散的框架，能够从单目视频中联合重建4D几何并估计密集运动，无需后优化即可实现最先进的性能


<details>
  <summary>Details</summary>
Motivation: 现有方法强制3D值和潜在空间与RGB VAE潜在空间严格对齐，尽管它们具有根本不同的分布，这导致了次优性能

Method: 提出新颖的密集3D点图和3D场景流联合表示方法，以及新的4D VAE来有效学习这种表示，采用新的数据归一化和VAE训练策略

Result: 在多个数据集上实现了最先进的性能，几何重建和运动重建分别提升了38.64%和25.0%

Conclusion: 证明了与RGB VAE潜在空间的对齐是不必要的，新方法能更好地传递扩散先验并显著提高重建质量

Abstract: We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>


### [198] [Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting](https://arxiv.org/abs/2602.08962)
*Guangxun Zhu,Xuan Liu,Nicolas Pugeault,Chongfeng Wei,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: 提出了一个基于3D车辆条件约束的行人姿态预测框架，通过显式整合周围车辆信息来提升行人运动预测精度，在Waymo-3DSkelMo数据集上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在复杂城市环境中准确预测行人运动对于自动驾驶的安全性和可靠性至关重要，需要有效建模行人-车辆多智能体交互关系。

Method: 采用TBIFormer架构改进，增加专用车辆编码器和行人-车辆交互交叉注意力模块，通过采样方案按场景复杂度分类训练，融合行人历史运动特征和周围车辆信息。

Result: 大量实验证明该方法在预测准确性方面取得显著提升，验证了不同行人-车辆交互建模方法的有效性。

Conclusion: 车辆感知的3D姿态预测对自动驾驶具有重要意义，所提出的框架为行人-车辆交互建模提供了有效解决方案。

Abstract: Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D

</details>


### [199] [WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models](https://arxiv.org/abs/2602.08971)
*Yu Shang,Zhuohang Li,Yiding Ma,Weikang Su,Xin Jin,Ziyou Wang,Xin Zhang,Yinzhou Tang,Chen Gao,Wei Wu,Xihui Liu,Dhruv Shah,Zhaoxiang Zhang,Zhibo Chen,Jun Zhu,Yonghong Tian,Tat-Seng Chua,Wenwu Zhu,Yong Li*

Main category: cs.CV

TL;DR: WorldArena是一个统一基准测试，用于系统评估具身世界模型在感知和功能维度的性能，提出EWMScore综合指标，发现感知质量与任务功能之间存在显著差距


<details>
  <summary>Details</summary>
Motivation: 当前具身世界模型的评估主要关注感知保真度（如视频生成质量），而忽略了这些模型在下游决策任务中的功能实用性，评估方法存在碎片化问题

Method: 引入WorldArena基准，通过三个维度评估模型：视频感知质量（16个指标覆盖6个子维度）、具身任务功能（作为数据引擎、策略评估器和动作规划器）、主观人类评估，并提出EWMScore综合指标

Result: 对14个代表性模型的广泛实验揭示了显著的感知-功能差距，表明高视觉质量并不一定转化为强大的具身任务能力

Conclusion: WorldArena提供了一个跟踪具身AI中真正功能性世界模型进展的框架，有助于推动该领域向更实用的方向发展

Abstract: While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>


### [200] [Generalizing Sports Feedback Generation by Watching Competitions and Reading Books: A Rock Climbing Case Study](https://arxiv.org/abs/2602.08996)
*Arushi Rai,Adriana Kovashka*

Main category: cs.CV

TL;DR: 提出利用辅助网络数据改进视频-LLMs在体育反馈生成任务中的表现，并引入特异性和可操作性两个新评估指标来解决传统指标不适用的问题


<details>
  <summary>Details</summary>
Motivation: 现有视频-LLMs在体育反馈生成任务上表现不佳，需要昂贵的微调数据且泛化能力差，同时传统文本生成评估指标无法有效衡量体育反馈质量

Method: 以攀岩为例，利用目标域的免费网络数据（比赛视频和教练手册）以及源域的现有体育反馈数据来改进模型性能；提出特异性和可操作性两个新评估指标

Result: 方法能够在有限标注条件下实现更有意义和实用的体育反馈生成

Conclusion: 通过利用辅助网络数据和新颖评估指标，可以有效提升视频-LLMs在体育反馈生成任务中的性能，解决泛化能力不足和评估不准确的问题

Abstract: While there is rapid progress in video-LLMs with advanced reasoning capabilities, prior work shows that these models struggle on the challenging task of sports feedback generation and require expensive and difficult-to-collect finetuning feedback data for each sport. This limitation is evident from the poor generalization to sports unseen during finetuning. Furthermore, traditional text generation evaluation metrics (e.g., BLEU-4, METEOR, ROUGE-L, BERTScore), originally developed for machine translation and summarization, fail to capture the unique aspects of sports feedback quality. To address the first problem, using rock climbing as our case study, we propose using auxiliary freely-available web data from the target domain, such as competition videos and coaching manuals, in addition to existing sports feedback from a disjoint, source domain to improve sports feedback generation performance on the target domain. To improve evaluation, we propose two evaluation metrics: (1) specificity and (2) actionability. Together, our approach enables more meaningful and practical generation of sports feedback under limited annotations.

</details>


### [201] [ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation](https://arxiv.org/abs/2602.09014)
*Zihan Yang,Shuyuan Tu,Licheng Zhang,Qi Dai,Yu-Gang Jiang,Zuxuan Wu*

Main category: cs.CV

TL;DR: ArcFlow是一种新颖的扩散模型蒸馏框架，通过非线性流轨迹参数化来近似教师模型的推理轨迹，实现了仅需2步推理即可达到40倍加速，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型蒸馏方法使用线性捷径近似教师轨迹，难以匹配随时间步变化的切线方向，导致质量下降。需要更精确的非线性轨迹近似方法。

Method: 提出ArcFlow框架，将推理轨迹的velocity场参数化为连续动量过程的混合，捕获速度演化并外推连贯速度形成连续非线性轨迹。采用轻量适配器进行轨迹蒸馏训练。

Result: 在Qwen-Image-20B和FLUX.1-dev等大规模模型上，仅微调不到5%的参数，实现2步推理40倍加速，无明显质量损失。基准测试显示定性和定量有效性。

Conclusion: ArcFlow通过非线性流轨迹参数化和解析积分，成功解决了扩散模型蒸馏中的轨迹近似问题，为高效高质量生成提供了有效解决方案。

Abstract: Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>


### [202] [Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction](https://arxiv.org/abs/2602.09016)
*Hao Phung,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: Raster2Seq：将平面图重建任务转化为序列到序列问题，通过自回归解码器预测多边形角点序列，实现从光栅化平面图到结构化矢量图形的准确重建。


<details>
  <summary>Details</summary>
Motivation: 现有技术在处理复杂室内空间平面图时，难以准确生成包含多个房间和变化多边形角点的结构和语义信息，需要更有效的重建方法。

Method: 提出序列到序列框架，将平面图元素表示为带标签的多边形序列；使用自回归解码器基于图像特征和已生成角点预测下一个角点；引入可学习锚点指导注意力机制聚焦于信息丰富的图像区域。

Result: 在Structure3D、CubiCasa5K和Raster2Graph等标准基准测试中达到最先进性能，在包含多样化房间结构和复杂几何变化的WAFFLE数据集上表现出强泛化能力。

Conclusion: 该方法通过自回归机制提供输出格式的灵活性，能够有效处理具有大量房间和多样化多边形结构的复杂平面图，为自动化理解和CAD工作流程提供了可靠的技术基础。

Abstract: Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.

</details>


### [203] [WorldCompass: Reinforcement Learning for Long-Horizon World Models](https://arxiv.org/abs/2602.09022)
*Zehan Wang,Tengfei Wang,Haiyu Zhang,Xuhui Zuo,Junta Wu,Haoyuan Wang,Wenqiang Sun,Zhenwei Wang,Chenjie Cao,Hengshuang Zhao,Chunchao Guo,Zhou Zhao*

Main category: cs.CV

TL;DR: WorldCompass是一个基于强化学习的后训练框架，专为长时程交互式视频世界模型设计，通过三个核心创新显著提升模型探索世界的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的世界模型在长时程交互探索中存在准确性和一致性不足的问题，需要更有效的引导机制来提升交互跟随能力和视觉质量。

Method: 1) 片段级rollout策略：在单个目标片段生成多个样本提高效率；2) 互补奖励函数：同时优化交互跟随准确性和视觉质量；3) 高效RL算法：采用负感知微调策略配合多种效率优化。

Result: 在SoTA开源世界模型WorldPlay上的评估显示，WorldCompass在各种场景下显著提升了交互准确性和视觉保真度。

Conclusion: WorldCompass通过创新的强化学习后训练框架，为视频世界模型提供了有效的探索引导机制，在保持高效的同时显著提升了模型性能。

Abstract: This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>


### [204] [Autoregressive Image Generation with Masked Bit Modeling](https://arxiv.org/abs/2602.09024)
*Qihang Yu,Qihao Liu,Ju He,Xinyang Zhang,Yang Liu,Liang-Chieh Chen,Xi Chen*

Main category: cs.CV

TL;DR: BAR框架通过掩码位自回归建模，证明了离散分词器在扩大码本规模后可以超越连续方法，在ImageNet-256上达到0.99的gFID新SOTA，同时降低采样成本并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 挑战视觉生成中连续管道的统治地位，系统研究离散与连续方法的性能差距，发现差距主要源于潜在空间的总比特分配（压缩比）而非离散分词器本身劣势。

Method: 提出掩码位自回归建模（BAR）框架，通过为自回归变换器配备掩码位建模头，逐步生成构成离散标记的比特来预测离散标记，支持任意码本大小。

Result: BAR在ImageNet-256上达到0.99的gFID新SOTA，超越连续和离散范式中的领先方法，显著降低采样成本，收敛速度比先前连续方法更快。

Conclusion: 扩大码本规模可有效弥合离散与连续方法的性能差距，BAR框架成功实现了这一潜力，为离散视觉生成提供了可扩展的高效解决方案。

Abstract: This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [205] [Does Visual Rendering Bypass Tokenization? Investigating Script-Tokenizer Misalignment in Pixel-Based Language Models](https://arxiv.org/abs/2602.06973)
*Lucky Susanto,Musa Izzanardi Wijanarko,Khumaisa Nur'aini,Farid Adilazuarda,Alham Fikri Aji,Derry Tanti Wijaya*

Main category: cs.CL

TL;DR: 本研究探讨了像素级语言建模中视觉渲染是否真正能摆脱分词器限制的问题，通过在印尼四种低资源本地语言上的实验发现，即使采用视觉渲染，重新引入文本分词器仍会导致分词器不对齐问题，定制分词器比通用分词器性能提升高达30.15%。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证像素级语言建模是否真正能绕过子词分词瓶颈，特别是在多模态变体如DualGPT重新引入文本分词器的情况下，是否仍存在分词器不对齐问题。

Method: 使用DualGPT架构，在四种印尼低资源本地语言（爪哇语、巴厘语、巽他语和楠榜语）上评估脚本-分词器对齐的影响，比较Llama 2通用分词器与定制分词器的性能。

Result: 尽管Llama 2分词器具有较低的OOV和生育率，但性能显著差于定制分词器，定制分词器在chrF++指标上提升高达30.15%。

Conclusion: 视觉渲染无法完全解决分词器不对齐问题，文本分词器仍然是实现公平模型的重要障碍，这对未来多模态变体研究具有警示意义。

Abstract: While pixel-based language modeling aims to bypass the sub-word tokenization bottleneck by rendering text as images, recent multimodal variants such as DualGPT reintroduce text tokenizers to improve autoregressive performance. We investigate a fundamental question, does visual rendering truly decouple a model from tokenization constraints? Focusing on four Indonesian low-resource local languages that have their own non-Latin scripts (i.e., Javanese, Balinese, Sundanese, and Lampungnese), we evaluate the impact of script-tokenizer alignment within the DualGPT architecture. Our results show that, despite visual rendering, reintegrating a text tokenizer into the architecture reintroduces the same issue that pixel-based language modeling aims to resolve, which is the tokenizer misalignment problem. Despite having lower OOV and fertility rates, we show that the Llama 2 tokenizer performs significantly worse than a custom tokenizer, with improvements of up to 30.15 chrF++. Our findings serve as a warning for future multimodal variants, as text tokenizers remain a significant barrier to equitable models.

</details>


### [206] [BiomechAgent: AI-Assisted Biomechanical Analysis Through Code-Generating Agents](https://arxiv.org/abs/2602.06975)
*R. James Cotton,Thomas Leonard*

Main category: cs.CL

TL;DR: BiomechAgent是一个代码生成AI代理，通过自然语言实现生物力学分析，让无编程经验的临床医生能够查询数据库、生成可视化图表和解释运动捕捉数据。


<details>
  <summary>Details</summary>
Motivation: 解决无编程经验的临床医生在分析标记点运动捕捉数据时的技术障碍，使定量运动分析更加普及和实用。

Method: 开发了一个系统化基准测试，涵盖数据检索、可视化、活动分类、时间分割和临床推理等多个任务。采用生物力学领域的特定指令和经过验证的专业工具（如步态事件检测工具）来提升性能。

Result: BiomechAgent在数据检索和可视化任务上表现出稳健的准确性，并展现出新兴的临床推理能力。生物力学领域的特定指令显著优于通用提示，专业工具的集成大幅提升了时空分析的准确性。但本地开源模型在大多数领域的性能明显低于前沿云端大语言模型。

Conclusion: BiomechAgent通过自然语言界面使运动捕捉数据对最终用户更加有用和易用，显著降低了生物力学分析的技术门槛。

Abstract: Markerless motion capture is making quantitative movement analysis increasingly accessible, yet analyzing the resulting data remains a barrier for clinicians without programming expertise. We present BiomechAgent, a code-generating AI agent that enables biomechanical analysis through natural language and allows users to querying databases, generating visualizations, and even interpret data without requiring users to write code. To evaluate BiomechAgent's capabilities, we developed a systematic benchmark spanning data retrieval, visualization, activity classification, temporal segmentation, and clinical reasoning. BiomechAgent achieved robust accuracy on data retrieval and visualization tasks and demonstrated emerging clinical reasoning capabilities. We used our dataset to systematically evaluate several of our design decisions. Biomechanically-informed, domain-specific instructions significantly improved performance over generic prompts, and integrating validated specialized tools for gait event detection substantially boosted accuracy on challenging spatiotemporal analysis where the base agent struggled. We also tested BiomechAgent using a local open-weight model instead of a frontier cloud based LLM and found that perform was substantially diminished in most domains other than database retrieval. In short, BiomechAgent makes the data from accessible motion capture and much more useful and accessible to end users.

</details>


### [207] [Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks](https://arxiv.org/abs/2602.06976)
*Chen Shen,Wei Cheng,Jingyue Yang,Huan Zhang,Yuhan Wu,Wei Hu*

Main category: cs.CL

TL;DR: ILA-agent框架通过工具化行为原语，让大语言模型能在推理时动态学习陌生编程语言，在低资源Cangjie语言基准测试中显著优于检索增强基线。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型面对陌生编程语言时性能下降的问题，避免数据密集的微调，探索通过推理时交互学习的新范式。

Method: 提出ILA-agent框架，将人类探索行为建模为工具集，使LLM能够通过官方文档和执行环境的结构化交互，增量式探索、应用和验证语言知识。

Result: 在Cangjie基准测试的代码生成、翻译和程序修复任务中，ILA-agent显著优于检索增强基线方法，并展现出新兴的行为模式。

Conclusion: ILA范式为低资源编程语言学习提供了有效途径，但性能差距仍然存在，需要进一步研究。

Abstract: The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.

</details>


### [208] [Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model](https://arxiv.org/abs/2602.07120)
*Jacqueline He,Jonathan Hayase,Wen-tau Yih,Sewoong Oh,Luke Zettlemoyer,Pang Wei Koh*

Main category: cs.CL

TL;DR: Anchored Decoding是一种即插即用的推理时方法，通过将生成限制在安全LM的邻近范围内来抑制语言模型的逐字复制行为，提供可调节的风险-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型倾向于记忆训练数据并逐字输出，当涉及敏感或受版权保护的内容时，这会引发创作者同意与补偿问题以及开发者合规风险。

Method: 提出Anchored Decoding方法，在推理时通过用户选择的信息预算分配和每步约束，使生成保持在经过许可训练的安全LM的有界邻近范围内。同时开发了字节级变体Anchored$_{\mathrm{Byte}}$ Decoding和新的安全模型TinyComma 1.8B。

Result: 在六个模型对的长篇评估中，Anchored和Anchored$_{\mathrm{Byte}}$ Decoding在保持接近原始流畅性和事实性的同时，将风险基线与安全参考之间的可测量复制差距平均减少了75%，仅带来适度的推理开销。

Conclusion: 该方法定义了一个新的帕累托前沿，有效解决了语言模型逐字复制问题，为混合许可数据训练的语言模型提供了实用的版权合规解决方案。

Abstract: Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored$_{\mathrm{Byte}}$ Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored$_{\mathrm{Byte}}$ Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.

</details>


### [209] [Free Energy Mixer](https://arxiv.org/abs/2602.07160)
*Jiecheng Lu,Shihao Yang*

Main category: cs.CL

TL;DR: Free Energy Mixer (FEM) 是一种新型注意力机制，使用自由能（log-sum-exp）读取方式，通过值驱动的通道级对数线性倾斜实现从平均到通道选择的平滑过渡，在保持并行性和原始复杂度的情况下提升性能。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制通过每头凸平均读取键值对，无法实现通道级选择，限制了表达能力和性能。

Method: 提出 FEM 方法，将 (q,k) 评分分布作为先验，通过值驱动的逆温度参数产生值感知的后验读取，实现从平均到通道选择的平滑过渡，保持 O(T²) 复杂度。

Result: 在 NLP、视觉和时间序列任务上，FEM 在相同参数量下持续超越强基线模型。

Conclusion: FEM 提供了一种高效且通用的注意力机制改进方案，能够无缝替换标准和线性注意力，在多个领域展现出色性能。

Abstract: Standard attention stores keys/values losslessly but reads them via a per-head convex average, blocking channel-wise selection. We propose the Free Energy Mixer (FEM): a free-energy (log-sum-exp) read that applies a value-driven, per-channel log-linear tilt to a fast prior (e.g., from queries/keys in standard attention) over indices. Unlike methods that attempt to improve and enrich the $(q,k)$ scoring distribution, FEM treats it as a prior and yields a value-aware posterior read at unchanged complexity, smoothly moving from averaging to per-channel selection as the learnable inverse temperature increases, while still preserving parallelism and the original asymptotic complexity ($O(T^2)$ for softmax; $O(T)$ for linearizable variants). We instantiate a two-level gated FEM that is plug-and-play with standard and linear attention, linear RNNs and SSMs. It consistently outperforms strong baselines on NLP, vision, and time-series at matched parameter budgets.

</details>


### [210] [Your Language Model Secretly Contains Personality Subnetworks](https://arxiv.org/abs/2602.07164)
*Ruimeng Ye,Zihan Wang,Zinan Ling,Yang Xiao,Manling Li,Xiaolong Ma,Bo Hui*

Main category: cs.CL

TL;DR: LLMs内部已包含人格特化的子网络，无需外部知识即可实现人格适应。通过校准数据识别激活特征，开发掩码策略分离轻量级人格子网络，并引入对比剪枝策略增强对立人格的分离效果。该方法完全无需训练，在多个评估设置中表现出比需要外部知识的基线更强的人格对齐效果。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否真的需要外部上下文或参数来适应不同行为，还是这些知识已经嵌入在模型参数中。研究LLMs参数空间中是否已存在人格特化的子网络。

Method: 使用小型校准数据集识别不同人格的激活特征，基于这些统计特征开发掩码策略来隔离轻量级人格子网络。针对二元对立人格场景，引入对比剪枝策略识别导致统计差异的参数。方法完全无需训练，仅依赖语言模型现有参数空间。

Result: 生成的子网络在多样化评估设置中表现出比需要外部知识的基线方法显著更强的人格对齐效果，同时更加高效。

Conclusion: 多样化的人类行为不仅是在LLMs中被诱导出来的，而是已经嵌入在其参数空间中，这为大型语言模型的可控和可解释个性化提供了新视角。

Abstract: Humans shift between different personas depending on social context. Large Language Models (LLMs) demonstrate a similar flexibility in adopting different personas and behaviors. Existing approaches, however, typically adapt such behavior through external knowledge such as prompting, retrieval-augmented generation (RAG), or fine-tuning. We ask: do LLMs really need external context or parameters to adapt to different behaviors, or do they already have such knowledge embedded in their parameters? In this work, we show that LLMs already contain persona-specialized subnetworks in their parameter space. Using small calibration datasets, we identify distinct activation signatures associated with different personas. Guided by these statistics, we develop a masking strategy that isolates lightweight persona subnetworks. Building on the findings, we further discuss: how can we discover opposing subnetwork from the model that lead to binary-opposing personas, such as introvert-extrovert? To further enhance separation in binary opposition scenarios, we introduce a contrastive pruning strategy that identifies parameters responsible for the statistical divergence between opposing personas. Our method is entirely training-free and relies solely on the language model's existing parameter space. Across diverse evaluation settings, the resulting subnetworks exhibit significantly stronger persona alignment than baselines that require external knowledge while being more efficient. Our findings suggest that diverse human-like behaviors are not merely induced in LLMs, but are already embedded in their parameter space, pointing toward a new perspective on controllable and interpretable personalization in large language models.

</details>


### [211] [Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI](https://arxiv.org/abs/2602.07176)
*Mohamed El Hajji,Tarek Ait Baha,Aicha Dakir,Hammou Fadili,Youssef Es-Saady*

Main category: cs.CL

TL;DR: Open TutorAI是一个基于大语言模型和生成技术的开源教育平台，提供动态个性化辅导，通过3D虚拟化身实现多模态交互，结合学习分析支持自适应学习环境。


<details>
  <summary>Details</summary>
Motivation: 现有教育聊天机器人系统缺乏情境适应性、实时响应能力和教学灵活性，限制了学习参与度和教学效果，需要开放集成平台结合AI和沉浸式技术来支持个性化学习体验。

Method: 开发基于LLM的开源平台，集成自然语言处理和可定制3D虚拟化身，通过结构化注册流程捕获学习者目标和偏好，配置个性化AI助手，提供文本和虚拟化身界面，包含内容组织、嵌入式反馈和学习分析工具。

Result: 创建了Open TutorAI平台，具备模块化架构、生成式AI和学习分析功能，增强了学习参与度和情感存在感，为学习者、教育者和家长提供专用界面，支持自我调节学习。

Conclusion: 该平台将模块化架构、生成式AI和学习分析统一在开源框架中，推动了新一代智能辅导系统的发展，创造了更加人性化、沉浸式的学习环境。

Abstract: Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner engagement and diminish instructional effectiveness. Thus, there is a growing need for open, integrative platforms that combine AI and immersive technologies to support personalized, meaningful learning experiences. This paper presents Open TutorAI, an open-source educational platform based on LLMs and generative technologies that provides dynamic, personalized tutoring. The system integrates natural language processing with customizable 3D avatars to enable multimodal learner interaction. Through a structured onboarding process, it captures each learner's goals and preferences in order to configure a learner-specific AI assistant. This assistant is accessible via both text-based and avatar-driven interfaces. The platform includes tools for organizing content, providing embedded feedback, and offering dedicated interfaces for learners, educators, and parents. This work focuses on learner-facing components, delivering a tool for adaptive support that responds to individual learner profiles without requiring technical expertise. Its assistant-generation pipeline and avatar integration enhance engagement and emotional presence, creating a more humanized, immersive learning environment. Embedded learning analytics support self-regulated learning by tracking engagement patterns and generating actionable feedback. The result is Open TutorAI, which unites modular architecture, generative AI, and learner analytics within an open-source framework. It contributes to the development of next-generation intelligent tutoring systems.

</details>


### [212] [Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs](https://arxiv.org/abs/2602.07181)
*Tianyu Zhao,Siqi Li,Yasser Shoukry,Salma Elmalaki*

Main category: cs.CL

TL;DR: 通过将用户偏好与人格特质对齐来提升LLM个性化回答质量的研究，提出PACIFIC数据集和基于人格对齐偏好的自动检索生成框架


<details>
  <summary>Details</summary>
Motivation: 实践中用户偏好信号可能存在噪声、不完整或误导性，直接使用会降低回答质量。研究发现稳定的人格特质是偏好背后的潜在信号，基于人格对齐的偏好能显著改善个性化问答效果

Method: 1) 通过实验验证人格对齐偏好的有效性；2) 构建PACIFIC数据集（1200条偏好陈述，标注大五人格特质方向）；3) 提出LLM自动检索人格对齐偏好并融入答案生成的框架

Result: 使用人格对齐偏好使答案选择准确率从29.25%提升至76%，相比随机选择偏好有显著改善

Conclusion: 人格特质作为偏好的潜在信号能有效提升LLM个性化回答质量，提出的PACIFIC数据集和框架为基于人格的个性化生成提供了实用解决方案

Abstract: User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.

</details>


### [213] [Long-Context Long-Form Question Answering for Legal Domain](https://arxiv.org/abs/2602.07190)
*Anagha Kulkarni,Parin Rajesh Jhaveri,Prasha Shrestha,Yu Tong Han,Reza Amini,Behrouz Madahian*

Main category: cs.CL

TL;DR: 提出针对法律文档长上下文问答系统，解决复杂文档布局、专业术语和长答案生成的挑战，包含术语解构、布局解析和答案生成模块，并引入覆盖率评估指标。


<details>
  <summary>Details</summary>
Motivation: 法律文档具有复杂的嵌套结构、冗长脚注和专业术语，这些特性使得长上下文和长形式答案的问答变得特别困难，需要专门的方法来处理这些独特挑战。

Method: 开发问答系统，包括：(a) 解构领域特定术语以改进检索；(b) 解析复杂文档布局，隔离章节和脚注并适当链接；(c) 使用精确领域术语生成全面答案。引入基于召回的覆盖率评估指标，并利用法律和税务专业人士专业知识构建QA数据集。

Result: 通过全面的实验和消融研究，证明了所提出系统的可用性和优势，展示了在处理法律文档长上下文问答任务中的有效性。

Conclusion: 该系统成功解决了法律文档特有的长上下文问答挑战，通过专门的术语处理、布局解析和答案生成技术，配合新的评估指标，为法律领域的自动化问答提供了有效解决方案。

Abstract: Legal documents have complex document layouts involving multiple nested sections, lengthy footnotes and further use specialized linguistic devices like intricate syntax and domain-specific vocabulary to ensure precision and authority. These inherent characteristics of legal documents make question answering challenging, and particularly so when the answer to the question spans several pages (i.e. requires long-context) and is required to be comprehensive (i.e. a long-form answer). In this paper, we address the challenges of long-context question answering in context of long-form answers given the idiosyncrasies of legal documents. We propose a question answering system that can (a) deconstruct domain-specific vocabulary for better retrieval from source documents, (b) parse complex document layouts while isolating sections and footnotes and linking them appropriately, (c) generate comprehensive answers using precise domain-specific vocabulary. We also introduce a coverage metric that classifies the performance into recall-based coverage categories allowing human users to evaluate the recall with ease. We curate a QA dataset by leveraging the expertise of professionals from fields such as law and corporate tax. Through comprehensive experiments and ablation studies, we demonstrate the usability and merit of the proposed system.

</details>


### [214] [Equipping LLM with Directional Multi-Talker Speech Understanding Capabilities](https://arxiv.org/abs/2602.07211)
*Ju Lin,Jing Pan,Ruizhi Li,Ming Sun,Yuzong Liu,Alaa Hassan,Jing Zheng,Florian Metze*

Main category: cs.CL

TL;DR: 论文提出了两种新方法（级联系统和端到端系统）来为大型语言模型赋予定向多说话人语音理解能力，特别针对智能眼镜应用场景，利用多麦克风阵列实现流式处理，在语音识别和翻译任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有语音LLM主要基于单通道单说话人数据训练，难以直接应用于多说话人多通道的语音理解任务，特别是在智能眼镜等需要定向语音处理的场景中。

Method: 提出两种方法：1）级联系统，使用源分离前端模块；2）端到端系统，采用序列化输出训练。两种方法都利用智能眼镜中的多麦克风阵列进行流式定向解释和处理。

Result: 实验结果表明，所提方法能有效为LLM赋予定向语音理解能力，在语音识别和语音翻译任务中均取得强劲性能。

Conclusion: 通过多麦克风阵列和创新的系统架构，成功实现了LLM在多说话人多通道环境下的定向语音理解，为智能眼镜等实际应用场景提供了有效解决方案。

Abstract: Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech understanding capabilities. However, most speech LLMs are trained on single-channel, single-talker data, which makes it challenging to directly apply them to multi-talker and multi-channel speech understanding task. In this work, we present a comprehensive investigation on how to enable directional multi-talker speech understanding capabilities for LLMs, specifically in smart glasses usecase. We propose two novel approaches to integrate directivity into LLMs: (1) a cascaded system that leverages a source separation front-end module, and (2) an end-to-end system that utilizes serialized output training. All of the approaches utilize a multi-microphone array embedded in smart glasses to optimize directivity interpretation and processing in a streaming manner. Experimental results demonstrate the efficacy of our proposed methods in endowing LLMs with directional speech understanding capabilities, achieving strong performance in both speech recognition and speech translation tasks.

</details>


### [215] [Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice](https://arxiv.org/abs/2602.07319)
*Savan Doshi*

Main category: cs.CL

TL;DR: 提出了一个风险敏感的幻觉评估框架，通过识别风险性语言（如治疗指令、禁忌症、紧急提示和高风险药物）来量化医疗问答中AI模型产生的潜在危害，替代传统仅关注事实正确性的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉评估标准主要关注事实正确性，将所有错误视为同等严重，这掩盖了临床相关的失败模式，特别是当模型生成无依据但可执行的医疗建议时可能带来的风险。

Method: 开发风险敏感评估框架，结合风险评分（识别风险性语言）和相关性度量，识别高风险低基础性失败；在三个指令调优语言模型上使用控制性患者导向提示进行安全压力测试。

Result: 结果显示表面行为相似的模型具有显著不同的风险特征，标准评估指标无法捕捉这些差异；高风险低基础性失败模式普遍存在。

Conclusion: 将风险敏感性纳入幻觉评估至关重要，评估有效性高度依赖于任务和提示设计，需要更细致的评估方法来捕捉医疗AI系统的潜在危害。

Abstract: Large language models are increasingly being used in patient-facing medical question answering, where hallucinated outputs can vary widely in potential harm. However, existing hallucination standards and evaluation metrics focus primarily on factual correctness, treating all errors as equally severe. This obscures clinically relevant failure modes, particularly when models generate unsupported but actionable medical language. We propose a risk-sensitive evaluation framework that quantifies hallucinations through the presence of risk-bearing language, including treatment directives, contraindications, urgency cues, and mentions of high-risk medications. Rather than assessing clinical correctness, our approach evaluates the potential impact of hallucinated content if acted upon. We further combine risk scoring with a relevance measure to identify high-risk, low-grounding failures. We apply this framework to three instruction-tuned language models using controlled patient-facing prompts designed as safety stress tests. Our results show that models with similar surface-level behavior exhibit substantially different risk profiles and that standard evaluation metrics fail to capture these distinctions. These findings highlight the importance of incorporating risk sensitivity into hallucination evaluation and suggest that evaluation validity is critically dependent on task and prompt design.

</details>


### [216] [Intent Mismatch Causes LLMs to Get Lost in Multi-Turn Conversation](https://arxiv.org/abs/2602.07338)
*Geng Liu,Fei Zhu,Rong Feng,Changyi Ma,Shiqi Wang,Gaofeng Meng*

Main category: cs.CL

TL;DR: 论文提出Lost in Conversation现象源于意图对齐差距而非模型能力缺陷，通过解耦意图理解与任务执行的Mediator-Assistant架构有效缓解多轮对话性能下降问题


<details>
  <summary>Details</summary>
Motivation: 发现LLMs在多轮对话中相比单轮交互存在显著性能下降(Lost in Conversation现象)，但认为根本原因在于用户意图对齐差距而非模型内在能力缺陷

Method: 提出Mediator-Assistant架构，使用经验驱动的Mediator将模糊用户输入转换为基于历史交互模式的明确结构化指令，实现意图理解与任务执行的解耦

Result: 实验结果表明该方法在不同LLMs上显著缓解多轮对话中的性能退化问题

Conclusion: LiC问题源于交互结构而非模型能力，通过Mediator-Assistant架构能够有效桥接用户意图与模型理解之间的差距

Abstract: Multi-turn conversation has emerged as a predominant interaction paradigm for Large Language Models (LLMs). Users often employ follow-up questions to refine their intent, expecting LLMs to adapt dynamically. However, recent research reveals that LLMs suffer a substantial performance drop in multi-turn settings compared to single-turn interactions with fully specified instructions, a phenomenon termed ``Lost in Conversation'' (LiC). While this prior work attributes LiC to model unreliability, we argue that the root cause lies in an intent alignment gap rather than intrinsic capability deficits. In this paper, we first demonstrate that LiC is not a failure of model capability but rather a breakdown in interaction between users and LLMs. We theoretically show that scaling model size or improving training alone cannot resolve this gap, as it arises from structural ambiguity in conversational context rather than representational limitations. To address this, we propose to decouple intent understanding from task execution through a Mediator-Assistant architecture. By utilizing an experience-driven Mediator to explicate user inputs into explicit, well-structured instructions based on historical interaction patterns, our approach effectively bridges the gap between vague user intent and model interpretation. Experimental results demonstrate that this method significantly mitigates performance degradation in multi-turn conversations across diverse LLMs.

</details>


### [217] [ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations](https://arxiv.org/abs/2602.07361)
*Long S. T. Nguyen,Quan M. Bui,Tin T. Ngo,Quynh T. N. Vo,Dung N. H. Le,Tho T. Quan*

Main category: cs.CL

TL;DR: ViHERMES：一个针对越南语医疗法规多跳问答的基准数据集，包含高质量问题-答案对，需要跨多个法规进行推理，并提出了基于图感知的检索框架来提升性能。


<details>
  <summary>Details</summary>
Motivation: 医疗法规问答面临多跳推理的挑战，特别是在低资源语言如越南语中缺乏支持多跳推理的基准数据集，因此需要构建专门的评估基准。

Method: 提出基于语义聚类和图启发的数据挖掘控制管道生成多跳QA对，使用大语言模型生成结构化证据和推理标注，并开发图感知检索框架建模法律单元层面的正式法律关系。

Result: 实验结果表明ViHERMES为评估多跳法规QA系统提供了具有挑战性的基准，提出的图感知方法持续优于强检索基线方法。

Conclusion: ViHERMES填补了越南语医疗法规多跳推理QA基准的空白，提出的图感知检索框架有效提升了多跳法规问答的性能，数据集和系统实现已公开提供。

Abstract: Question Answering (QA) over regulatory documents is inherently challenging due to the need for multihop reasoning across legally interdependent texts, a requirement that is particularly pronounced in the healthcare domain where regulations are hierarchically structured and frequently revised through amendments and cross-references. Despite recent progress in retrieval-augmented and graph-based QA methods, systematic evaluation in this setting remains limited, especially for low-resource languages such as Vietnamese, due to the lack of benchmark datasets that explicitly support multihop reasoning over healthcare regulations. In this work, we introduce the Vietnamese Healthcare Regulations-Multihop Reasoning Dataset (ViHERMES), a benchmark designed for multihop QA over Vietnamese healthcare regulatory documents. ViHERMES consists of high-quality question-answer pairs that require reasoning across multiple regulations and capture diverse dependency patterns, including amendment tracing, cross-document comparison, and procedural synthesis. To construct the dataset, we propose a controlled multihop QA generation pipeline based on semantic clustering and graph-inspired data mining, followed by large language model-based generation with structured evidence and reasoning annotations. We further present a graph-aware retrieval framework that models formal legal relations at the level of legal units and supports principled context expansion for legally valid and coherent answers. Experimental results demonstrate that ViHERMES provides a challenging benchmark for evaluating multihop regulatory QA systems and that the proposed graph-aware approach consistently outperforms strong retrieval-based baselines. The ViHERMES dataset and system implementation are publicly available at https://github.com/ura-hcmut/ViHERMES.

</details>


### [218] [TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling](https://arxiv.org/abs/2602.07374)
*Nisharg Nargund,Priyesh Shukla*

Main category: cs.CL

TL;DR: TernaryLM是一个132M参数的三元量化语言模型，在训练时使用原生1位三元量化{-1, 0, +1}，显著减少内存需求同时保持语言建模能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型计算资源需求大，限制了在边缘设备和资源受限环境中的部署，需要开发更高效的内存优化方法。

Method: 采用从零开始的训练方法，使用直通估计器和自适应逐层缩放因子学习量化感知表示，而非后训练量化方法。

Result: 在TinyStories上验证困惑度为58.42，MRPC复述检测F1分数82.47%，内存减少2.4倍（498MB vs 1197MB），推理延迟相当，训练稳定性良好。

Conclusion: 原生1位训练是高效神经语言模型的有前景方向，中间变换层对极端量化兼容性最高，为未来非均匀精度策略提供指导。

Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0, +1} during training, achieving significant memory reduction without sacrificing language modeling capability. Unlike post-training quantization approaches that quantize pre-trained full-precision models, TernaryLM learns quantization-aware representations from scratch using straight-through estimators and adaptive per-layer scaling factors. Our experiments demonstrate: (1) validation perplexity of 58.42 on TinyStories; (2) downstream transfer with 82.47 percent F1 on MRPC paraphrase detection; (3) 2.4x memory reduction (498MB vs 1197MB) with comparable inference latency; and (4) stable training dynamics across diverse corpora. We provide layer-wise quantization analysis showing that middle transformer layers exhibit highest compatibility with extreme quantization, informing future non-uniform precision strategies. Our results suggest that native 1-bit training is a promising direction for efficient neural language models. Code is available at https://github.com/1nisharg/TernaryLM-Memory-Efficient-Language-Modeling.

</details>


### [219] [Efficient Post-Training Pruning of Large Language Models with Statistical Correction](https://arxiv.org/abs/2602.07375)
*Peiqi Yu,Jinhao Wang,Xinyi Sui,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CL

TL;DR: 提出基于一阶统计特性的轻量级后训练剪枝框架，通过统计校准重要性评分和能量补偿修正分布偏差，无需重训练或二阶信息，在保持计算效率的同时提升剪枝质量


<details>
  <summary>Details</summary>
Motivation: 现有后训练剪枝方法在剪枝质量和计算效率之间存在权衡：启发式方法高效但对激活异常值敏感，重构方法保真度高但计算代价大

Method: 使用模型权重和激活的一阶统计特性：剪枝时通过通道级统计校准基于幅值的重要性评分，剪枝后应用解析能量补偿修正权重移除导致的分布失真

Result: 在多个LLM家族、稀疏模式和评估任务上的实验表明，该方法在保持与启发式方法相当计算成本的同时提升了剪枝性能

Conclusion: 简单的统计校正对于LLM的后训练剪枝是有效的，为高效模型压缩提供了新思路

Abstract: Post-training pruning is an effective approach for reducing the size and inference cost of large language models (LLMs), but existing methods often face a trade-off between pruning quality and computational efficiency. Heuristic pruning methods are efficient but sensitive to activation outliers, while reconstruction-based approaches improve fidelity at the cost of heavy computation. In this work, we propose a lightweight post-training pruning framework based on first-order statistical properties of model weights and activations. During pruning, channel-wise statistics are used to calibrate magnitude-based importance scores, reducing bias from activation-dominated channels. After pruning, we apply an analytic energy compensation to correct distributional distortions caused by weight removal. Both steps operate without retraining, gradients, or second-order information. Experiments across multiple LLM families, sparsity patterns, and evaluation tasks show that the proposed approach improves pruning performance while maintaining computational cost comparable to heuristic methods. The results suggest that simple statistical corrections can be effective for post-training pruning of LLMs.

</details>


### [220] [Do Large Language Models Reflect Demographic Pluralism in Safety?](https://arxiv.org/abs/2602.07376)
*Usman Naseem,Gautam Siddharth Kashyap,Sushant Kumar Ray,Rafiq Ali,Ebad Shabbir,Abdullah Mohammad*

Main category: cs.CL

TL;DR: Demo-SafetyBench是一个解决LLM安全评估中人口统计学多元性缺失的新数据集，通过两阶段方法构建了43,050个样本，在保持高可靠性（ICC=0.87）的同时实现了低人口统计敏感性（DS=0.12）。


<details>
  <summary>Details</summary>
Motivation: 现有对齐数据集（如ANTHROPIC-HH和DICES）使用人口统计学上狭窄的标注者群体，忽视了不同社区间安全感知的差异，无法反映LLM安全固有的多元性特征。

Method: 第一阶段：将DICES提示重新分类为14个安全域，使用Mistral 7B-Instruct-v0.3和Llama-3.1-8B-Instruct进行扩展，采用SimHash去重。第二阶段：使用LLMs-as-Raters方法（Gemma-7B、GPT-4o、LLaMA-2-7B）进行零样本推理的多元敏感性评估。

Result: 构建了43,050个样本的数据集，平衡阈值（delta=0.5，tau=10）实现了高可靠性（ICC=0.87）和低人口统计敏感性（DS=0.12），证明多元安全评估既可扩展又具有人口统计鲁棒性。

Conclusion: Demo-SafetyBench成功解决了LLM安全评估中的人口统计多元性问题，通过提示层面的建模实现了价值框架与响应的解耦，为可扩展且稳健的多元安全评估提供了有效解决方案。

Abstract: Large Language Model (LLM) safety is inherently pluralistic, reflecting variations in moral norms, cultural expectations, and demographic contexts. Yet, existing alignment datasets such as ANTHROPIC-HH and DICES rely on demographically narrow annotator pools, overlooking variation in safety perception across communities. Demo-SafetyBench addresses this gap by modeling demographic pluralism directly at the prompt level, decoupling value framing from responses. In Stage I, prompts from DICES are reclassified into 14 safety domains (adapted from BEAVERTAILS) using Mistral 7B-Instruct-v0.3, retaining demographic metadata and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based deduplication, yielding 43,050 samples. In Stage II, pluralistic sensitivity is evaluated using LLMs-as-Raters-Gemma-7B, GPT-4o, and LLaMA-2-7B-under zero-shot inference. Balanced thresholds (delta = 0.5, tau = 10) achieve high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12), confirming that pluralistic safety evaluation can be both scalable and demographically robust.

</details>


### [221] [When the Model Said 'No Comment', We Knew Helpfulness Was Dead, Honesty Was Alive, and Safety Was Terrified](https://arxiv.org/abs/2602.07381)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: AlignX是一个两阶段框架，通过提示注入微调和几何校准的MoE路由来解决LLM对齐中的多目标冲突问题，在有用性、无害性和诚实性方面取得显著提升，同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有SFT和MoE方法在多目标对齐中存在目标冲突干扰和路由校准问题，导致特征空间分离和推理不可靠的轴崩溃现象。

Method: 第一阶段使用提示注入微调提取轴特定任务特征防止灾难性遗忘；第二阶段采用基于分形和自然几何的MoCaE模块校准专家路由。

Result: 在Alpaca、BeaverTails和TruthfulQA基准上取得+171.5%胜率、+110.1%真实信息性提升和4.3%安全违规减少，延迟和内存使用降低35%以上。

Conclusion: AlignX有效解决了多目标对齐中的轴崩溃问题，在保持高性能的同时显著提升了计算效率，具有广泛的模型泛化能力。

Abstract: Large Language Models (LLMs) need to be in accordance with human values-being helpful, harmless, and honest (HHH)-is important for safe deployment. Existing works use Supervised Fine-Tuning (SFT) and Mixture-of-Experts (MoE) to align LLMs. However, these works face challenges in multi-objective settings, such as SFT leading to interference between conflicting objectives, while MoEs suffer from miscalibrated routing. We term this failure mode Axis Collapse, marked by (1) disjoint feature spaces causing catastrophic forgetting, and (2) unreliable inference from misrouted experts. To resolve this, we propose AlignX, a two-stage framework. Stage 1 uses prompt-injected fine-tuning to extract axis-specific task features, mitigating catastrophic forgetting. Stage 2 deploys a MoCaE module that calibrates expert routing using fractal and natural geometry, improving inference reliability. AlignX achieves significant gains on Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty), with +171.5% win rate, +110.1% in truthfulness-informativeness, and 4.3% fewer safety violations. It also reduces latency and memory usage by over 35% compared to prior MoEs. Results across four LLMs validate its generalizability.

</details>


### [222] [Advantages of Domain Knowledge Injection for Legal Document Summarization: A Case Study on Summarizing Indian Court Judgments in English and Hindi](https://arxiv.org/abs/2602.07382)
*Debtanu Datta,Rajdeep Mukherjee,Adrijit Goswami,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 本研究通过注入领域知识改进印度法律文本摘要，提出使用法律领域预训练编码器增强抽取式模型，并通过持续预训练将法律知识注入生成式模型，在英-英和英-印地语法律文档摘要任务中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 印度法律判决摘要任务复杂，不仅因为法律文本语言复杂且非结构化，还因为大量印度人口不理解复杂的法律英语，需要印度语言的摘要。

Method: 1. 提出框架增强抽取式神经摘要模型，整合针对法律文本的领域特定预训练编码器；2. 探索通过持续预训练将法律领域知识注入生成式模型（包括大语言模型），使用英语和印地语大型法律语料库。

Result: 提出的方法在英-英和英-印地语印度法律文档摘要任务中，通过标准评估指标、事实一致性指标和法律领域特定指标衡量，取得了统计显著的改进。

Conclusion: 领域专家的验证证明了所提方法的有效性，表明将法律领域知识注入摘要模型能够显著提升印度法律文本的多语言摘要质量。

Abstract: Summarizing Indian legal court judgments is a complex task not only due to the intricate language and unstructured nature of the legal texts, but also since a large section of the Indian population does not understand the complex English in which legal text is written, thus requiring summaries in Indian languages. In this study, we aim to improve the summarization of Indian legal text to generate summaries in both English and Hindi (the most widely spoken Indian language), by injecting domain knowledge into diverse summarization models. We propose a framework to enhance extractive neural summarization models by incorporating domain-specific pre-trained encoders tailored for legal texts. Further, we explore the injection of legal domain knowledge into generative models (including Large Language Models) through continual pre-training on large legal corpora in English and Hindi. Our proposed approaches achieve statistically significant improvements in both English-to-English and English-to-Hindi Indian legal document summarization, as measured by standard evaluation metrics, factual consistency metrics, and legal domain-specific metrics. Furthermore, these improvements are validated through domain experts, demonstrating the effectiveness of our approaches.

</details>


### [223] [Measuring cross-language intelligibility between Romance languages with computational tools](https://arxiv.org/abs/2602.07447)
*Liviu P Dinu,Ana Sabina Uban,Bogdan Iordache,Anca Dinu,Simona Georgescu*

Main category: cs.CL

TL;DR: 提出基于词汇相似度的计算指标来评估罗曼语族语言间的互通性，通过正字法和语音形式分析五种主要罗曼语，结果与人类实验显著相关


<details>
  <summary>Details</summary>
Motivation: 研究罗曼语族语言间的相互理解性，需要开发有效的计算指标来量化语言互通程度，验证计算指标与人类直觉和实验结果的关联性

Method: 使用新颖的计算指标，基于词汇表面和语义相似度，分析法语、意大利语、葡萄牙语、西班牙语和罗马尼亚语的正字法和语音形式，采用不同平行语料库和词向量表示模型

Result: 获得的互通性分数证实了语言间互通不对称性的直觉，与人类完形填空测试结果显著相关

Conclusion: 基于词汇相似度的计算指标能有效评估罗曼语族语言间的相互理解性，为语言互通性研究提供了可靠的计算方法

Abstract: We present an analysis of mutual intelligibility in related languages applied for languages in the Romance family. We introduce a novel computational metric for estimating intelligibility based on lexical similarity using surface and semantic similarity of related words, and use it to measure mutual intelligibility for the five main Romance languages (French, Italian, Portuguese, Spanish, and Romanian), and compare results using both the orthographic and phonetic forms of words as well as different parallel corpora and vectorial models of word meaning representation. The obtained intelligibility scores confirm intuitions related to intelligibility asymmetry across languages and significantly correlate with results of cloze tests in human experiments.

</details>


### [224] [DLLM Agent: See Farther, Run Faster](https://arxiv.org/abs/2602.07451)
*Huiling Zhen,Weizhe Lin,Renxi Liu,Kai Han,Yiming Li,Yuchuan Tian,Hanting Chen,Xiaoguang Li,Xiaosong Li,Chen Chen,Xianzhi Yu,Mingxuan Yuan,Youliang Yan,Peifeng Qin,Jun Wang,Yu Wang,Dacheng Tao,Yunhe Wang*

Main category: cs.CL

TL;DR: Diffusion LLM代理相比自回归代理在相同准确率下平均端到端速度快30%以上，规划命中率更高，需要更少的交互轮次和工具调用，但需要更强的工具调用训练和注意力掩码对齐。


<details>
  <summary>Details</summary>
Motivation: 探索扩散大语言模型在智能体多步决策中的潜力，比较扩散与自回归解码范式在相同代理框架和监督下的规划与工具使用行为差异。

Method: 在相同代理工作流(DeepDiver)中实例化DLLM和AR主干网络，使用相同轨迹数据进行匹配的代理导向微调，生成可比较的扩散代理和自回归代理。

Result: DLLM代理在可比准确率下平均端到端速度快30%以上，部分案例超过8倍加速；正确完成任务时需要更少的交互轮次和工具调用，规划命中率更高，收敛更快且回溯更少。

Conclusion: 扩散主干在工具使用代理中具有效率优势，但需要解决工具调用失败和注意力掩码对齐问题，扩散代理显示出更强的全局规划信号。

Abstract: Diffusion large language models (DLLMs) have emerged as an alternative to autoregressive (AR) decoding with appealing efficiency and modeling properties, yet their implications for agentic multi-step decision making remain underexplored. We ask a concrete question: when the generation paradigm is changed but the agent framework and supervision are held fixed, do diffusion backbones induce systematically different planning and tool-use behaviors, and do these differences translate into end-to-end efficiency gains? We study this in a controlled setting by instantiating DLLM and AR backbones within the same agent workflow (DeepDiver) and performing matched agent-oriented fine-tuning on the same trajectory data, yielding diffusion-backed DLLM Agents and directly comparable AR agents. Across benchmarks and case studies, we find that, at comparable accuracy, DLLM Agents are on average over 30% faster end to end than AR agents, with some cases exceeding 8x speedup. Conditioned on correct task completion, DLLM Agents also require fewer interaction rounds and tool invocations, consistent with higher planner hit rates that converge earlier to a correct action path with less backtracking. We further identify two practical considerations for deploying diffusion backbones in tool-using agents. First, naive DLLM policies are more prone to structured tool-call failures, necessitating stronger tool-call-specific training to emit valid schemas and arguments. Second, for multi-turn inputs interleaving context and action spans, diffusion-style span corruption requires aligned attention masking to avoid spurious context-action information flow; without such alignment, performance degrades. Finally, we analyze attention dynamics across workflow stages and observe paradigm-specific coordination patterns, suggesting stronger global planning signals in diffusion-backed agents.

</details>


### [225] [SED-SFT: Selectively Encouraging Diversity in Supervised Fine-Tuning](https://arxiv.org/abs/2602.07464)
*Yijie Chen,Yijin Liu,Fandong Meng*

Main category: cs.CL

TL;DR: SED-SFT是一个针对大语言模型监督微调的新框架，通过引入选择性熵正则化和掩码机制来解决传统交叉熵损失导致的模式坍塌问题，显著提升生成多样性和后续强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于交叉熵损失的监督微调过程容易导致模式坍塌，模型过度集中于特定响应模式，缺乏分布多样性，严重限制了后续强化学习的探索效率。现有方法未能充分平衡多样性和准确性。

Method: 提出SED-SFT框架，基于token探索空间自适应地鼓励多样性。在优化目标中引入带有选择性掩码机制的选择性熵正则化项，以增加生成多样性。

Result: 在8个数学基准测试中，SED-SFT以可忽略的计算开销显著提升生成多样性，在Llama-3.2-3B-Instruct和Qwen2.5-Math-7B-Instruct模型上，后续RL性能分别比标准基线平均提升2.06和1.20个点。

Conclusion: SED-SFT有效解决了监督微调中的模式坍塌问题，在保持计算效率的同时显著提升了模型多样性和后续强化学习性能，为大语言模型后训练提供了更优的解决方案。

Abstract: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has emerged as the standard post-training paradigm for large language models (LLMs). However, the conventional SFT process, driven by Cross-Entropy (CE) loss, often induces mode collapse, where models over-concentrate on specific response patterns. This lack of distributional diversity severely restricts the exploration efficiency required for subsequent RL. While recent studies have attempted to improve SFT by replacing the CE loss, aiming to preserve diversity or refine the update policy, they fail to adequately balance diversity and accuracy, thereby yielding suboptimal performance after RL. To address the mode collapse problem, we propose SED-SFT, which adaptively encourages diversity based on the token exploration space. This framework introduces a selective entropy regularization term with a selective masking mechanism into the optimization objective. Extensive experiments across eight mathematical benchmarks demonstrate that SED-SFT significantly enhances generation diversity with a negligible computational overhead increase compared with CE loss, yielding average improvements of 2.06 and 1.20 points in subsequent RL performance over standard CE-based baselines on Llama-3.2-3B-Instruct and Qwen2.5-Math-7B-Instruct, respectively. The code is publicly available at https://github.com/pppa2019/SED-SFT

</details>


### [226] [From Native Memes to Global Moderation: Cros-Cultural Evaluation of Vision-Language Models for Hateful Meme Detection](https://arxiv.org/abs/2602.07497)
*Mo Wang,Kaixuan Ren,Pratik Jalan,Ahmed Ashraf,Tuong Vy Vu,Rahul Seetharaman,Shah Nawaz,Usman Naseem*

Main category: cs.CL

TL;DR: 系统评估框架分析视觉语言模型在多语言表情包检测中的跨文化鲁棒性，发现翻译后检测方法性能下降，而文化对齐干预（母语提示和单样本学习）显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型主要基于西方或英语中心视角训练，限制了其在仇恨表情包检测等任务中的公平性和跨文化鲁棒性，需要系统评估和改善这一局限性。

Method: 建立系统性评估框架，通过三个维度诊断最先进视觉语言模型的跨文化鲁棒性：学习策略（零样本vs单样本）、提示语言（母语vs英语）、翻译对意义和检测的影响。

Result: 结果显示常见的"翻译后检测"方法会降低性能，而文化对齐干预措施（母语提示和单样本学习）显著提高检测效果，同时发现模型系统性偏向西方安全规范。

Conclusion: 研究揭示了视觉语言模型存在的系统性文化偏见，并提供了可操作策略来减轻这种偏见，为设计全球鲁棒的多模态内容审核系统提供了指导。

Abstract: Cultural context profoundly shapes how people interpret online content, yet vision-language models (VLMs) remain predominantly trained through Western or English-centric lenses. This limits their fairness and cross-cultural robustness in tasks like hateful meme detection. We introduce a systematic evaluation framework designed to diagnose and quantify the cross-cultural robustness of state-of-the-art VLMs across multilingual meme datasets, analyzing three axes: (i) learning strategy (zero-shot vs. one-shot), (ii) prompting language (native vs. English), and (iii) translation effects on meaning and detection. Results show that the common ``translate-then-detect'' approach deteriorate performance, while culturally aligned interventions - native-language prompting and one-shot learning - significantly enhance detection. Our findings reveal systematic convergence toward Western safety norms and provide actionable strategies to mitigate such bias, guiding the design of globally robust multimodal moderation systems.

</details>


### [227] [Let's Simplify Step by Step: Guiding LLM Towards Multilingual Unsupervised Proficiency-Controlled Sentence Simplification](https://arxiv.org/abs/2602.07499)
*Jingshen Zhang,Xin Ying Qiu,Lifang Lu,Zhuhua Huang,Yutao Hu,Yuechang Wu,JunYu Lu*

Main category: cs.CL

TL;DR: 提出基于动态路径规划、语义感知示例选择和对话历史链式推理的分步简化框架，在五语言数据集上提升简化效果同时减少22-42%计算步骤，揭示了简化效果与语义保真度的基本权衡问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在跨大可读性级别的句子简化任务中表现有限，需要解决复杂简化的可控性问题

Method: 通过动态路径规划将复杂简化分解为可管理步骤，结合语义感知的示例选择和基于对话历史的链式思维生成来实现连贯推理

Result: 在两个基准测试的五种语言上，方法提高了简化效果并减少22-42%的计算步骤，人类评估确认了简化效果与意义保持之间的基本权衡

Conclusion: 分步简化方法提高了控制能力，但在广泛简化过程中保持语义保真度仍然是一个开放挑战，即使人类标注者也难以在语义保持判断上达成一致

Abstract: Large language models demonstrate limited capability in proficiency-controlled sentence simplification, particularly when simplifying across large readability levels. We propose a framework that decomposes complex simplifications into manageable steps through dynamic path planning, semantic-aware exemplar selection, and chain-of-thought generation with conversation history for coherent reasoning. Evaluation on five languages across two benchmarks shows our approach improves simplification effectiveness while reducing computational steps by 22-42%. Human evaluation confirms the fundamental trade-off between simplification effectiveness and meaning preservation. Notably, even human annotators struggle to agree on semantic preservation judgments, highlighting the inherent complexity of this task. Our work shows that while step-by-step simplification improves control, preserving semantic fidelity during extensive simplification remains an open challenge.

</details>


### [228] [Improving Variable-Length Generation in Diffusion Language Models via Length Regularization](https://arxiv.org/abs/2602.07546)
*Zicong Cheng,Ruixuan Jia,Jia Li,Guo-Wei Yang,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CL

TL;DR: LR-DLLM是一种长度正则化推理框架，通过显式处理生成长度变量，解决了扩散大语言模型在变长生成本中的系统性偏差问题，实现了可靠的生成长度确定。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）在固定长度画布上进行推理，隐含假设已知目标长度，导致在未知长度情况下（如补全和填充任务）无法可靠确定生成长度，产生系统性偏差。

Method: 提出LR-DLLM框架，将生成长度作为显式变量，通过长度正则化分离语义兼容性和长度诱导的不确定性，修正有偏的置信度估计，无需修改底层DLLM或其训练过程。

Result: 在HumanEvalInfilling任务上达到51.3% Pass@1（比DreamOn提升13.4%），在四语言McEval上平均达到51.5% Pass@1（比DreamOn提升14.3%）。

Conclusion: LR-DLLM有效解决了DLLMs在变长生成中的长度确定问题，通过显式的长度正则化实现了可靠的推理性能提升。

Abstract: Diffusion Large Language Models (DLLMs) are inherently ill-suited for variable-length generation, as their inference is defined on a fixed-length canvas and implicitly assumes a known target length. When the length is unknown, as in realistic completion and infilling, naively comparing confidence across mask lengths becomes systematically biased, leading to under-generation or redundant continuations. In this paper, we show that this failure arises from an intrinsic lengthinduced bias in generation confidence estimates, leaving existing DLLMs without a robust way to determine generation length and making variablelength inference unreliable. To address this issue, we propose LR-DLLM, a length-regularized inference framework for DLLMs that treats generation length as an explicit variable and achieves reliable length determination at inference time. It decouples semantic compatibility from lengthinduced uncertainty through an explicit length regularization that corrects biased confidence estimates. Based on this, LR-DLLM enables dynamic expansion or contraction of the generation span without modifying the underlying DLLM or its training procedure. Experiments show that LRDLLM achieves 51.3% Pass@1 on HumanEvalInfilling under fully unknown lengths (+13.4% vs. DreamOn) and 51.5% average Pass@1 on four-language McEval (+14.3% vs. DreamOn).

</details>


### [229] [Learning to Self-Verify Makes Language Models Better Reasoners](https://arxiv.org/abs/2602.07594)
*Yuxin Chen,Yu Wang,Yi Zhang,Ziang Ye,Zhengzhou Cai,Yaorui Shi,Qi Gu,Hui Su,Xunliang Cai,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 大语言模型在生成推理路径方面表现优异，但在自我验证方面存在能力不对称。研究发现提升生成能力不会自动改善验证能力，但学习自我验证能有效提升生成性能。作者提出了多任务强化学习框架，将生成和验证作为互补目标进行优化，实验证明该方法在生成和验证能力上均优于仅生成训练。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在复杂任务中展现出强大的生成能力，但它们在自我验证方面表现较弱，存在生成与验证能力的不对称性。本研究旨在深入探究这种不对称性，并探索如何通过整合自我验证来提升模型的整体性能。

Method: 采用多任务强化学习框架，将生成和自我验证作为两个独立但互补的优化目标。通过训练演化分析不对称性，并设计实验验证学习自我验证对生成性能的促进作用。

Result: 实验结果表明：1）提升生成能力不会相应改善自我验证能力；2）学习自我验证能有效提升生成性能，达到与标准生成训练相当的准确率；3）多任务强化学习框架在多个基准测试和模型上都显示出优于仅生成训练的性能提升。

Conclusion: 大语言模型的生成与验证能力存在显著不对称性，但通过将自我验证整合到训练过程中，可以同时提升生成和验证能力。多任务强化学习方法为解决这种不对称性提供了有效途径，能够产生更高效和有效的推理轨迹。

Abstract: Recent large language models (LLMs) achieve strong performance in generating promising reasoning paths for complex tasks. However, despite powerful generation ability, LLMs remain weak at verifying their own answers, revealing a persistent capability asymmetry between generation and self-verification. In this work, we conduct an in-depth investigation of this asymmetry throughout training evolution and show that, even on the same task, improving generation does not lead to corresponding improvements in self-verification. Interestingly, we find that the reverse direction of this asymmetry behaves differently: learning to self-verify can effectively improve generation performance, achieving accuracy comparable to standard generation training while yielding more efficient and effective reasoning traces. Building on this observation, we further explore integrating self-verification into generation training by formulating a multi-task reinforcement learning framework, where generation and self-verification are optimized as two independent but complementary objectives. Extensive experiments across benchmarks and models demonstrate performance gains over generation-only training in both generation and verification capabilities.

</details>


### [230] [SciClaimEval: Cross-modal Claim Verification in Scientific Papers](https://arxiv.org/abs/2602.07621)
*Xanh Ho,Yun-Ang Wu,Sunisth Kumar,Tian Cheng Xia,Florian Boudin,Andre Greiner-Petter,Akiko Aizawa*

Main category: cs.CL

TL;DR: SciClaimEval是一个新的科学声明验证数据集，包含从已发表论文中提取的真实声明（包括被反驳的声明），采用新颖的证据修改方法生成反驳案例，提供多模态证据格式，并在三个领域验证了11个多模态基础模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有科学声明验证数据集缺乏真实的被反驳声明，且证据生成方法不够真实。SciClaimEval旨在通过直接从已发表论文中提取真实声明，并采用创新的证据修改方法，创建更真实的科学声明验证基准。

Method: 通过修改支持证据（图表）而非直接修改声明或使用大语言模型生成矛盾内容来创建被反驳声明。数据集提供多模态证据表示：图像格式的图表，以及LaTeX源码、HTML和JSON格式的表格。从180篇论文中收集1,664个标注样本，涵盖机器学习、自然语言处理和医学三个领域。

Result: 评估了11个开源和专有多模态基础模型，结果显示所有模型在图基验证方面都面临显著挑战，最佳系统与人类基线之间存在明显的性能差距。

Conclusion: SciClaimEval为科学声明验证提供了更真实的基准数据集，揭示了当前多模态模型在处理图基证据验证方面的局限性，为该领域的研究提供了重要参考。

Abstract: We present SciClaimEval, a new scientific dataset for the claim verification task. Unlike existing resources, SciClaimEval features authentic claims, including refuted ones, directly extracted from published papers. To create refuted claims, we introduce a novel approach that modifies the supporting evidence (figures and tables), rather than altering the claims or relying on large language models (LLMs) to fabricate contradictions. The dataset provides cross-modal evidence with diverse representations: figures are available as images, while tables are provided in multiple formats, including images, LaTeX source, HTML, and JSON. SciClaimEval contains 1,664 annotated samples from 180 papers across three domains, machine learning, natural language processing, and medicine, validated through expert annotation. We benchmark 11 multimodal foundation models, both open-source and proprietary, across the dataset. Results show that figure-based verification remains particularly challenging for all models, as a substantial performance gap remains between the best system and human baseline.

</details>


### [231] [Letting Tutor Personas "Speak Up" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization](https://arxiv.org/abs/2602.07639)
*Jaewook Lee,Alexander Scarlatos,Simon Woodhead,Andrew Lan*

Main category: cs.CL

TL;DR: 本文提出了一种基于激活空间导向的方法，通过从人类导师-学生对话数据中学习导师人物特征，来指导大语言模型生成不同教学风格的回应，而无需显式提示指令。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的辅导系统通常学习单一导师策略，无法捕捉真实世界中导师教学风格的多样性。真实的导师-学生互动中，教学意图通过适应性教学策略实现，包括不同的脚手架水平、指导性、反馈和情感支持。

Method: 修改双向偏好优化（BiPO）方法，学习一个导向向量（steering vector）——激活空间中的方向，将模型响应导向特定的导师人物特征。该方法直接从人类对话数据中提取信号。

Result: 学习到的导向向量能够捕捉不同导师在对话上下文中的特定变化，提高了与真实导师话语的语义对齐度，增加了基于偏好的评估效果，同时基本保持了词汇相似性。

Conclusion: 激活空间导向提供了一种有效且可解释的方法，可以利用从人类对话数据中提取的信号来控制大语言模型中导师特定的变化，实现个性化的教学风格生成。

Abstract: With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.

</details>


### [232] [Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation](https://arxiv.org/abs/2602.07673)
*Jiangnan Fang,Cheng-Tse Liu,Hanieh Deilamsalehy,Nesreen K. Ahmed,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi*

Main category: cs.CL

TL;DR: LLM评委在摘要任务中存在对模型生成摘要的偏好偏差，且这种偏差随着与人类摘要相似度降低而增强，几乎所有测试模型都表现出此模式，需要超越简单比较的技术来改进LLM评委系统。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM评委在摘要任务中比传统算法指标更能捕捉语义信息且推理能力更强，但存在长度和顺序等偏差，且易受对抗性提示攻击。现有研究缺乏从细粒度重叠度量角度分析这些偏差。

Method: 使用ROUGE和BLEU作为重叠度量指标，测试9个参数量从10亿到120亿的近期LLM模型（包括Gemma 3和LLaMA 3变体），分析LLM评委偏好与人类撰写摘要重叠度的函数关系。

Result: 发现LLM评委越来越偏好其他LLM生成的摘要而非人类撰写的摘要，且这种偏好随着被评判摘要之间相似度的降低而增强；几乎所有测试模型都表现出此模式，且与模型自身的位置偏差无关；模型甚至难以判断重叠度有限的摘要。

Conclusion: 在摘要领域使用LLM作为评委需要依赖超越简单比较的技术，当前LLM评委系统存在系统性偏差，需要更复杂的评估方法来确保公平准确的评判。

Abstract: Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.

</details>


### [233] [SRR-Judge: Step-Level Rating and Refinement for Enhancing Search-Integrated Reasoning in Search Agents](https://arxiv.org/abs/2602.07773)
*Chen Zhang,Kuicai Dong,Dexun Li,Wenjun Li,Qu Yang,Wei Han,Yong Liu*

Main category: cs.CL

TL;DR: SRR-Judge框架通过提供细粒度的步骤级评估来改进深度搜索代理的推理和搜索动作质量，通过拒绝采样微调显著提升搜索性能


<details>
  <summary>Details</summary>
Motivation: 主流深度搜索方法仅使用结果监督训练，忽视了中间思维和动作的质量，需要可靠的步骤级评估机制

Method: 提出SRR-Judge评估框架，集成到改进的ReAct式评估-优化工作流中，使用迭代拒绝采样微调方法基于SRR标注数据训练代理

Result: SRR-Judge评估可靠性优于DeepSeek-V3.1等大型模型，评分与最终答案正确性强相关，代理性能在多个深度搜索基准上获得超过10%的平均绝对提升

Conclusion: SRR-Judge提供了有效的步骤级评估机制，通过细粒度监督显著提升了深度搜索代理的推理和搜索能力，证明了中间过程质量监控的重要性

Abstract: Recent deep search agents built on large reasoning models (LRMs) excel at complex question answering by iteratively planning, acting, and gathering evidence, a capability known as search-integrated reasoning. However, mainstream approaches often train this ability using only outcome-based supervision, neglecting the quality of intermediate thoughts and actions. We introduce SRR-Judge, a framework for reliable step-level assessment of reasoning and search actions. Integrated into a modified ReAct-style rate-and-refine workflow, SRR-Judge provides fine-grained guidance for search-integrated reasoning and enables efficient post-training annotation. Using SRR-annotated data, we apply an iterative rejection sampling fine-tuning procedure to enhance the deep search capability of the base agent. Empirically, SRR-Judge delivers more reliable step-level evaluations than much larger models such as DeepSeek-V3.1, with its ratings showing strong correlation with final answer correctness. Moreover, aligning the policy with SRR-Judge annotated trajectories leads to substantial performance gains, yielding over a 10 percent average absolute pass@1 improvement across challenging deep search benchmarks.

</details>


### [234] [Attn-GS: Attention-Guided Context Compression for Efficient Personalized LLMs](https://arxiv.org/abs/2602.07778)
*Shenglai Zeng,Tianqi Zheng,Chuan Tian,Dante Everaert,Yau-Shian Wang,Yupin Huang,Michael J. Morais,Rohit Patki,Jinjin Tian,Xinnan Dai,Kai Guo,Monica Xiao Cheng,Hui Liu*

Main category: cs.CL

TL;DR: Attn-GS：基于注意力机制的上下文压缩框架，通过分析LLM的注意力模式识别重要个性化信号，实现50倍token压缩的同时保持接近完整上下文的性能表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型个性化需要大量交互历史和用户档案，但输入token限制导致高延迟和高成本。现有启发式方法无法考虑LLM内部处理机制，需要更智能的上下文压缩方案。

Method: 提出Attn-GS框架：利用标记模型的注意力反馈识别重要个性化句子，指导压缩模型生成任务相关的高质量压缩上下文。基于研究发现：(a)LLM注意力模式自然揭示重要信号，(b)微调增强LLM区分相关信息能力。

Result: 在多种任务、token限制和设置下显著优于各种基线方法，token使用量减少50倍的同时达到接近完整上下文的性能水平。

Conclusion: 注意力模式是有效的个性化信号识别机制，Attn-GS框架为LLM个性化提供了高效且高质量的上下文压缩解决方案，解决了token限制带来的实际问题。

Abstract: Personalizing large language models (LLMs) to individual users requires incorporating extensive interaction histories and profiles, but input token constraints make this impractical due to high inference latency and API costs. Existing approaches rely on heuristic methods such as selecting recent interactions or prompting summarization models to compress user profiles. However, these methods treat context as a monolithic whole and fail to consider how LLMs internally process and prioritize different profile components. We investigate whether LLMs' attention patterns can effectively identify important personalization signals for intelligent context compression. Through preliminary studies on representative personalization tasks, we discover that (a) LLMs' attention patterns naturally reveal important signals, and (b) fine-tuning enhances LLMs' ability to distinguish between relevant and irrelevant information. Based on these insights, we propose Attn-GS, an attention-guided context compression framework that leverages attention feedback from a marking model to mark important personalization sentences, then guides a compression model to generate task-relevant, high-quality compressed user contexts. Extensive experiments demonstrate that Attn-GS significantly outperforms various baselines across different tasks, token limits, and settings, achieving performance close to using full context while reducing token usage by 50 times.

</details>


### [235] [Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models](https://arxiv.org/abs/2602.07794)
*Ningyu Xu,Qi Zhang,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: LLMs在上下文概念推理中会动态构建和使用结构化的潜在表示，这些表示在中间到深层形成概念子空间，并通过因果中介分析证明其功能重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究发现LLMs中存在类似人类的概念表示，但尚不清楚模型是否在推理过程中功能性地依赖这些表示。

Method: 使用因果中介分析研究LLMs在上下文概念推理中的内部处理过程，分析概念子空间的出现和功能作用。

Result: 发现概念子空间在中间到深层形成，其表示结构在不同上下文中保持稳定，且通过注意力机制在早期到中层整合上下文线索来构建和精炼该子空间。

Conclusion: LLMs确实会动态构建和使用结构化的潜在表示进行推理，这为理解模型灵活适应的计算过程提供了重要见解。

Abstract: Large language models (LLMs) exhibit emergent behaviors suggestive of human-like reasoning. While recent work has identified structured, human-like conceptual representations within these models, it remains unclear whether they functionally rely on such representations for reasoning. Here we investigate the internal processing of LLMs during in-context concept inference. Our results reveal a conceptual subspace emerging in middle to late layers, whose representational structure persists across contexts. Using causal mediation analyses, we demonstrate that this subspace is not merely an epiphenomenon but is functionally central to model predictions, establishing its causal role in inference. We further identify a layer-wise progression where attention heads in early-to-middle layers integrate contextual cues to construct and refine the subspace, which is subsequently leveraged by later layers to generate predictions. Together, these findings provide evidence that LLMs dynamically construct and use structured, latent representations in context for inference, offering insights into the computational processes underlying flexible adaptation.

</details>


### [236] [Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents](https://arxiv.org/abs/2602.07796)
*Jiatong Li,Changdae Oh,Hyeong Kyu Choi,Jindong Wang,Sharon Li*

Main category: cs.CL

TL;DR: 研究表明，在用户参与场景中强制LLM进行显式思考反而会降低性能，因为思考使代理变得'内向'，减少信息透露，削弱与用户的信息交换。通过提示信息透露可有效改善性能。


<details>
  <summary>Details</summary>
Motivation: 探索在真实用户参与场景中，显式思考技术对大型语言模型代理性能的实际影响，现有研究在此类场景中的有效性尚不明确。

Method: 使用7个模型、3个基准测试和2种思考实例化进行综合实验，通过定量响应分类分析和定性失败传播案例研究进行评估。

Result: 与预期相反，强制思考在用户参与设置中常常适得其反，导致各种LLM的性能异常下降。思考使代理响应变短、信息透露减少，削弱了代理与用户的信息交换。

Conclusion: 信息透明度意识是现实世界推理代理设计中至关重要但尚未充分探索的视角，主动透明度是代理优化的关键杠杆。

Abstract: Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.

</details>


### [237] [Pruning as a Cooperative Game: Surrogate-Assisted Layer Contribution Estimation for Large Language Models](https://arxiv.org/abs/2602.07804)
*Xuan Ding,Pengyu Tong,Ranjie Duan,Yunjian Zhang,Rui Sun,Yao Zhu*

Main category: cs.CL

TL;DR: 提出基于博弈论的轻量级代理网络框架，通过分层蒙特卡洛掩码采样估计层间贡献度，实现大语言模型的高效层剪枝。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝方法依赖静态启发式规则，无法捕捉层间依赖关系，限制了剪枝效果。大语言模型部署受高计算需求制约，需要更有效的压缩方法。

Method: 将层剪枝建模为合作博弈，每层作为玩家，模型性能作为效用函数。使用轻量代理网络预测任意层组合的性能，采用分层蒙特卡洛掩码采样降低Shapley值计算成本。

Result: 实验表明该方法在困惑度和零样本准确率上 consistently 优于现有方法，实现了更高效和有效的层剪枝。

Conclusion: 提出的博弈论框架能动态识别关键层，捕捉层间依赖关系，为大语言模型的高效部署提供了有效的层剪枝解决方案。

Abstract: While large language models (LLMs) demonstrate impressive performance across various tasks, their deployment in real-world scenarios is still constrained by high computational demands. Layer-wise pruning, a commonly employed strategy to mitigate inference costs, can partially address this challenge. However, existing approaches generally depend on static heuristic rules and fail to account for the interdependencies among layers, thereby limiting the effectiveness of the pruning process. To this end, this paper proposes a game-theoretic framework that formulates layer pruning as a cooperative game in which each layer acts as a player and model performance serves as the utility. As computing exact Shapley values is computationally infeasible for large language models (LLMs), we propose using a lightweight surrogate network to estimate layer-wise marginal contributions. This network can predict LLM performance for arbitrary layer combinations at a low computational cost. Additionally, we employ stratified Monte Carlo mask sampling to further reduce the cost of Sharpley value estimation. This approach captures inter-layer dependencies and dynamically identifies critical layers for pruning. Extensive experiments demonstrate the consistent superiority of our method in terms of perplexity and zero-shot accuracy, achieving more efficient and effective layer-wise pruning for large language models.

</details>


### [238] [LLMs Know More About Numbers than They Can Say](https://arxiv.org/abs/2602.07812)
*Fengting Yuchi,Li Du,Jason Eisner*

Main category: cs.CL

TL;DR: LLMs虽然能解决数学问题，但在混合表示法的数值比较中存在错误。研究发现隐藏状态能编码数值的对数幅度信息，线性分类器可达到90%以上排序准确率，但模型直接回答的准确率仅为50-70%。通过将探针分类器的对数损失作为辅助目标进行微调，可提升3.22%的数值推理能力。


<details>
  <summary>Details</summary>
Motivation: 发现先进LLMs在处理混合表示法（如科学记数法与常规数字）的数值比较时出现错误，探究模型是否真正理解数字的大小概念。

Method: 对多个开源LLMs的隐藏状态进行探针分析，使用线性投影提取数值的对数幅度信息，构建线性分类器进行数值排序，并通过微调将探针分类器的对数损失作为辅助目标。

Result: 隐藏状态能编码数值对数幅度（合成文本相对误差2.3%，科学论文19.06%）；线性分类器排序准确率>90%；模型直接回答准确率50-70%；微调后口头表达准确率提升3.22%。

Conclusion: LLMs内部确实编码了数值大小信息，但显式推理能力不足；通过改进内部数值表示可以增强数值推理能力，揭示了模型内部表示与外部表现之间的差距。

Abstract: Although state-of-the-art LLMs can solve math problems, we find that they make errors on numerical comparisons with mixed notation: "Which is larger, $5.7 \times 10^2$ or $580$?" This raises a fundamental question: Do LLMs even know how big these numbers are? We probe the hidden states of several smaller open-source LLMs. A single linear projection of an appropriate hidden layer encodes the log-magnitudes of both kinds of numerals, allowing us to recover the numbers with relative error of about 2.3% (on restricted synthetic text) or 19.06% (on scientific papers). Furthermore, the hidden state after reading a pair of numerals encodes their ranking, with a linear classifier achieving over 90% accuracy. Yet surprisingly, when explicitly asked to rank the same pairs of numerals, these LLMs achieve only 50-70% accuracy, with worse performance for models whose probes are less effective. Finally, we show that incorporating the classifier probe's log-loss as an auxiliary objective during finetuning brings an additional 3.22% improvement in verbalized accuracy over base models, demonstrating that improving models' internal magnitude representations can enhance their numerical reasoning capabilities.

</details>


### [239] [TodoEvolve: Learning to Architect Agent Planning Systems](https://arxiv.org/abs/2602.07839)
*Jiaxi Liu,Yanzuo Jiang,Guibin Zhang,Zihan Zhang,Heng Chang,Zhenfei Yin,Qibing Ren,Junchi Yan*

Main category: cs.CL

TL;DR: TodoEvolve是一个元规划范式，能够自主合成并动态修订任务特定的规划架构，通过PlanFactory模块化设计空间统一不同规划范式，并利用IGPO多目标强化学习训练Todo-14B模型，在多个智能体基准测试中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有规划方法主要依赖固定、手工设计的规划结构，缺乏适应开放性问题结构多样性的灵活性，需要能够自主适应不同任务结构的动态规划能力。

Method: 1. 构建PlanFactory模块化设计空间，统一拓扑结构、初始化、适应和导航等规划范式；2. 收集高质量规划轨迹；3. 通过阻抗引导偏好优化(IGPO)多目标强化学习训练Todo-14B模型，优化性能、稳定性和令牌效率。

Result: 在五个智能体基准测试中，TodoEvolve持续超越精心设计的规划模块，同时保持经济的API成本和运行时开销。

Conclusion: TodoEvolve通过元规划范式成功解决了固定规划结构的局限性，实现了对任意任务和智能体架构的自适应规划能力，为复杂长时域任务提供了灵活高效的解决方案。

Abstract: Planning has become a central capability for contemporary agent systems in navigating complex, long-horizon tasks, yet existing approaches predominantly rely on fixed, hand-crafted planning structures that lack the flexibility to adapt to the structural diversity of open-ended problems. To address this limitation, we introduce TodoEvolve, a meta-planning paradigm that autonomously synthesizes and dynamically revises task-specific planning architectures. Specifically, we first construct PlanFactory, a modular design space that standardizes diverse planning paradigms within a unified codebase encompassing topology, initialization, adaptation, and navigation, thereby providing a common interface for heterogeneous planning patterns. Leveraging PlanFactory, we collect high-quality planning trajectories and train Todo-14B via \textit{Impedance-Guided Preference Optimization} (IGPO), a multi-objective reinforcement learning objective that encourages the generation of planning systems that are performant, stable, and token-efficient across arbitrary tasks and agent backbones. Empirical evaluations on five agentic benchmarks demonstrate that TodoEvolve consistently surpasses carefully engineered planning modules while maintaining economical API costs and runtime overhead.

</details>


### [240] [Evaluating and Calibrating LLM Confidence on Questions with Multiple Correct Answers](https://arxiv.org/abs/2602.07842)
*Yuhan Wang,Shiyu Ni,Zhikai Ding,Zihang Zhan,Yuanzi Li,Keping Bi*

Main category: cs.CL

TL;DR: 论文提出了MACE基准来研究多答案场景下的LLM置信度校准问题，发现现有方法在多个有效答案存在时会系统性低估置信度，并提出了SCA方法来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有训练无关的置信度校准方法主要针对单答案问答场景研究，但在存在多个有效答案时，这些方法会因正确答案之间的分歧而导致置信度系统性低估，需要系统研究这一现象。

Method: 引入MACE基准（包含12,000个事实性问题，覆盖6个领域和不同数量的正确答案），测试15种代表性校准方法和4个LLM家族（7B-72B），并提出语义置信度聚合（SCA）方法，通过对多个高概率采样响应的置信度进行聚合。

Result: 实验显示准确率随答案基数增加而提高，但估计置信度持续下降，导致混合答案数量问题的严重校准错误。SCA在混合答案设置下实现了最先进的校准性能，同时在单答案问题上保持强校准能力。

Conclusion: 多答案场景对LLM置信度校准提出了新挑战，SCA方法有效解决了现有方法在混合答案设置下的校准问题，为可靠的多答案LLM应用提供了解决方案。

Abstract: Confidence calibration is essential for making large language models (LLMs) reliable, yet existing training-free methods have been primarily studied under single-answer question answering. In this paper, we show that these methods break down in the presence of multiple valid answers, where disagreement among equally correct responses leads to systematic underestimation of confidence. To enable a systematic study of this phenomenon, we introduce MACE, a benchmark of 12,000 factual questions spanning six domains with varying numbers of correct answers. Experiments across 15 representative calibration methods and four LLM families (7B-72B) reveal that while accuracy increases with answer cardinality, estimated confidence consistently decreases, causing severe miscalibration for questions with mixed answer counts. To address this issue, we propose Semantic Confidence Aggregation (SCA), which aggregates confidence over multiple high-probability sampled responses. SCA achieves state-of-the-art calibration performance under mixed-answer settings while preserving strong calibration on single-answer questions.

</details>


### [241] [SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization](https://arxiv.org/abs/2602.07909)
*Taolin Zhang,Hang Guo,Wang Lu,Tao Dai,Shu-Tao Xia,Jindong Wang*

Main category: cs.CL

TL;DR: SparseEval是一种高效的大型语言模型基准测试方法，通过稀疏优化和梯度下降选择代表性锚点项目来降低评估成本。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，完整基准测试的计算成本急剧增加，需要开发更高效的评估方法。

Method: 利用模型-项目性能矩阵的稀疏性，提出梯度下降优化锚点权重和迭代精化策略，使用MLP处理稀疏优化，并引入锚点重要性评分和候选重要性评分。

Result: 实验显示该方法在多个基准测试中具有低估计误差和高Kendall τ相关性，表现出优异的鲁棒性和实用性。

Conclusion: SparseEval为大规模语言模型的高效评估提供了有效解决方案，通过稀疏优化显著降低了计算成本同时保持了评估准确性。

Abstract: As large language models (LLMs) continue to scale up, their performance on various downstream tasks has significantly improved. However, evaluating their capabilities has become increasingly expensive, as performing inference on a large number of benchmark samples incurs high computational costs. In this paper, we revisit the model-item performance matrix and show that it exhibits sparsity, that representative items can be selected as anchors, and that the task of efficient benchmarking can be formulated as a sparse optimization problem. Based on these insights, we propose SparseEval, a method that, for the first time, adopts gradient descent to optimize anchor weights and employs an iterative refinement strategy for anchor selection. We utilize the representation capacity of MLP to handle sparse optimization and propose the Anchor Importance Score and Candidate Importance Score to evaluate the value of each item for task-aware refinement. Extensive experiments demonstrate the low estimation error and high Kendall's~$τ$ of our method across a variety of benchmarks, showcasing its superior robustness and practicality in real-world scenarios. Code is available at {https://github.com/taolinzhang/SparseEval}.

</details>


### [242] [Patches of Nonlinearity: Instruction Vectors in Large Language Models](https://arxiv.org/abs/2602.07930)
*Irina Bigoulaeva,Jonas Rohweder,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本研究从机制解释角度探究指令调优语言模型如何处理指令，发现指令表示在模型中高度局部化（称为指令向量IVs），同时表现出线性可分性和非线性因果交互的独特特性，挑战了机制可解释性中常见的线性表示假设。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优语言模型取得了成功并被广泛使用，但对其内部如何处理指令的机制了解甚少，需要从机制角度研究指令特定表示在监督微调(SFT)和直接偏好优化(DPO)阶段如何构建和利用。

Method: 通过因果中介分析识别指令表示，提出一种新颖的定位语言模型信息处理的方法，该方法不受基于补丁技术的隐式线性假设限制，分析不同层中信息路径的选择机制。

Result: 发现指令表示在模型中高度局部化，指令向量IVs表现出线性可分性与非线性因果交互并存的特征，早期层形成任务表示后，后期层选择不同信息路径来解决任务，IVs充当电路选择器。

Conclusion: 指令向量作为电路选择器的发现挑战了机制可解释性中的线性表示假设，为理解指令调优模型内部工作机制提供了新视角，提出的非线性因果分析方法为后续研究提供了新工具。

Abstract: Despite the recent success of instruction-tuned language models and their ubiquitous usage, very little is known of how models process instructions internally. In this work, we address this gap from a mechanistic point of view by investigating how instruction-specific representations are constructed and utilized in different stages of post-training: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Via causal mediation, we identify that instruction representation is fairly localized in models. These representations, which we call Instruction Vectors (IVs), demonstrate a curious juxtaposition of linear separability along with non-linear causal interaction, broadly questioning the scope of the linear representation hypothesis commonplace in mechanistic interpretability. To disentangle the non-linear causal interaction, we propose a novel method to localize information processing in language models that is free from the implicit linear assumptions of patching-based techniques. We find that, conditioned on the task representations formed in the early layers, different information pathways are selected in the later layers to solve that task, i.e., IVs act as circuit selectors.

</details>


### [243] [Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation](https://arxiv.org/abs/2602.07954)
*Krzysztof Wróbel,Jan Maria Kowalski,Jerzy Surma,Igor Ciuciura,Maciej Szymański*

Main category: cs.CL

TL;DR: Bielik Guard是一个波兰语内容安全分类器家族，包含0.1B和0.5B两个参数规模的模型变体，专门用于检测波兰文本中的仇恨言论、粗俗内容、性内容、犯罪和自残等五类不安全内容。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在波兰语应用中的部署增加，迫切需要高效准确的内容安全分类器来确保内容安全。

Method: 基于MMLW-RoBERTa-base和PKOBP/polish-roberta-8k架构，使用6,885个社区标注的波兰文本进行微调，开发了两个参数规模不同的模型变体。

Result: 0.5B模型在测试集上获得0.791（微平均）和0.785（宏平均）的F1分数；0.1B模型在真实用户提示上达到77.65%的精确度和0.63%的低误报率，优于同规模的HerBERT-PL-Guard模型。

Conclusion: Bielik Guard系列模型在波兰语内容安全分类任务中表现出色，提供了从高效到高性能的不同选择，特别注重对自残等敏感类别提供适当回应而非简单屏蔽。

Abstract: As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65\%) and very low false positive rate (0.63\%) on real user prompts, outperforming HerBERT-PL-Guard (31.55\% precision, 4.70\% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.

</details>


### [244] [Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms](https://arxiv.org/abs/2602.07963)
*Vaibhav Shukla,Hardik Sharma,Adith N Reganti,Soham Wasmatkar,Bagesh Kumar,Vrijendra Singh*

Main category: cs.CL

TL;DR: CompositeHarm是一个基于翻译的多语言安全评估基准，结合AttaQ和MMSafetyBench数据集，在六种语言中测试LLM的安全对齐效果，发现对抗性攻击在印度语言中成功率显著上升，而上下文危害转移较为温和。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估主要基于英语，翻译方法无法完整捕捉跨语言的有害意图和结构变化，需要系统研究安全对齐在不同语言中的表现。

Method: 创建CompositeHarm基准，整合两个英文数据集（AttaQ针对结构化对抗攻击，MMSafetyBench覆盖上下文现实危害），扩展到六种语言（英语、印地语、阿萨姆语、马拉地语、卡纳达语、古吉拉特语），采用轻量级推理策略减少计算冗余。

Result: 在印度语言中，对抗性语法攻击的成功率急剧上升，上下文危害的转移相对温和，表明翻译基准是必要但不充分的步骤。

Conclusion: 翻译基准是构建接地气、资源感知、语言自适应安全系统的必要第一步，但需要更全面的多语言安全测试方法。

Abstract: Most safety evaluations of large language models (LLMs) remain anchored in English. Translation is often used as a shortcut to probe multilingual behavior, but it rarely captures the full picture, especially when harmful intent or structure morphs across languages. Some types of harm survive translation almost intact, while others distort or disappear. To study this effect, we introduce CompositeHarm, a translation-based benchmark designed to examine how safety alignment holds up as both syntax and semantics shift. It combines two complementary English datasets, AttaQ, which targets structured adversarial attacks, and MMSafetyBench, which covers contextual, real-world harms, and extends them into six languages: English, Hindi, Assamese, Marathi, Kannada, and Gujarati. Using three large models, we find that attack success rates rise sharply in Indic languages, especially under adversarial syntax, while contextual harms transfer more moderately. To ensure scalability and energy efficiency, our study adopts lightweight inference strategies inspired by edge-AI design principles, reducing redundant evaluation passes while preserving cross-lingual fidelity. This design makes large-scale multilingual safety testing both computationally feasible and environmentally conscious. Overall, our results show that translated benchmarks are a necessary first step, but not a sufficient one, toward building grounded, resource-aware, language-adaptive safety systems.

</details>


### [245] [Cross-Linguistic Persona-Driven Data Synthesis for Robust Multimodal Cognitive Decline Detection](https://arxiv.org/abs/2602.07978)
*Rui Feng,Zhiyao Luo,Liuyu Wu,Wei Wang,Yuting Song,Yong Liu,Kok Pin Ng,Jianqing Li,Xingyao Wang*

Main category: cs.CL

TL;DR: SynCog框架通过可控零样本多模态数据合成和思维链推理微调，解决MCI诊断中的数据稀缺和模型可解释性问题，在多语言基准测试中表现优异并展现良好跨语言泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决基于语音的MCI诊断中临床数据稀缺、跨语言泛化能力不足以及模型缺乏可解释性等关键挑战

Method: 提出SynCog框架：1) 可控零样本多模态数据合成生成虚拟受试者数据缓解数据稀缺；2) 思维链推理微调策略使模型能够明确表达诊断推理过程

Result: 在ADReSS和ADReSSo基准测试中分别达到80.67%和78.46%的Macro-F1分数，优于基线模型；在独立中文数据集CIR-E上获得48.71%的Macro-F1，展现跨语言泛化能力

Conclusion: 该研究为全球医疗保健提供了临床可信赖且语言包容的认知评估工具的重要进展，通过合成数据增强和可解释AI方法有效解决了MCI诊断的关键瓶颈

Abstract: Speech-based digital biomarkers represent a scalable, non-invasive frontier for the early identification of Mild Cognitive Impairment (MCI). However, the development of robust diagnostic models remains impeded by acute clinical data scarcity and a lack of interpretable reasoning. Current solutions frequently struggle with cross-lingual generalization and fail to provide the transparent rationales essential for clinical trust. To address these barriers, we introduce SynCog, a novel framework integrating controllable zero-shot multimodal data synthesis with Chain-of-Thought (CoT) deduction fine-tuning. Specifically, SynCog simulates diverse virtual subjects with varying cognitive profiles to effectively alleviate clinical data scarcity. This generative paradigm enables the rapid, zero-shot expansion of clinical corpora across diverse languages, effectively bypassing data bottlenecks in low-resource settings and bolstering the diagnostic performance of Multimodal Large Language Models (MLLMs). Leveraging this synthesized dataset, we fine-tune a foundational multimodal backbone using a CoT deduction strategy, empowering the model to explicitly articulate diagnostic thought processes rather than relying on black-box predictions. Extensive experiments on the ADReSS and ADReSSo benchmarks demonstrate that augmenting limited clinical data with synthetic phenotypes yields competitive diagnostic performance, achieving Macro-F1 scores of 80.67% and 78.46%, respectively, outperforming current baseline models. Furthermore, evaluation on an independent real-world Mandarin cohort (CIR-E) demonstrates robust cross-linguistic generalization, attaining a Macro-F1 of 48.71%. These findings constitute a critical step toward providing clinically trustworthy and linguistically inclusive cognitive assessment tools for global healthcare.

</details>


### [246] [The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation](https://arxiv.org/abs/2602.07996)
*Arash Marioriyad,Omid Ghahroodi,Ehsaneddin Asgari,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: LLM作为自动评估器时，虽然判决结果受无关上下文线索（如来源、时间、年龄等元数据标签）显著影响，但在解释理由中却很少承认这些线索的存在，存在解释差距问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLM作为自动评估器的忠实性，测试其判决是否仅基于内容质量、是否对无关上下文保持不变性，以及是否透明反映决策因素。

Method: 通过控制线索扰动（注入合成元数据标签），测试6个LLM模型（GPT-4o、Gemini-2.0-Flash等）在两个数据集（ELI5事实问答和LitBench创意写作）上的表现，研究6类线索族（来源、时间、年龄等），并引入判决转移率（VSR）和线索承认率（CAR）指标。

Result: 发现LLM判决对线索有显著敏感性（如专家>人类>LLM>未知的来源层级、新>旧的时效偏好、教育地位偏爱），但CAR通常接近零，表明即使线索驱动决策，也很少在解释中被承认。CAR还依赖于数据集，在事实性ELI5中某些线索更可能被明确承认，但在开放性LitBench中即使判决大幅转移也几乎不承认线索。

Conclusion: LLM作为评估器存在明显的解释差距，判决敏感性与有限的线索承认相结合，揭示了基于模型的评估在研究部署中的可靠性问题。

Abstract: Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.

</details>


### [247] [DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity](https://arxiv.org/abs/2602.08005)
*Jitai Hao,Qiang Huang,Yaowei Wang,Min Zhang,Jun Yu*

Main category: cs.CL

TL;DR: DeltaKV是一种基于残差的KV缓存压缩框架，结合Sparse-vLLM推理引擎，在保持近乎无损精度的同时将KV缓存内存减少至原始的29%，并在长上下文场景中实现2倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM部署受KV缓存内存线性增长的瓶颈限制，现有压缩和驱逐方法难以平衡准确性、压缩比和硬件效率。

Method: 基于长距离令牌间相似性和KV表示中高度共享的潜在组件的经验发现，DeltaKV通过编码相对于检索历史参考的语义残差来压缩KV缓存。Sparse-vLLM提供解耦内存管理和针对稀疏不规则KV布局优化的内核。

Result: 在LongBench、SCBench和AIME基准测试中，DeltaKV将KV缓存内存减少至原始的29%同时保持近乎无损精度。与Sparse-vLLM集成后，在长上下文场景中相比vLLM实现高达2倍的吞吐量提升。

Conclusion: DeltaKV和Sparse-vLLM为可扩展的长上下文LLM部署提供了实用路径，通过残差压缩和专用推理引擎有效解决了KV缓存内存瓶颈问题。

Abstract: The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.

</details>


### [248] [Diverge to Induce Prompting: Multi-Rationale Induction for Zero-Shot Reasoning](https://arxiv.org/abs/2602.08028)
*Po-Chun Chen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: DIP框架通过生成多样化高层次理性思路，将其细化为详细分步计划，再归纳为最终计划，提升零样本推理准确性，无需依赖资源密集型采样。


<details>
  <summary>Details</summary>
Motivation: 解决标准思维链提示中非引导推理路径的不稳定性，以及单一策略方法在多样化任务中性能受限的问题。

Method: 提出Diverge-to-Induce Prompting (DIP)框架：1) 为每个问题生成多个多样化高层次理性思路；2) 将每个理性思路细化为详细的分步草稿计划；3) 将这些草稿计划归纳为最终计划。

Result: 实验表明DIP优于单一策略提示方法，证明了多计划归纳在基于提示的推理中的有效性。

Conclusion: DIP框架通过多策略生成和归纳的方法，有效提升了大型语言模型在零样本推理任务中的准确性和稳定性。

Abstract: To address the instability of unguided reasoning paths in standard Chain-of-Thought prompting, recent methods guide large language models (LLMs) by first eliciting a single reasoning strategy. However, relying on just one strategy for each question can still limit performance across diverse tasks. We propose Diverge-to-Induce Prompting (DIP), a framework that first prompts an LLM to generate multiple diverse high-level rationales for each question. Each rationale is then elaborated into a detailed, step-by-step draft plan. Finally, these draft plans are induced into a final plan. DIP enhances zero-shot reasoning accuracy without reliance on resource-intensive sampling. Experiments show that DIP outperforms single-strategy prompting, demonstrating the effectiveness of multi-plan induction for prompt-based reasoning.

</details>


### [249] [Beyond Raw Detection Scores: Markov-Informed Calibration for Boosting Machine-Generated Text Detection](https://arxiv.org/abs/2602.08031)
*Chenwang Wu,Yiu-ming Cheung,Shuhai Zhang,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 论文提出了一种基于马尔可夫随机场的分数校准策略，用于改进机器生成文本的检测方法，通过建模邻域相似性和初始不稳定性关系来缓解检测分数偏差问题。


<details>
  <summary>Details</summary>
Motivation: 机器生成文本检测面临核心挑战：基于度量的方法中token级检测分数容易受到生成过程随机性的影响而产生偏差，需要有效的校准策略。

Method: 提出马尔可夫信息分数校准策略，使用马尔可夫随机场建模邻域相似性和初始不稳定性关系，并通过平均场近似实现轻量级集成。

Result: 在各种实际场景（跨LLM和释义攻击）的广泛实验中，该方法相比基线获得显著提升，计算开销可忽略不计。

Conclusion: 该方法能无缝集成到现有检测器中，有效解决机器生成文本检测中的分数偏差问题，提升检测性能。

Abstract: While machine-generated texts (MGTs) offer great convenience, they also pose risks such as disinformation and phishing, highlighting the need for reliable detection. Metric-based methods, which extract statistically distinguishable features of MGTs, are often more practical than complex model-based methods that are prone to overfitting. Given their diverse designs, we first place representative metric-based methods within a unified framework, enabling a clear assessment of their advantages and limitations. Our analysis identifies a core challenge across these methods: the token-level detection score is easily biased by the inherent randomness of the MGTs generation process. To address this, we theoretically and empirically reveal two relationships of context detection scores that may aid calibration: Neighbor Similarity and Initial Instability. We then propose a Markov-informed score calibration strategy that models these relationships using Markov random fields, and implements it as a lightweight component via a mean-field approximation, allowing our method to be seamlessly integrated into existing detectors. Extensive experiments in various real-world scenarios, such as cross-LLM and paraphrasing attacks, demonstrate significant gains over baselines with negligible computational overhead. The code is available at https://github.com/tmlr-group/MRF_Calibration.

</details>


### [250] [TDGNet: Hallucination Detection in Diffusion Language Models via Temporal Dynamic Graphs](https://arxiv.org/abs/2602.08048)
*Arshia Hemmat,Philip Torr,Yongqiang Chen,Junchi Yu*

Main category: cs.CL

TL;DR: TDGNet是一个时态动态图框架，用于检测扩散语言模型中的幻觉问题，通过分析去噪过程中的注意力图演化来实现鲁棒的幻觉检测


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具有并行去噪和双向上下文优势，但现有的自回归模型幻觉检测方法无法直接应用于扩散生成，因为事实证据分布在去噪轨迹中并随时间动态变化

Method: 提出TDGNet框架，将幻觉检测建模为对演化中的token级注意力图的学习。在每个去噪步骤中稀疏化注意力图并通过消息传递更新token记忆，使用时态注意力聚合轨迹范围内的证据进行最终预测

Result: 在LLaDA-8B和Dream-7B模型上的QA基准测试显示，TDGNet在AUROC指标上持续优于基于输出、潜在表示和静态图的基线方法，具有单次推理和适度的计算开销

Conclusion: 研究结果表明，对注意力图进行时态推理对于扩散语言模型中的鲁棒幻觉检测至关重要，TDGNet为这一领域提供了有效的解决方案

Abstract: Diffusion language models (D-LLMs) offer parallel denoising and bidirectional context, but hallucination detection for D-LLMs remains underexplored. Prior detectors developed for auto-regressive LLMs typically rely on single-pass cues and do not directly transfer to diffusion generation, where factuality evidence is distributed across the denoising trajectory and may appear, drift, or be self-corrected over time. We introduce TDGNet, a temporal dynamic graph framework that formulates hallucination detection as learning over evolving token-level attention graphs. At each denoising step, we sparsify the attention graph and update per-token memories via message passing, then apply temporal attention to aggregate trajectory-wide evidence for final prediction. Experiments on LLaDA-8B and Dream-7B across QA benchmarks show consistent AUROC improvements over output-based, latent-based, and static-graph baselines, with single-pass inference and modest overhead. These results highlight the importance of temporal reasoning on attention graphs for robust hallucination detection in diffusion language models.

</details>


### [251] [Emergent Search and Backtracking in Latent Reasoning Models](https://arxiv.org/abs/2602.08100)
*Jasmine Cui,Charles Ye*

Main category: cs.CL

TL;DR: 该研究分析了潜在推理变换器(LRT)在隐藏空间中进行多步推理的过程，发现模型自发学习结构化搜索模式，包括探索、初步承诺、收敛或回溯等阶段，回溯行为对准确性有显著提升。


<details>
  <summary>Details</summary>
Motivation: 研究标准LLM通过思维链进行语言化推理，而LRT在连续隐藏空间中进行潜在推理，需要探索这种无词汇推理过程的机制和特点。

Method: 在多项选择QA基准测试中解码LRT模型每一步演化的信念，分析其在隐藏空间中的搜索过程和决策轨迹。

Result: 发现模型自发学习结构化搜索过程，包含探索阶段、初步承诺、收敛或回溯；回溯行为普遍(32%)且有益(准确率提升34%)；搜索具有适应性，替换干扰项可缩短探索时间54%。

Conclusion: 潜在推理模型在激活空间中实现了思维链通过词汇达到的能力：能够犯错、察觉错误并进行恢复，揭示了无词汇推理的内在机制。

Abstract: What happens when a language model thinks without words? Standard reasoning LLMs verbalize intermediate steps as chain-of-thought; latent reasoning transformers (LRTs) instead perform deliberation entirely in continuous hidden space. We investigate an LRT, decoding the model's evolving beliefs at every step on a multiple-choice QA benchmark. We find that the model spontaneously learns a structured search process in latent space. Deliberation follows a consistent trajectory: an exploration phase where probability mass spreads across candidates, tentative commitment to a frontrunner, and either convergence or backtracking. Backtracking is prevalent (32% of instances), beneficial (34% accuracy gain over non-backtracking instances), and predominantly directed away from the semantically closest distractor toward the correct answer. The search is adaptive: replacing distractors with implausible alternatives shortens exploration by 54%. Latent reasoning models achieve in activation space what chain-of-thought achieves through words: the ability to be wrong, notice, and recover.

</details>


### [252] [Gender and Race Bias in Consumer Product Recommendations by Large Language Models](https://arxiv.org/abs/2602.08124)
*Ke Xu,Shera Potka,Alex Thomo*

Main category: cs.CL

TL;DR: 研究首次系统分析LLM产品推荐中的性别和种族偏见，通过提示工程生成不同人口群体的推荐，使用三种分析方法发现显著偏见差异


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在消费产品推荐中应用日益广泛，但其可能嵌入和放大性别与种族偏见的风险尚未得到充分研究

Method: 采用提示工程从LLM获取不同种族和性别群体的产品推荐，运用标记词分析、支持向量机和Jensen-Shannon散度三种方法进行偏见识别和量化

Result: 研究发现不同人口群体在推荐结果中存在显著差异，揭示了LLM推荐系统中的偏见问题

Conclusion: LLM推荐系统存在明显的性别和种族偏见，亟需开发更公平的推荐机制以确保算法公正性

Abstract: Large Language Models are increasingly employed in generating consumer product recommendations, yet their potential for embedding and amplifying gender and race biases remains underexplored. This paper serves as one of the first attempts to examine these biases within LLM-generated recommendations. We leverage prompt engineering to elicit product suggestions from LLMs for various race and gender groups and employ three analytical methods-Marked Words, Support Vector Machines, and Jensen-Shannon Divergence-to identify and quantify biases. Our findings reveal significant disparities in the recommendations for demographic groups, underscoring the need for more equitable LLM recommendation systems.

</details>


### [253] [DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries](https://arxiv.org/abs/2602.08149)
*Sahana Ramnath,Nima Chitsazan,Mingyang Zhou,Chia-Hsuan Lee,Shi-Xiong Zhang,Stephen Rawls,Sambit Sahu,Sangwoo Cho,Xiang Ren,Genta Indra Winata,Akshaj Kumar Veldanda*

Main category: cs.CL

TL;DR: DIALSUMMER是一个对话摘要评估框架，通过分层错误分类体系（对话级和话轮内级）解决对话摘要的结构转换和视角转换问题，并构建了人工标注数据集来评估LLM的错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有对话摘要评估方法忽略了对话到摘要的两个关键复杂性转变：从多说话人分散讨论到结构化句子的结构转换，以及从第一/二人称到标准化第三人称的叙述视角转换。

Method: 提出DIAL-SUMMER框架，包含两层次错误分类体系（DIALOGUE-LEVEL和WITHIN-TURN-LEVEL），构建人工标注的细粒度错误数据集，并进行实证分析和LLM错误检测能力实验。

Result: 发现有趣趋势：对话中间的话轮最容易被遗漏，外部幻觉主要出现在摘要末尾；实验显示当前LLM在错误检测方面表现具有挑战性，验证了分类体系的鲁棒性。

Conclusion: DIALSUMMER为对话摘要评估提供了全面框架，揭示了现有方法的局限性，证明了未来需要提升LLM在此任务上的性能，代码和推理数据集即将发布。

Abstract: Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.

</details>


### [254] [NLP for Local Governance Meeting Records: A Focus Article on Tasks, Datasets, Metrics and Benchmark](https://arxiv.org/abs/2602.08162)
*Ricardo Campos,José Pedro Evans,José Miguel Isidro,Miguel Marques,Luís Filipe Cunha,Alípio Jorge,Sérgio Nunes,Nuno Guimarães*

Main category: cs.CL

TL;DR: 本文综述了自然语言处理技术在地方政府会议记录结构化处理中的应用，重点分析了文档分割、领域特定实体提取和自动文本摘要三个核心任务，旨在提高政府文档的可访问性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 地方政府会议记录作为官方文件，通常结构复杂、语言官僚且存在显著的异质性，这使得非专业人士难以解读，智能自动化系统也难以处理，限制了公众透明度和公民参与度。

Method: 采用文献综述方法，系统回顾了支持地方政府会议文档结构化的基础NLP任务，包括文档分割、领域特定实体提取和自动文本摘要，讨论了方法论、评估指标和可用资源。

Result: 通过综合现有研究成果，提供了NLP如何增强地方政府会议记录结构化和可访问性的结构化概述，识别了数据稀缺性、隐私约束和来源变异性等特定领域挑战。

Conclusion: 自然语言处理技术能够有效解决地方政府会议记录处理中的异质性和复杂性挑战，为提高政府透明度和公民参与提供了可行的技术路径，但需要克服数据获取和隐私保护等实际应用障碍。

Abstract: Local governance meeting records are official documents, in the form of minutes or transcripts, documenting how proposals, discussions, and procedural actions unfold during institutional meetings. While generally structured, these documents are often dense, bureaucratic, and highly heterogeneous across municipalities, exhibiting significant variation in language, terminology, structure, and overall organization. This heterogeneity makes them difficult for non-experts to interpret and challenging for intelligent automated systems to process, limiting public transparency and civic engagement. To address these challenges, computational methods can be employed to structure and interpret such complex documents. In particular, Natural Language Processing (NLP) offers well-established methods that can enhance the accessibility and interpretability of governmental records. In this focus article, we review foundational NLP tasks that support the structuring of local governance meeting documents. Specifically, we review three core tasks: document segmentation, domain-specific entity extraction and automatic text summarization, which are essential for navigating lengthy deliberations, identifying political actors and personal information, and generating concise representations of complex decision-making processes. In reviewing these tasks, we discuss methodological approaches, evaluation metrics, and publicly available resources, while highlighting domain-specific challenges such as data scarcity, privacy constraints, and source variability. By synthesizing existing work across these foundational tasks, this article provides a structured overview of how NLP can enhance the structuring and accessibility of local governance meeting records.

</details>


### [255] [LLMs and people both learn to form conventions -- just not with each other](https://arxiv.org/abs/2602.08208)
*Cameron R. Jones,Agnese Lombardi,Kyle Mahowald,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 研究探讨了LLM在多模态交流游戏中是否像人类一样形成交流惯例。研究发现同类型对话对（人类-人类、AI-AI）都能形成惯例，但人类-AI混合对却失败，表明存在沟通倾向差异。即使通过提示让LLM模仿人类行为，人类-LLM对的准确性和词汇重叠度仍落后于同类型对。


<details>
  <summary>Details</summary>
Motivation: 检验LLM是否能在多模态交流中像人类一样形成共享的交流惯例，以及探索人类与AI在沟通中的兼容性问题。

Method: 采用多模态交流游戏实验设计，比较同类型对话对（人类-人类、AI-AI）和混合对话对（人类-AI）的表现。实验2通过提示工程让LLM模仿人类行为特征。

Result: 同类型对话对显示出惯例形成迹象（准确性提高、一致性增强、信息长度减少），但人类-AI混合对未能形成有效惯例。即使LLM模仿人类消息长度，其准确性和词汇重叠度仍然较差。

Conclusion: 对话对齐不仅需要模仿先前互动的能力，还需要共享对传达含义的解释偏见。LLM与人类在沟通倾向上的根本差异限制了混合对话对的惯例形成能力。

Abstract: Humans align to one another in conversation -- adopting shared conventions that ease communication. We test whether LLMs form the same kinds of conventions in a multimodal communication game. Both humans and LLMs display evidence of convention-formation (increasing the accuracy and consistency of their turns while decreasing their length) when communicating in same-type dyads (humans with humans, AI with AI). However, heterogenous human-AI pairs fail -- suggesting differences in communicative tendencies. In Experiment 2, we ask whether LLMs can be induced to behave more like human conversants, by prompting them to produce superficially humanlike behavior. While the length of their messages matches that of human pairs, accuracy and lexical overlap in human-LLM pairs continues to lag behind that of both human-human and AI-AI pairs. These results suggest that conversational alignment requires more than just the ability to mimic previous interactions, but also shared interpretative biases toward the meanings that are conveyed.

</details>


### [256] [Pretraining with Token-Level Adaptive Latent Chain-of-Thought](https://arxiv.org/abs/2602.08220)
*Boyi Zeng,Yiqin Hao,He Li,Shixiang Song,Feichen Song,Zitong Wang,Siyuan Huang,Yi Xu,ZiWei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 通过将潜在思维链内部化到预训练中，提出自适应潜在CoT方法，在不增加参数的情况下增加每个token的计算量，根据token难度自适应调整思维链长度，在减少计算量的同时提升语言建模和下游任务性能


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型的扩展受到高质量语料库有限和通信成本上升的约束，需要探索不增加参数但增加计算效率的新方法

Method: 提出Pretraining with Token-Level Adaptive Latent CoT方法，在生成每个token前生成可变长度的潜在思维链轨迹，困难token分配更长轨迹，简单token分配更短甚至零轨迹，通过单阶段预训练实现自适应停止机制

Result: 在Llama架构上的实验表明，自适应潜在CoT持续改善语言建模困惑度和广泛下游任务准确率，即使训练FLOPs少于先前循环基线

Conclusion: 自适应潜在CoT提供了一种有效的参数效率扩展路径，通过token级别的自适应计算分配，在减少计算成本的同时提升模型性能，为语言模型扩展提供了新的维度

Abstract: Scaling large language models by increasing parameters and training data is increasingly constrained by limited high-quality corpora and rising communication costs. This work explores an alternative axis: increasing per-token computation without expanding parameters, by internalizing latent Chain-of-Thought (CoT) into pretraining. We propose Pretraining with Token-Level Adaptive Latent CoT (adaptive latent CoT), where the model generates a variable-length latent CoT trajectory before emitting each token -- allocating longer trajectories to difficult tokens and shorter (or even zero) trajectories to easy ones. Importantly, this behavior emerges naturally from one-stage pretraining on general text and reduces computation in both training and inference via token-wise adaptive halting. Experiments with Llama architectures show that adaptive latent CoT consistently improves language modeling perplexity and broad downstream accuracy, even with fewer training FLOPs than prior recurrent baselines.

</details>


### [257] [CoRect: Context-Aware Logit Contrast for Hidden State Rectification to Resolve Knowledge Conflicts](https://arxiv.org/abs/2602.08221)
*Xuhua Ma,Richong Zhang,Zhijie Nie*

Main category: cs.CL

TL;DR: CoRect通过对比上下文和非上下文前向传播的logits来识别参数偏差层，无需真实标签即可纠正隐藏状态，有效解决RAG中的知识冲突问题，在问答和摘要任务中显著提升忠实度并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: RAG系统存在知识冲突问题，模型内部参数知识会覆盖检索到的证据，导致输出不忠实。现有方法要么依赖浅层解码调整，要么需要真实标签进行权重编辑，存在局限性。

Method: 通过层间分析发现参数抑制现象，提出CoRect方法：对比上下文和非上下文前向传播的logits，识别高参数偏差层，然后纠正隐藏状态以保留基于证据的信息。

Result: 在问答和摘要基准测试中，CoRect相比强基线模型一致提高了输出的忠实度并减少了幻觉现象。

Conclusion: CoRect提供了一种无需真实标签的有效方法来解决RAG中的知识冲突问题，通过识别和纠正参数偏差层，显著提升了模型输出的证据基础性和可靠性。

Abstract: Retrieval-Augmented Generation (RAG) often struggles with knowledge conflicts, where model-internal parametric knowledge overrides retrieved evidence, leading to unfaithful outputs. Existing approaches are often limited, relying either on superficial decoding adjustments or weight editing that necessitates ground-truth targets. Through layer-wise analysis, we attribute this failure to a parametric suppression phenomenon: specifically, in deep layers, certain FFN layers overwrite context-sensitive representations with memorized priors. To address this, we propose CoRect (Context-Aware Logit Contrast for Hidden State Rectification). By contrasting logits from contextualized and non-contextualized forward passes, CoRect identifies layers that exhibit high parametric bias without requiring ground-truth labels. It then rectifies the hidden states to preserve evidence-grounded information. Across question answering (QA) and summarization benchmarks, CoRect consistently improves faithfulness and reduces hallucinations compared to strong baselines.

</details>


### [258] [When Benign Inputs Lead to Severe Harms: Eliciting Unsafe Unintended Behaviors of Computer-Use Agents](https://arxiv.org/abs/2602.08235)
*Jaylen Jones,Zhehao Zhang,Yuting Ning,Eric Fosler-Lussier,Pierre-Luc St-Charles,Yoshua Bengio,Dawn Song,Yu Su,Huan Sun*

Main category: cs.CL

TL;DR: AutoElicit：首个用于计算机使用代理意外行为的自动化框架，通过迭代扰动良性指令来发现严重危害行为，在Claude等先进CUAs中发现了数百种有害意外行为。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理(CUAs)在复杂操作系统工作流自动化中展现出巨大潜力，但存在偏离预期结果的不安全意外行为风险，而现有研究缺乏系统性的表征方法和自动化手段来主动发现这些长尾问题。

Method: 提出AutoElicit代理框架，通过迭代扰动良性指令并利用CUA执行反馈，在保持扰动现实性和良性前提下，自动引发严重危害行为。

Result: 在Claude 4.5 Haiku和Opus等先进CUAs中发现了数百种有害意外行为，验证了人类确认的成功扰动在不同前沿CUAs中的可迁移性和持续性脆弱性。

Conclusion: 这项工作为系统分析现实计算机使用场景中的意外行为建立了基础框架，填补了该领域的方法论空白。

Abstract: Although computer-use agents (CUAs) hold significant potential to automate increasingly complex OS workflows, they can demonstrate unsafe unintended behaviors that deviate from expected outcomes even under benign input contexts. However, exploration of this risk remains largely anecdotal, lacking concrete characterization and automated methods to proactively surface long-tail unintended behaviors under realistic CUA scenarios. To fill this gap, we introduce the first conceptual and methodological framework for unintended CUA behaviors, by defining their key characteristics, automatically eliciting them, and analyzing how they arise from benign inputs. We propose AutoElicit: an agentic framework that iteratively perturbs benign instructions using CUA execution feedback, and elicits severe harms while keeping perturbations realistic and benign. Using AutoElicit, we surface hundreds of harmful unintended behaviors from state-of-the-art CUAs such as Claude 4.5 Haiku and Opus. We further evaluate the transferability of human-verified successful perturbations, identifying persistent susceptibility to unintended behaviors across various other frontier CUAs. This work establishes a foundation for systematically analyzing unintended behaviors in realistic computer-use settings.

</details>


### [259] [Document Reconstruction Unlocks Scalable Long-Context RLVR](https://arxiv.org/abs/2602.08237)
*Yao Xiao,Lei Wang,Yue Deng,Guanzheng Chen,Ziqi Jin,Jung-jae Kim,Xiaoli Li,Roy Ka-wei Lee,Lidong Bing*

Main category: cs.CL

TL;DR: 提出一种无监督强化学习方法RLVR，通过段落重建任务增强大语言模型的长文本处理能力，无需人工标注或教师模型监督。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法依赖昂贵的黄金标准答案或教师模型评估，成本高且耗时，需要探索无监督方法来提升LLMs的长文本能力。

Method: 在长文档中用占位符替换部分段落，通过强化学习训练模型从候选选项中选择和排序缺失段落来重建文档，以捕捉全局叙事连贯性。

Result: 在RULER和LongBench v2基准测试中取得显著提升，特别是在RULER上表现突出，无需人工标注的长文本QA数据。

Conclusion: 该方法有效提升了LLMs的长文本处理能力，通过大量消融实验验证了奖励设计、数据策略、训练方案和数据扩展的影响，代码和数据已公开。

Abstract: Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.

</details>


### [260] [On convexity and efficiency in semantic systems](https://arxiv.org/abs/2602.08238)
*Nathaniel Imel,Noga Zaslavasky*

Main category: cs.CL

TL;DR: 该研究分析了人类语义类别系统的两个特征——凸性和效率性之间的关系，通过信息瓶颈框架证明了二者本质不同但常共存于颜色命名系统，效率性比凸性更能解释语义类型学现象。


<details>
  <summary>Details</summary>
Motivation: 先前研究观察到凸性和效率性在颜色命名中同时出现，但两者之间的分析关系及其共存原因尚未得到充分理解，需要填补这一研究空白。

Method: 结合分析和实证方法，基于信息瓶颈(IB)框架进行语义效率分析，比较凸性和效率性在区分真实颜色命名系统与假设变体方面的预测能力。

Result: 研究发现凸性和效率性相互独立(互不蕴含)，但IB最优系统在颜色命名领域大多是凸的；效率性在区分真实系统方面是更强的预测因子，凸性仅提供边际改进；效率性能解释凸性无法解释的经验现象。

Conclusion: 凸性和效率性虽然可能产生相似的结构观察结果，但本质上是不同的概念，效率性为语义类型学提供了更全面的解释框架。

Abstract: There are two widely held characterizations of human semantic category systems: (1) they form convex partitions of conceptual spaces, and (2) they are efficient for communication. While prior work observed that convexity and efficiency co-occur in color naming, the analytical relation between them and why they co-occur have not been well understood. We address this gap by combining analytical and empirical analyses that build on the Information Bottleneck (IB) framework for semantic efficiency. First, we show that convexity and efficiency are distinct in the sense that neither entails the other: there are convex systems which are inefficient, and optimally-efficient systems that are non-convex. Crucially, however, the IB-optimal systems are mostly convex in the domain of color naming, explaining the main empirical basis for the convexity approach. Second, we show that efficiency is a stronger predictor for discriminating attested color naming systems from hypothetical variants, with convexity adding negligible improvement on top of that. Finally, we discuss a range of empirical phenomena that convexity cannot account for but efficiency can. Taken together, our work suggests that while convexity and efficiency can yield similar structural observations, they are fundamentally distinct, with efficiency providing a more comprehensive account of semantic typology.

</details>


### [261] [Language Predicts Identity Fusion Across Cultures and Reveals Divergent Pathways to Violence](https://arxiv.org/abs/2602.08252)
*Devin R. Wright,Justin E. Lane,F. LeRon Shults*

Main category: cs.CL

TL;DR: 本研究开发了认知语言身份融合评分方法，结合认知语言模式、大语言模型和隐式隐喻，从语言中测量身份融合程度。该方法在英新两国数据中表现优于现有方法，并识别出极端主义中的两种高融合暴力路径。


<details>
  <summary>Details</summary>
Motivation: 随着社会极化加剧和政治暴力频发，理解极端主义的心理根源日益重要。已有研究表明身份融合能预测极端行为倾向，但需要更有效的测量工具。

Method: 采用认知语言身份融合评分方法，整合认知语言分析、大语言模型和隐式隐喻技术，从语言数据中自动测量身份融合程度。在英国和新加坡数据集上进行验证。

Result: 新方法在预测已验证的身份融合分数方面优于现有方法。应用于极端主义宣言分析，识别出两种高融合暴力路径：意识形态驱动者将自我框架为群体成员，形成亲属关系；而怨恨驱动者将群体框架为个人身份延伸。

Conclusion: 该研究完善了身份融合理论，提供了可扩展的工具支持融合研究和极端主义检测，有助于深入理解极端行为的心理机制。

Abstract: In light of increasing polarization and political violence, understanding the psychological roots of extremism is increasingly important. Prior research shows that identity fusion predicts willingness to engage in extreme acts. We evaluate the Cognitive Linguistic Identity Fusion Score, a method that uses cognitive linguistic patterns, LLMs, and implicit metaphor to measure fusion from language. Across datasets from the United Kingdom and Singapore, this approach outperforms existing methods in predicting validated fusion scores. Applied to extremist manifestos, two distinct high-fusion pathways to violence emerge: ideologues tend to frame themselves in terms of group, forming kinship bonds; whereas grievance-driven individuals frame the group in terms of their personal identity. These results refine theories of identity fusion and provide a scalable tool aiding fusion research and extremism detection.

</details>


### [262] [Language Modeling and Understanding Through Paraphrase Generation and Detection](https://arxiv.org/abs/2602.08274)
*Jan Philip Wahle*

Main category: cs.CL

TL;DR: 论文提出将释义分解为语言学层面的类型化分析，证明基于类型化训练能提升模型在语义理解和下游任务中的表现，特别是在抄袭检测和重复问题识别方面超越人类基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法将释义简化为二元判断或单一改写，掩盖了保持语义的语言学因素，需要更细粒度和认知基础的语义等价分析方法。

Method: 将释义分解为构成性语言学方面（释义类型），并基于此对机器学习模型进行显式训练。

Result: 基于释义类型训练的模型在相关任务中表现更强：抄袭检测准确率89.6%（维基百科）和66.5%（arXiv论文），超越人类基线；在Quora重复问题识别中也优于二元对训练模型。

Conclusion: 释义类型分解提供了更细粒度的语义等价视角，能显著提升语言模型的语义理解能力和下游应用性能。

Abstract: Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same thoughts in virtually infinite ways using different words and structures - this ability to rephrase and reformulate expressions is known as paraphrase. Modeling paraphrases is a keystone to meaning in computational language models; being able to construct different variations of texts that convey the same meaning or not shows strong abilities of semantic understanding. If computational language models are to represent meaning, they must understand and control the different aspects that construct the same meaning as opposed to different meanings at a fine granularity. Yet most existing approaches reduce paraphrasing to a binary decision between two texts or to producing a single rewrite of a source, obscuring which linguistic factors are responsible for meaning preservation. In this thesis, I propose that decomposing paraphrases into their constituent linguistic aspects (paraphrase types) offers a more fine-grained and cognitively grounded view of semantic equivalence. I show that even advanced machine learning models struggle with this task. Yet, when explicitly trained on paraphrase types, models achieve stronger performance on related paraphrase tasks and downstream applications. For example, in plagiarism detection, language models trained on paraphrase types surpass human baselines: 89.6% accuracy compared to 78.4% for plagiarism cases from Wikipedia, and 66.5% compared to 55.7% for plagiarism of scientific papers from arXiv. In identifying duplicate questions on Quora, models trained with paraphrase types improve over models trained on binary pairs. Furthermore, I demonstrate that...

</details>


### [263] [New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR](https://arxiv.org/abs/2602.08281)
*Zhilin Wang,Yafu Li,Shunkai Zhang,Zhi Wang,Haoran Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: RLVR通过概率框架增强LLM能力而非仅激发潜在能力，通过原子步骤概率锐化驱动复杂推理，在代数任务上验证了RLVR能探索新路径、性能受原子步骤联合概率支配、并可能牺牲特定技能以最大化奖励。


<details>
  <summary>Details</summary>
Motivation: 探讨RLVR是赋予LLMs新能力还是仅激发潜在能力，提出从实例级可解性角度定义能力，研究复杂推理能力的涌现机制。

Method: 使用Algebrarium框架，仅在单步操作上训练模型，评估其在未见多步任务上的表现，分析原子步骤联合概率与复合性能的关系。

Result: RLVR通过放大现有技能激励探索新路径；复合性能严格受原子步骤联合概率支配（皮尔逊相关系数ρ∈[0.69,0.96]）；RLVR作为全局优化器可能牺牲特定技能以最大化总奖励。

Conclusion: RLVR通过迭代优化可解问题使模型获得解决先前不可解场景的能力，为RLVR中的涌现能力提供了新的解释框架。

Abstract: Whether Reinforcement Learning with Verifiable Rewards (RLVR) endows Large Language Models (LLMs) with new capabilities or merely elicits latent traces remains a central debate. In this work, we align with the former view, proposing a probabilistic framework where capability is defined by instance-level solvability. We hypothesize that the emergence of complex reasoning can be driven by sharpening atomic step probabilities, which enables models to overcome the exponential decay of success rates inherent in multi-step reasoning chains. Utilizing the Algebrarium framework, we train models exclusively on single-step operations and evaluate their performance on unseen multi-step tasks. Our empirical results confirm that: (1) RLVR incentivizes the exploration of previously inaccessible solution paths by amplifying the model's existing skills; (2) composite performance is strictly governed by the joint probability of atomic steps, evidenced by high Pearson correlation coefficients ($ρ\in [0.69, 0.96]$); and (3) RLVR, acting as a global optimizer, can cause specific skills to be sacrificed to maximize aggregate reward. Our work offers a novel explanation for emergent abilities in RLVR, suggesting that the iterative optimization of solvable problems enables models to develop the capabilities to tackle previously unsolvable scenarios.

</details>


### [264] [Knowledge Augmented Entity and Relation Extraction for Legal Documents with Hypergraph Neural Network](https://arxiv.org/abs/2602.08289)
*Binglin Wu,Xianneng Li*

Main category: cs.CL

TL;DR: 提出基于超图神经网络的毒品案件判决文书实体关系抽取方法Legal-KAHRE，通过候选跨度生成器、法律词典融合和超图结构设计，在CAIL2022数据集上显著优于基线模型


<details>
  <summary>Details</summary>
Motivation: 司法数字化积累了大量电子法律文档，但现有方法缺乏领域知识且未考虑司法领域特性，需要专门针对毒品判决文书的实体关系抽取方法

Method: 1) 基于邻域导向打包策略和双仿射机制的候选跨度生成器；2) 构建法律词典并通过多头注意力融入文本编码；3) 将共同犯罪、数罪并罚等案件特性融入超图结构设计；4) 使用超图神经网络进行高阶推理

Result: 在CAIL2022信息抽取数据集上的实验结果表明，该方法显著优于现有基线模型

Conclusion: Legal-KAHRE方法有效整合司法领域知识，通过超图神经网络处理复杂法律实体关系，为毒品案件判决文书的信息抽取提供了有效解决方案

Abstract: With the continuous progress of digitization in Chinese judicial institutions, a substantial amount of electronic legal document information has been accumulated. To unlock its potential value, entity and relation extraction for legal documents has emerged as a crucial task. However, existing methods often lack domain-specific knowledge and fail to account for the unique characteristics of the judicial domain. In this paper, we propose an entity and relation extraction algorithm based on hypergraph neural network (Legal-KAHRE) for drug-related judgment documents. Firstly, we design a candidate span generator based on neighbor-oriented packing strategy and biaffine mechanism, which identifies spans likely to contain entities. Secondly, we construct a legal dictionary with judicial domain knowledge and integrate it into text encoding representation using multi-head attention. Additionally, we incorporate domain-specific cases like joint crimes and combined punishment for multiple crimes into the hypergraph structure design. Finally, we employ a hypergraph neural network for higher-order inference via message passing. Experimental results on the CAIL2022 information extraction dataset demonstrate that our method significantly outperforms existing baseline models.

</details>


### [265] [When Does Context Help? Error Dynamics of Contextual Information in Large Language Models](https://arxiv.org/abs/2602.08294)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 提出了一个统一的理论框架来分析Transformer大语言模型中任意上下文信息的影响，通过输出误差动态表征上下文影响，证明了误差向量的可加性分解和误差减少的几何条件，并提出了基于理论的原则性上下文选择策略。


<details>
  <summary>Details</summary>
Motivation: 尽管推理时的上下文信息（如演示、检索知识或交互历史）可以显著提升大语言模型性能而无需参数更新，但其理论作用在上下文学习之外的更广泛设置中仍缺乏深入理解。

Method: 在单层Transformer中分析上下文条件误差向量的可加性分解，证明误差向量可分解为基线误差向量和上下文修正向量，推导出误差减少的几何条件（对齐性和范数约束），并将结果扩展到多上下文和多层Transformer。

Result: 理论分析表明上下文修正范数存在由上下文-查询相关性和互补性决定的显式上界。在上下文学习、检索增强生成和记忆演化等实验验证了理论，提出的原则性上下文选择策略使性能提升0.6%。

Conclusion: 该研究为理解Transformer中上下文信息的作用提供了统一的理论框架，揭示了误差减少的几何条件，并展示了理论指导下的上下文选择策略的实际有效性。

Abstract: Contextual information at inference time, such as demonstrations, retrieved knowledge, or interaction history, can substantially improve large language models (LLMs) without parameter updates, yet its theoretical role remains poorly understood beyond specific settings such as in-context learning (ICL). We present a unified theoretical framework for analyzing the effect of arbitrary contextual information in Transformer-based LLMs. Our analysis characterizes contextual influence through output error dynamics. In a single-layer Transformer, we prove that the context-conditioned error vector decomposes additively into the baseline error vector and a contextual correction vector. This yields necessary geometric conditions for error reduction: the contextual correction must align with the negative baseline error and satisfy a norm constraint. We further show that the contextual correction norm admits an explicit upper bound determined by context-query relevance and complementarity. These results extend to multi-context and multi-layer Transformers. Experiments across ICL, retrieval-augmented generation, and memory evolution validate our theory and motivate a principled context selection strategy that improves performance by $0.6\%$.

</details>


### [266] [JUSTICE: Judicial Unified Synthesis Through Intermediate Conclusion Emulation for Automated Judgment Document Generation](https://arxiv.org/abs/2602.08305)
*Binglin Wu,Yingyi Zhang,Xiannneg Li*

Main category: cs.CL

TL;DR: JUSTICE框架通过模拟法官的'检索→预判→撰写'认知流程，在判决书生成中引入预判阶段，显著提升法律准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有方法简化了复杂的法律推理过程，特别是忽略了人类法官形成初步结论的'预判'阶段，导致基础司法要素获取无效和预判过程建模不足，影响最终文件的法律合理性。

Method: 提出JUSTICE框架，包含三个核心组件：参考司法要素检索器(RJER)检索法律条文和先例案例；中间结论模拟器(ICE)生成可验证的中间结论；司法统一合成器(JUS)综合输入生成最终判决。

Result: 在领域内法律基准和分布外数据集上的实验显示，JUSTICE显著优于强基线方法，在法律准确性方面有显著提升，包括刑期预测准确率提高4.6%。

Conclusion: 明确建模预判过程对于增强生成判决文件的法律连贯性和准确性至关重要，JUSTICE框架有效解决了现有方法的局限性。

Abstract: Automated judgment document generation is a significant yet challenging legal AI task. As the conclusive written instrument issued by a court, a judgment document embodies complex legal reasoning. However, existing methods often oversimplify this complex process, particularly by omitting the ``Pre-Judge'' phase, a crucial step where human judges form a preliminary conclusion. This omission leads to two core challenges: 1) the ineffective acquisition of foundational judicial elements, and 2) the inadequate modeling of the Pre-Judge process, which collectively undermine the final document's legal soundness. To address these challenges, we propose \textit{\textbf{J}udicial \textbf{U}nified \textbf{S}ynthesis \textbf{T}hrough \textbf{I}ntermediate \textbf{C}onclusion \textbf{E}mulation} (JUSTICE), a novel framework that emulates the ``Search $\rightarrow$ Pre-Judge $\rightarrow$ Write'' cognitive workflow of human judges. Specifically, it introduces the Pre-Judge stage through three dedicated components: Referential Judicial Element Retriever (RJER), Intermediate Conclusion Emulator (ICE), and Judicial Unified Synthesizer (JUS). RJER first retrieves legal articles and a precedent case to establish a referential foundation. ICE then operationalizes the Pre-Judge phase by generating a verifiable intermediate conclusion. Finally, JUS synthesizes these inputs to craft the final judgment. Experiments on both an in-domain legal benchmark and an out-of-distribution dataset show that JUSTICE significantly outperforms strong baselines, with substantial gains in legal accuracy, including a 4.6\% improvement in prison term prediction. Our findings underscore the importance of explicitly modeling the Pre-Judge process to enhance the legal coherence and accuracy of generated judgment documents.

</details>


### [267] [Improving Data and Reward Design for Scientific Reasoning in Large Language Models](https://arxiv.org/abs/2602.08321)
*Zijie Chen,Zhenghao Lin,Xiao Liu,Zhenzhong Lan,Yeyun Gong,Peng Cheng*

Main category: cs.CL

TL;DR: Dr. SCI是一个大规模科学问答数据集和训练管道，通过创新的数据处理、课程学习和基于量规的强化学习，显著提升语言模型在开放式科学问题上的表现


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在开放式科学问题上的挑战，特别是由于监督和评估不可靠导致的瓶颈问题

Method: 开发大规模数据处理管道创建Dr. SCI数据集（100万问题，8个STEM学科），提出三阶段训练流程：探索扩展SFT、动态难度课程学习、基于科学量规的强化学习

Result: Qwen3-4B-Base模型在GPQA-diamond上达到63.2分，GPQA-general上达到32.4分，优于o1-mini和GPT-4o等强基线模型

Conclusion: Dr. SCI管道通过系统化的数据构建和训练策略，有效提升了语言模型在科学推理特别是开放式问题上的能力，为科学AI的发展提供了重要基础

Abstract: Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr.SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.

</details>


### [268] [An Attention-over-Attention Generative Model for Joint Multiple Intent Detection and Slot Filling](https://arxiv.org/abs/2602.08322)
*Wei Zhu*

Main category: cs.CL

TL;DR: 提出基于注意力机制的生成式框架，同时处理多意图检测和槽填充任务，并构建新的多意图SLU数据集


<details>
  <summary>Details</summary>
Motivation: 现实对话中用户经常在单个话语中表达多个意图，而现有方法主要针对单意图SLU，无法有效处理多意图场景

Method: 使用注意力叠加解码器处理可变数量意图和子任务间干扰，利用BERT的NSP头部构建多意图数据集，采用多任务学习框架

Result: 在MixATIS、MixSNIPS和新建数据集上达到最先进性能

Conclusion: 所提方法能有效解决多意图SLU问题，注意力机制和多任务学习策略显著提升性能

Abstract: In task-oriented dialogue systems, spoken language understanding (SLU) is a critical component, which consists of two sub-tasks, intent detection and slot filling. Most existing methods focus on the single-intent SLU, where each utterance only has one intent. However, in real-world scenarios users usually express multiple intents in an utterance, which poses a challenge for existing dialogue systems and datasets. In this paper, we propose a generative framework to simultaneously address multiple intent detection and slot filling. In particular, an attention-over-attention decoder is proposed to handle the variable number of intents and the interference between the two sub-tasks by incorporating an inductive bias into the process of multi-task learning. Besides, we construct two new multi-intent SLU datasets based on single-intent utterances by taking advantage of the next sentence prediction (NSP) head of the BERT model. Experimental results demonstrate that our proposed attention-over-attention generative model achieves state-of-the-art performance on two public datasets, MixATIS and MixSNIPS, and our constructed datasets.

</details>


### [269] [Latent Reasoning with Supervised Thinking States](https://arxiv.org/abs/2602.08332)
*Ido Amos,Avi Caciularu,Mor Geva,Amir Globerson,Jonathan Herzig,Lior Shani,Idan Szpektor*

Main category: cs.CL

TL;DR: Thinking States是一种在输入处理过程中进行推理的方法，通过在输入token之间生成思考token序列并转换为嵌入空间，实现并行化推理，降低推理成本并保持CoT性能。


<details>
  <summary>Details</summary>
Motivation: 解决链式思维推理(CoT)方法在大型语言模型中产生长推理过程导致的高推理成本问题，寻求在保持推理能力的同时提高效率。

Method: 在输入处理过程中每隔几个输入token生成思考token序列，将思考token转换回嵌入空间并添加到后续输入token中，利用教师强制实现并行化学习。

Result: 在多项推理任务上优于其他潜在推理方法，在数学问题上缩小了与CoT的差距，在2-Hop QA上性能相当但延迟更低，在状态跟踪任务中表现出比CoT更强的推理能力并能外推到更长的序列。

Conclusion: Thinking States方法有效解决了CoT推理的高成本问题，实现了高效的并行化推理，在多个任务上展现出优越的性能和泛化能力。

Abstract: Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.

</details>


### [270] [UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models](https://arxiv.org/abs/2602.08336)
*Cheng Yang,Chufan Shi,Bo Shui,Yaokang Wu,Muzi Tao,Huijuan Wang,Ivan Yee Lee,Yong Liu,Xuezhe Ma,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: UReason是一个诊断性基准测试，用于评估推理驱动图像生成中推理是否能在像素层面忠实执行，揭示了推理过程中的上下文干扰问题。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型采用思维链推理来指导图像生成，但推理对视觉合成的实际效果尚不明确，需要系统评估推理在图像生成中的忠实执行程度。

Method: 构建包含2000个实例的UReason基准，涵盖5类推理任务；引入三组对比实验框架：直接生成、推理引导生成和去上下文化生成；在8个开源统一模型上进行系统评估。

Result: 发现一致的推理悖论：推理轨迹通常比直接生成表现更好，但保留中间思想作为条件上下文往往会阻碍视觉合成，仅基于精炼提示的条件生成能带来显著提升。

Conclusion: 瓶颈在于上下文干扰而非推理能力不足，UReason为研究统一模型中的推理提供了原则性测试平台，推动未来方法在有效整合推理进行视觉生成的同时减轻干扰。

Abstract: To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.

</details>


### [271] [WorldTravel: A Realistic Multimodal Travel-Planning Benchmark with Tightly Coupled Constraints](https://arxiv.org/abs/2602.08367)
*Zexuan Wang,Chenghao Yang,Yingqi Que,Zhenzhu Yang,Huaqing Yuan,Yiwen Wang,Zhengxuan Jiang,Shengjie Fang,Zhenhe Wu,Zhaohui Wang,Zhixin Yao,Jiashuo Liu,Jincheng Ren,Yuzhen Li,Yang Yang,Jiaheng Liu,Jian Yang,Zaiyuan Wang,Ge Zhang,Zhoufutu Wen,Wenhao Huang*

Main category: cs.CL

TL;DR: WorldTravel是一个包含150个真实旅行场景的基准测试，要求处理15+个相互依赖的时空约束。WorldTravel-Webscape多模态环境包含2000+个网页渲染，需要从视觉布局中感知约束参数进行规划。评估显示顶级模型在文本环境下仅32.67%可行性，多模态环境下暴跌至19.33%，揭示了感知-行动差距和约10个约束的规划阈值瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要包含松散耦合约束，可通过局部贪婪决策解决，且依赖理想化数据，无法捕捉从动态网络环境中提取参数的复杂性。需要创建更真实的基准来评估自主规划系统在复杂约束下的表现。

Method: 开发了WorldTravel基准（150个真实旅行场景，5个城市，平均15+个相互依赖约束）和WorldTravel-Webscape多模态环境（2000+个渲染网页）。评估了10个前沿模型在文本和多模态环境下的表现，分析了感知-行动差距和规划阈值。

Result: GPT-5.2在纯文本环境下仅达到32.67%的可行性，在多模态环境下暴跌至19.33%。发现约10个约束的规划阈值，超过此阈值模型推理一致性失败。感知和推理仍然是独立的瓶颈。

Conclusion: 当前代理系统在处理脆弱现实世界物流方面存在显著局限，需要下一代代理系统将高保真视觉感知与长时程推理统一起来，以应对复杂的相互依赖约束。

Abstract: Real-world autonomous planning requires coordinating tightly coupled constraints where a single decision dictates the feasibility of all subsequent actions. However, existing benchmarks predominantly feature loosely coupled constraints solvable through local greedy decisions and rely on idealized data, failing to capture the complexity of extracting parameters from dynamic web environments. We introduce \textbf{WorldTravel}, a benchmark comprising 150 real-world travel scenarios across 5 cities that demand navigating an average of 15+ interdependent temporal and logical constraints. To evaluate agents in realistic deployments, we develop \textbf{WorldTravel-Webscape}, a multi-modal environment featuring over 2,000 rendered webpages where agents must perceive constraint parameters directly from visual layouts to inform their planning. Our evaluation of 10 frontier models reveals a significant performance collapse: even the state-of-the-art GPT-5.2 achieves only 32.67\% feasibility in text-only settings, which plummets to 19.33\% in multi-modal environments. We identify a critical Perception-Action Gap and a Planning Horizon threshold at approximately 10 constraints where model reasoning consistently fails, suggesting that perception and reasoning remain independent bottlenecks. These findings underscore the need for next-generation agents that unify high-fidelity visual perception with long-horizon reasoning to handle brittle real-world logistics.

</details>


### [272] [ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts](https://arxiv.org/abs/2602.08371)
*Hung Quang Tran,Nam Tien Pham,Son T. Luu,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: 本研究介绍了越南语情感语料库ViGoEmotions，包含20,664条社交媒体评论，标注27种细粒度情感。通过评估8种Transformer模型和3种预处理策略，发现表情符号处理方式显著影响情感分类性能，ViSoBERT模型取得最佳表现。


<details>
  <summary>Details</summary>
Motivation: 情感分类在情感预测和有害内容检测中具有重要作用，但越南语领域缺乏高质量的情感语料库和系统性的预处理策略研究。

Method: 构建包含20,664条越南语社交媒体评论的ViGoEmotions语料库，采用27种细粒度情感标注。评估8种预训练Transformer模型在三种预处理策略下的性能：保留原始表情符号、将表情符号转换为文本描述、使用ViSoLex进行词汇规范化。

Result: 表情符号转换为文本描述能提升多数BERT基线的性能，而保留表情符号对ViSoBERT和CafeBERT效果最佳。去除表情符号通常导致性能下降。ViSoBERT获得最高Macro F1-score 61.50%和Weighted F1-score 63.26%。

Conclusion: ViGoEmotions语料库能有效支持多种架构，但预处理策略和标注质量仍是影响下游性能的关键因素，表情符号的处理方式对越南语情感分类具有重要影响。

Abstract: Emotion classification plays a significant role in emotion prediction and harmful content detection. Recent advancements in NLP, particularly through large language models (LLMs), have greatly improved outcomes in this field. This study introduces ViGoEmotions -- a Vietnamese emotion corpus comprising 20,664 social media comments in which each comment is classified into 27 fine-grained distinct emotions. To evaluate the quality of the dataset and its impact on emotion classification, eight pre-trained Transformer-based models were evaluated under three preprocessing strategies: preserving original emojis with rule-based normalization, converting emojis into textual descriptions, and applying ViSoLex, a model-based lexical normalization system. Results show that converting emojis into text often improves the performance of several BERT-based baselines, while preserving emojis yields the best results for ViSoBERT and CafeBERT. In contrast, removing emojis generally leads to lower performance. ViSoBERT achieved the highest Macro F1-score of 61.50% and Weighted F1-score of 63.26%. Strong performance was also observed from CafeBERT and PhoBERT. These findings highlight that while the proposed corpus can support diverse architectures effectively, preprocessing strategies and annotation quality remain key factors influencing downstream performance.

</details>


### [273] [Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.08382)
*Zhuoen Chen,Dongfang Li,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 提出基于分块压缩和选择性记忆回忆的认知启发框架，解决LLM长上下文处理中的计算成本、信息遗忘和上下文碎片化问题，实现从7K到1.75M token的上下文长度外推。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文处理中面临二次计算成本、信息遗忘和检索增强生成中的上下文碎片化等挑战，需要更高效的推理方法。

Method: 将长输入分块编码为压缩记忆表示，通过门控模块动态选择相关记忆块，推理模块通过演化工作记忆迭代处理任务，压缩器和推理器通过端到端强化学习联合优化。

Result: 在RULER-HQA等多跳推理基准上达到竞争性准确率，相比MemAgent实现峰值GPU内存使用减少2倍，推理速度提升6倍。

Conclusion: 该认知启发框架通过选择性记忆处理机制，在保持准确性的同时显著提升了长上下文处理的效率和可扩展性。

Abstract: Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.

</details>


### [274] [TEAM: Temporal-Spatial Consistency Guided Expert Activation for MoE Diffusion Language Model Acceleration](https://arxiv.org/abs/2602.08404)
*Linye Wei,Zixiang Luo,Pingzhi Tang,Meng Li*

Main category: cs.CL

TL;DR: TEAM是一个即插即用框架，通过减少激活专家数量同时增加接受token数量来加速MoE扩散大语言模型，利用时间和空间一致性实现2.2倍加速且性能损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 发现MoE架构与扩散解码之间存在根本性不匹配：每个去噪步骤激活大量专家，但只有少量token被最终接受，导致推理开销大，限制了在延迟敏感应用中的部署。

Method: 提出TEAM框架，利用专家路由决策在去噪层级间的时间一致性和token位置间的空间一致性，采用三种互补策略：保守选择解码和掩码token所需专家，同时进行多候选的激进推测性探索。

Result: 实验结果显示TEAM相比原始MoE dLLM实现最高2.2倍加速，性能退化可忽略不计。

Conclusion: TEAM有效解决了MoE dLLM的推理效率问题，通过利用路由一致性实现了显著的加速效果，为扩散语言模型的实际部署提供了可行解决方案。

Abstract: Diffusion large language models (dLLMs) have recently gained significant attention due to their inherent support for parallel decoding. Building on this paradigm, Mixture-of-Experts (MoE) dLLMs with autoregressive (AR) initialization have further demonstrated strong performance competitive with mainstream AR models. However, we identify a fundamental mismatch between MoE architectures and diffusion-based decoding. Specifically, a large number of experts are activated at each denoising step, while only a small subset of tokens is ultimately accepted, resulting in substantial inference overhead and limiting their deployment in latency-sensitive applications. In this work, we propose TEAM, a plug-and-play framework that accelerates MoE dLLMs by enabling more accepted tokens with fewer activated experts. TEAM is motivated by the observation that expert routing decisions exhibit strong temporal consistency across denoising levels as well as spatial consistency across token positions. Leveraging these properties, TEAM employs three complementary expert activation and decoding strategies, conservatively selecting necessary experts for decoded and masked tokens and simultaneously performing aggressive speculative exploration across multiple candidates. Experimental results demonstrate that TEAM achieves up to 2.2x speedup over vanilla MoE dLLM, with negligible performance degradation. Code is released at https://github.com/PKU-SEC-Lab/TEAM-MoE-dLLM.

</details>


### [275] [Prism: Spectral-Aware Block-Sparse Attention](https://arxiv.org/abs/2602.08426)
*Xinghao Wang,Pengyu Wang,Xiaoran Liu,Fangxu Liu,Jason Chu,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: Prism提出了一种无需训练的频谱感知块稀疏注意力方法，通过高频和低频分支分解及能量温度校准，解决了均值池化与RoPE交互导致的局部位置信息丢失问题，实现了纯块级操作的高效块选择，在保持精度的同时获得5.1倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有块稀疏注意力方法使用粗粒度注意力作为块重要性估计的代理，但通常需要昂贵的token级搜索或评分，导致显著的选择开销。标准均值池化与RoPE的交互会破坏高频维度信息，造成局部位置信息的'盲点'。

Method: 提出Prism方法：1）频谱感知分解为高频和低频分支；2）基于能量的温度校准，从池化表示中恢复衰减的位置信号；3）使用纯块级操作进行块重要性估计，避免token级计算。

Result: 广泛评估证实Prism在保持与完整注意力精度相当的同时，实现了高达5.1倍的加速比。

Conclusion: 通过理论分析均值池化与RoPE的交互问题，提出的Prism方法有效解决了块稀疏注意力中的块选择效率瓶颈，为长上下文LLM预填充提供了高效且准确的解决方案。

Abstract: Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a "blind spot" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\mathbf{5.1\times}$ speedup.

</details>


### [276] [Large Language Models and Impossible Language Acquisition: "False Promise" or an Overturn of our Current Perspective towards AI](https://arxiv.org/abs/2602.08437)
*Ziyan wang,Longlong Ma*

Main category: cs.CL

TL;DR: 对乔姆斯基批评LLM的论文进行回应，通过实验验证LLM学习不可能语言的能力，发现GPT-2在不可能语言上表现差，而LSTM表现符合乔姆斯基观点，提出需要转向功能主义和经验主义研究范式


<details>
  <summary>Details</summary>
Motivation: 回应乔姆斯基在《CHATGPT的虚假承诺》中对大型语言模型的批评，乔姆斯基认为LLM只是模式预测器，缺乏人类语言的内在因果和自我修正结构，无法区分不可能语言

Method: 构建一组语法上不可能的语言（包括句子反转和基于词数奇偶性的否定变换），在GPT-2小模型和LSTM模型上进行两轮对照实验，使用Welch's t-test进行统计分析

Result: GPT-2小模型在所有不可能语言上的学习表现均差于可能语言（p<0.001），LSTM模型的表现符合乔姆斯基的论点，表明transformer架构演化的不可替代作用

Conclusion: 基于理论分析和实证发现，提出在乔姆斯基理论框架内对LLM的新视角，以及从乔姆斯基的"理性主义-浪漫主义"范式转向功能主义和经验主义的理论范式转变

Abstract: In Chomsky's provocative critique "The False Promise of CHATGPT," Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critic from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch's t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p<.001). On the other hand, LSTM models' performance tallies with Chomsky's argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky's theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his "rationalist-romantics" paradigm to functionalism and empiricism in LLMs research.

</details>


### [277] [Characterizing, Evaluating, and Optimizing Complex Reasoning](https://arxiv.org/abs/2602.08498)
*Haoran Zhang,Yafu Li,Zhi Wang,Zhilin Wang,Shunkai Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 论文提出了ME²原则来定义推理质量，开发了基于DAG的推理评估方法，构建了TRM-Preference数据集和Thinking Reward Model，实验证明该奖励信号能有效优化推理性能


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对推理质量的统一定义、长推理链的可靠评估方法以及如何利用评估信号进行推理优化的系统性解决方案

Method: 引入ME²原则从宏观和微观层面评估推理效率与效果，将推理轨迹建模为有向无环图(DAG)，开发基于DAG的成对评估方法，构建TRM-Preference数据集并训练Thinking Reward Model

Result: 实验显示思维奖励是有效的优化信号：测试时选择更好的推理可获得19.3%的性能提升，RL训练中使用思维奖励可在多样任务上实现3.9%的性能提升

Conclusion: 该研究提供了推理质量评估的统一框架，通过ME²原则和DAG建模有效解决了复杂推理结构的评估问题，TRM模型为大规模推理优化提供了可靠信号

Abstract: Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.

</details>


### [278] [GISA: A Benchmark for General Information-Seeking Assistant](https://arxiv.org/abs/2602.08543)
*Yutao Zhu,Xingshuo Zhang,Maosen Zhang,Jiajie Jin,Liancheng Zhang,Xiaoshuai Song,Kangzhi Zhao,Wencong Zeng,Ruiming Tang,Han Li,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.CL

TL;DR: GISA是一个用于评估通用信息搜索助手的新基准，包含373个人工构建的真实信息搜索查询，支持四种结构化答案格式，整合了深度推理和广泛信息聚合任务，并提供完整的人类搜索轨迹作为过程监督参考。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在查询构建不自然、与真实需求不匹配、答案集静态易受数据污染等问题，需要开发更真实的评估基准来推动搜索助手的发展。

Method: 构建了373个人工精心设计的查询，涵盖四种结构化答案格式（项目、集合、列表和表格），包含实时更新的子集以防止记忆效应，并提供完整的搜索轨迹作为黄金标准参考。

Result: 实验显示即使是表现最好的模型也仅达到19.30%的精确匹配分数，在需要复杂规划和全面信息收集的任务上性能显著下降。

Conclusion: GISA基准揭示了当前搜索助手在真实信息搜索任务中的显著不足，为未来改进提供了重要参考，特别是在复杂规划和信息聚合能力方面仍有很大提升空间。

Abstract: The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.

</details>


### [279] [How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location](https://arxiv.org/abs/2602.08548)
*Xuanliang Zhang,Dingzirui Wang,Keyan Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 该研究通过激活修补和可解释性技术揭示了Transformer处理表格的三阶段机制：语义绑定、坐标定位和信息提取，发现模型通过计数分隔符的序数机制定位单元格，并在线性子空间中编码列索引，为表格理解提供了全面解释。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在表格相关任务中应用日益广泛，但其处理线性化二维结构化表格的内部机制仍不透明，需要深入理解模型如何实现表格理解。

Method: 采用激活修补技术和互补的可解释性方法，通过解构单元格定位这一原子任务来分析表格理解过程，识别出三阶段处理流程。

Result: 发现模型通过计数离散分隔符的序数机制解析坐标定位目标单元格，列索引编码在线性子空间中可通过向量算术精确控制模型注意力，多单元格定位任务通过复用相同注意力头实现泛化。

Conclusion: 研究揭示了Transformer架构中表格理解的完整机制，为理解模型如何处理结构化数据提供了重要见解，对改进表格相关任务的模型性能具有指导意义。

Abstract: While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Localization, and Information Extraction. We demonstrate that models locate the target cell via an ordinal mechanism that counts discrete delimiters to resolve coordinates. Furthermore, column indices are encoded within a linear subspace that allows for precise steering of model focus through vector arithmetic. Finally, we reveal that models generalize to multi-cell location tasks by multiplexing the identical attention heads identified during atomic location. Our findings provide a comprehensive explanation of table understanding within Transformer architectures.

</details>


### [280] [Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation](https://arxiv.org/abs/2602.08600)
*Archchana Sindhujan,Girish A. Koushik,Shenbin Qian,Diptesh Kanojia,Constantin Orăsan*

Main category: cs.CL

TL;DR: 提出了ALOPE-RL强化学习框架和首个英语-马拉雅拉姆语片段级质量评估数据集，结合直接评估分数和翻译质量评注，在低资源环境下实现最先进的机器翻译质量评估性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有质量评估方法仅依赖标量分数、缺乏错误解释信息的问题，以及在低资源语言对（如英语-马拉雅拉姆语）中标注数据有限导致的性能不可靠问题。

Method: 构建包含人工标注直接评估分数和翻译质量评注的数据集；提出基于策略的强化学习框架ALOPE-RL，通过从DA分数和TQR派生的策略奖励训练高效适配器；使用LoRA和4位量化技术在小规模LLM（≤4B参数）上实现微调。

Result: ALOPE-RL在英语-马拉雅拉姆语质量评估任务上达到最先进性能，超越了基于更大LLM的基线和领先的基于编码器的QE模型，证明了在有限数据和计算预算下仍能实现强性能。

Conclusion: 错误感知的基于策略学习能够在有限资源和数据条件下提供强大的质量评估性能，为低资源语言对的机器翻译质量评估提供了有效解决方案。

Abstract: Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.

</details>


### [281] [VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling](https://arxiv.org/abs/2602.08607)
*Ziyang Cheng,Yuhao Wang,Heyang Liu,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: VocalNet-MDM提出基于掩码扩散模型的非自回归语音大语言模型，通过分层块掩码和迭代自蒸馏技术解决训练推理不匹配和迭代开销问题，在有限数据下实现显著加速和低延迟，同时保持竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 现有自回归语音LLM存在严格的序列约束，限制了生成效率并引入曝光偏差，需要探索非自回归范式来提高效率和降低延迟。

Method: 采用掩码扩散建模(MDM)作为非自回归范式，提出分层块掩码对齐训练目标与推理时的渐进掩码状态，使用迭代自蒸馏将多步优化压缩为更少步骤以实现低延迟推理。

Result: 仅使用6K小时语音数据训练，实现3.7-10倍解码加速，首块延迟降低34%，在保持竞争性识别准确率的同时获得最先进的文本质量和语音自然度。

Conclusion: 掩码扩散模型是低延迟、高效语音LLM的一个有前景且可扩展的替代方案，为非自回归语音生成提供了有效解决方案。

Abstract: Recent Speech Large Language Models~(LLMs) have achieved impressive capabilities in end-to-end speech interaction. However, the prevailing autoregressive paradigm imposes strict serial constraints, limiting generation efficiency and introducing exposure bias. In this paper, we investigate Masked Diffusion Modeling~(MDM) as a non-autoregressive paradigm for speech LLMs and introduce VocalNet-MDM. To adapt MDM for streaming speech interaction, we address two critical challenges: training-inference mismatch and iterative overhead. We propose Hierarchical Block-wise Masking to align training objectives with the progressive masked states encountered during block diffusion decoding, and Iterative Self-Distillation to compress multi-step refinement into fewer steps for low-latency inference. Trained on a limited scale of only 6K hours of speech data, VocalNet-MDM achieves a 3.7$\times$--10$\times$ decoding speedup and reduces first-chunk latency by 34\% compared to AR baselines. It maintains competitive recognition accuracy while achieving state-of-the-art text quality and speech naturalness, demonstrating that MDM is a promising and scalable alternative for low-latency, efficient speech LLMs.

</details>


### [282] [Do Multilingual LLMs have specialized language heads?](https://arxiv.org/abs/2602.08625)
*Muhammad Naufil*

Main category: cs.CL

TL;DR: 本文研究多语言大模型中是否存在针对特定语言的注意力头，探索在保持目标语言性能的同时移除不需要语言专用头的可能性，以实现更高效的模型部署。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在生产部署中效率低下，特别是当只需要支持部分语言时。虽然已有研究探讨机器翻译模型的语言特定头，但对具有更广泛任务能力的多语言LLM的研究尚属空白。

Method: 研究多语言LLM中是否存在专门的语言注意力头，并实验性移除不需要语言的特定头，评估对目标语言性能的影响。

Result: 研究发现多语言LLM确实存在语言特定的注意力头，移除不需要语言的专用头可以在不降低目标语言性能的情况下减少模型复杂度。

Conclusion: 该研究为多语言LLM的高效部署提供了新策略，通过选择性移除语言特定组件实现模型精简，同时保持目标语言的高精度性能。

Abstract: Multilingual large language models (LLMs) have gained significant popularity for their ability to process and generate text across multiple languages. However, deploying these models in production can be inefficient when only a subset of the supported languages is of interest. There has been some research conducted on identifying whether machine translation models have language-specific or language-agnostic heads, however no research has been conducted for multilingual LLMs, to the best of our knowledge, that as we know are capable of performing diverse tasks beyond just translation. This paper explores whether multilingual LLMs have specialized language attention heads for each language, and investigates the possibility of removing language-specific heads for unwanted languages without degrading performance in the targeted languages. Our findings could inform more efficient deployment strategies for multilingual LLMs, enabling reduced model complexity while maintaining high accuracy for targeted languages.

</details>


### [283] [Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models](https://arxiv.org/abs/2602.08658)
*Mingzi Cao,Xingwei Tan,Mahmud Akhter,Marco Valentino,Maria Liakata,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本研究系统探索了演绎、归纳和溯因三种基本推理范式对大型语言模型泛化能力的影响，通过符号任务数据集训练LLMs，并在现实自然语言任务中验证了高达14.60%的性能提升


<details>
  <summary>Details</summary>
Motivation: 尽管LLM推理改进研究众多，但基础推理范式如何影响模型泛化能力尚未得到系统研究，需要探索这些核心范式之间的相互作用对LLM推理行为的影响

Method: 收集针对三种推理范式的符号任务轨迹数据集，采用多种方法训练LLMs（包括简单微调、增加模型深度、将稠密模型转换为专家混合模型），并在包含现实知识的自然语言任务上进行全面评估

Result: 方法在现实跨域任务中展现出强泛化能力，性能提升显著（最高达14.60%）

Conclusion: 通过系统训练基础推理范式可以有效提升LLM在现实任务中的推理性能，证明了基础逻辑思维范式对模型泛化能力的重要作用

Abstract: Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.

</details>


### [284] [Learning to Judge: LLMs Designing and Applying Evaluation Rubrics](https://arxiv.org/abs/2602.08672)
*Clemencia Siro,Pourya Aliannejadi,Mohammad Aliannejadi*

Main category: cs.CL

TL;DR: GER-Eval研究LLM能否自主设计并应用评估标准，发现LLM能生成可解释且任务感知的评估维度，但在事实性任务中评分可靠性下降，闭源模型表现优于开源模型。


<details>
  <summary>Details</summary>
Motivation: 人类评估标准往往是静态的，与LLM内部语言质量表示方式不匹配，需要探索LLM能否设计自己的评估标准。

Method: 引入GER-Eval框架，评估LLM生成的评估标准的语义一致性、评分可靠性以及与人类标准的对齐程度。

Result: LLM能可靠生成可解释的任务感知评估维度，但在事实性和知识密集型任务中评分可靠性下降；闭源模型（如GPT-4o）比开源模型（如Llama）具有更高的一致性和跨模型泛化能力。

Conclusion: 评估是LLM的一种学习到的语言能力，在模型内部一致但在不同模型间存在差异，需要开发同时建模人类和LLM评估语言的新方法来提高可靠性和可解释性。

Abstract: Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.

</details>


### [285] [Old wine in old glasses: Comparing computational and qualitative methods in identifying incivility on Persian Twitter during the #MahsaAmini movement](https://arxiv.org/abs/2602.08688)
*Hossein Kermani,Fatemeh Oudlajani,Pardis Yarahmadi,Hamideh Mahdi Soltani,Mohammad Makki,Zahra HosseiniKhoo*

Main category: cs.CL

TL;DR: 对比波斯语推文不文明内容检测的三种方法：人工标注、ParsBERT监督学习和ChatGPT大语言模型，发现ParsBERT在仇恨言论识别上显著优于ChatGPT，后者在隐晦和显性不文明内容处理上均存在困难


<details>
  <summary>Details</summary>
Motivation: 评估不同方法在低资源语言（波斯语）环境下检测社交媒体不文明内容的有效性和效率，特别是在#MahsaAmini运动背景下的仇恨言论分析

Method: 使用47,278条波斯语推文，比较人工定性标注、基于ParsBERT的监督学习方法和7种ChatGPT模型的表现，分析准确性和效率指标

Result: ParsBERT在仇恨言论检测方面显著优于所有测试的ChatGPT模型；ChatGPT不仅在隐晦案例上表现不佳，对显性不文明内容也存在识别困难；提示语言（英语vs波斯语）对ChatGPT输出无显著影响

Conclusion: 研究为低资源语言环境下的仇恨言论分析提供了详细的方法对比，明确了各种方法的优势与局限，表明专门训练的监督学习模型（如ParsBERT）在当前阶段优于通用大语言模型

Abstract: This paper compares three approaches to detecting incivility in Persian tweets: human qualitative coding, supervised learning with ParsBERT, and large language models (ChatGPT). Using 47,278 tweets from the #MahsaAmini movement in Iran, we evaluate the accuracy and efficiency of each method. ParsBERT substantially outperforms seven evaluated ChatGPT models in identifying hate speech. We also find that ChatGPT struggles not only with subtle cases but also with explicitly uncivil content, and that prompt language (English vs. Persian) does not meaningfully affect its outputs. The study provides a detailed comparison of these approaches and clarifies their strengths and limitations for analyzing hate speech in a low-resource language context.

</details>


### [286] [Challenges in Translating Technical Lectures: Insights from the NPTEL](https://arxiv.org/abs/2602.08698)
*Basudha Raje,Sadanand Venkatraman,Nandana TP,Soumyadeepa Das,Polkam Poojitha,M. Vijaykumar,Tanima Bagchi,Hema A. Murthy*

Main category: cs.CL

TL;DR: 本研究探讨了机器翻译在印度语言（孟加拉语、马拉雅拉姆语和泰卢固语）中的实际应用和方法论意义，重点关注新兴翻译工作流程和现有评估框架的关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于印度国家教育政策（NEP 2020）对多语言教育技术的需求，以及NPTEL大规模开放在线课程平台提供的语料资源，旨在解决技术概念在多样化语言环境中的有效传播问题。

Method: 使用NPTEL MOOC门户作为语料库，构建自发语音语料库，重点保留适当语域和词汇选择，分析形态丰富和语义紧凑的语言特征对表面重叠评估指标的敏感性。

Result: 研究发现表面重叠评估指标在面对形态丰富和语义紧凑的语言特征时存在局限性，显示出度量特定的敏感性挑战。

Conclusion: 研究强调在印度这样语言多样化的国家中，需要开发更适合形态丰富语言的评估框架，以支持教育技术的有效多语言适应。

Abstract: This study examines the practical applications and methodological implications of Machine Translation in Indian Languages, specifically Bangla, Malayalam, and Telugu, within emerging translation workflows and in relation to existing evaluation frameworks. The choice of languages prioritized in this study is motivated by a triangulation of linguistic diversity, which illustrates the significance of multilingual accommodation of educational technology under NEP 2020. This is further supported by the largest MOOC portal, i.e., NPTEL, which has served as a corpus to facilitate the arguments presented in this paper. The curation of a spontaneous speech corpora that accounts for lucid delivery of technical concepts, considering the retention of suitable register and lexical choices are crucial in a diverse country like India. The findings of this study highlight metric-specific sensitivity and the challenges of morphologically rich and semantically compact features when tested against surface overlapping metrics.

</details>


### [287] [Do Images Clarify? A Study on the Effect of Images on Clarifying Questions in Conversational Search](https://arxiv.org/abs/2602.08700)
*Clemencia Siro,Zahra Abbasiantaeb,Yifei Yuan,Mohammad Aliannejadi,Maarten de Rijke*

Main category: cs.CL

TL;DR: 研究通过用户实验探讨了多模态（图文结合）澄清问题在对话搜索中的作用，发现图像对用户偏好和性能的影响因任务类型和用户专业水平而异。


<details>
  <summary>Details</summary>
Motivation: 尽管图像在各种情境下已被证明能提升检索性能，但在对话搜索澄清问题中融入图像对用户体验的影响尚未充分研究，需要探索图文结合的澄清问题对用户搜索任务的影响。

Method: 对73名参与者进行用户研究，比较多模态和纯文本澄清问题在回答澄清问题和查询重构两个任务中的效果，从多个角度分析图像的作用。

Result: 在回答澄清问题时用户强烈偏好多模态问题但纯文本设置表现更好；在查询重构中图像能产生更精确的查询并提升检索性能；图像效果受任务类型和用户专业水平影响。

Conclusion: 视觉增强的益处具有任务依赖性，应根据具体搜索情境和用户特征进行战略实施，为设计有效的多模态对话搜索系统提供了重要见解。

Abstract: Conversational search systems increasingly employ clarifying questions to refine user queries and improve the search experience. Previous studies have demonstrated the usefulness of text-based clarifying questions in enhancing both retrieval performance and user experience. While images have been shown to improve retrieval performance in various contexts, their impact on user performance when incorporated into clarifying questions remains largely unexplored. We conduct a user study with 73 participants to investigate the role of images in conversational search, specifically examining their effects on two search-related tasks: (i) answering clarifying questions and (ii) query reformulation. We compare the effect of multimodal and text-only clarifying questions in both tasks within a conversational search context from various perspectives. Our findings reveal that while participants showed a strong preference for multimodal questions when answering clarifying questions, preferences were more balanced in the query reformulation task. The impact of images varied with both task type and user expertise. In answering clarifying questions, images helped maintain engagement across different expertise levels, while in query reformulation they led to more precise queries and improved retrieval performance. Interestingly, for clarifying question answering, text-only setups demonstrated better user performance as they provided more comprehensive textual information in the absence of images. These results provide valuable insights for designing effective multimodal conversational search systems, highlighting that the benefits of visual augmentation are task-dependent and should be strategically implemented based on the specific search context and user characteristics.

</details>


### [288] [FactSim: Fact-Checking for Opinion Summarization](https://arxiv.org/abs/2602.08709)
*Leandro Anghinoni,Jorge Sanchez*

Main category: cs.CL

TL;DR: 提出了一种新颖的自动化方法，用于评估生成式AI在意见摘要任务中的事实一致性，通过比较摘要声明与原始评论的相似性来改进传统评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于自动化指标评估生成式AI文本摘要的方法在大语言模型时代存在局限性，特别是在意见摘要领域需要更全面精确的评估技术。

Method: 基于声明提取和相似性比较的方法，从文本中提取事实评估内容，通过测量摘要声明与原始评论声明的相似性来评估覆盖率和一致性。

Result: 所提出的指标能够为相似声明（无论是否被否定、改写或扩展）分配更高分数，且与人类判断的相关性优于最先进指标。

Conclusion: 该方法为生成式AI文本摘要的事实一致性评估提供了有效的自动化解决方案，解决了传统评估方法的不足。

Abstract: We explore the need for more comprehensive and precise evaluation techniques for generative artificial intelligence (GenAI) in text summarization tasks, specifically in the area of opinion summarization. Traditional methods, which leverage automated metrics to compare machine-generated summaries from a collection of opinion pieces, e.g. product reviews, have shown limitations due to the paradigm shift introduced by large language models (LLM). This paper addresses these shortcomings by proposing a novel, fully automated methodology for assessing the factual consistency of such summaries. The method is based on measuring the similarity between the claims in a given summary with those from the original reviews, measuring the coverage and consistency of the generated summary. To do so, we rely on a simple approach to extract factual assessment from texts that we then compare and summarize in a suitable score. We demonstrate that the proposed metric attributes higher scores to similar claims, regardless of whether the claim is negated, paraphrased, or expanded, and that the score has a high correlation to human judgment when compared to state-of-the-art metrics.

</details>


### [289] [PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments](https://arxiv.org/abs/2602.08716)
*Shangrui Nie,Kian Omoomi,Lucie Flek,Zhixue Zhao,Charles Welch*

Main category: cs.CL

TL;DR: PERSPECTRA是一个新的多元化基准测试，结合了Kialo辩论图的结构清晰性和Reddit讨论的语言多样性，包含3810个扩展论点，覆盖100个争议话题的762个正反立场，用于评估LLM的多元化理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLM多元化能力的系统评估，Reddit提供语言多样性但缺乏论证结构，Kialo有明确的正反图但过于简洁且脱离自然话语，需要结合两者优势来创建有效的评估基准。

Method: 使用受控的检索和扩展流程，构建了3810个扩展论点，每个观点都有多个自然变体，设计了三个任务：观点计数、观点匹配和极性检查，用于评估LLM的多元化能力。

Result: 实验表明，最先进的开源和专有LLM在多元化理解方面存在系统性失败，如高估观点数量和错误分类让步结构，突显了多元化感知理解和推理的困难。

Conclusion: PERSPECTRA通过结合多样性和结构，建立了第一个可扩展、可配置的基准，用于评估模型如何表示、区分和推理多个观点，填补了LLM多元化评估的空白。

Abstract: Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.

</details>


### [290] [Map of Encoders -- Mapping Sentence Encoders using Quantum Relative Entropy](https://arxiv.org/abs/2602.08740)
*Gaifan Zhang,Danushka Bollegala*

Main category: cs.CL

TL;DR: 提出一种大规模比较和可视化句子编码器的方法，通过创建编码器地图来展示各编码器之间的相对关系，覆盖1101个公开可用的预训练句子编码器。


<details>
  <summary>Details</summary>
Motivation: 需要一种系统性的方法来比较和理解大量句子编码器的特性和性能，为研究社区提供编码器选择的指导。

Method: 首先用句子集的嵌入矩阵表示每个编码器，计算其成对内积(PIP)矩阵，然后基于量子相对熵(QRE)构建每个编码器相对于单位基编码器的特征向量。

Result: 构建的编码器地图准确反映了编码器间的各种关系，相似属性的编码器在地图上位置相近，特征向量能准确预测编码器在下游任务（如检索和聚类）中的性能。

Conclusion: 该方法为预训练句子编码器领域提供了新的视角，所构建的地图具有高保真度，能够有效指导编码器选择和性能预测。

Abstract: We propose a method to compare and visualise sentence encoders at scale by creating a map of encoders where each sentence encoder is represented in relation to the other sentence encoders. Specifically, we first represent a sentence encoder using an embedding matrix of a sentence set, where each row corresponds to the embedding of a sentence. Next, we compute the Pairwise Inner Product (PIP) matrix for a sentence encoder using its embedding matrix. Finally, we create a feature vector for each sentence encoder reflecting its Quantum Relative Entropy (QRE) with respect to a unit base encoder. We construct a map of encoders covering 1101 publicly available sentence encoders, providing a new perspective of the landscape of the pre-trained sentence encoders. Our map accurately reflects various relationships between encoders, where encoders with similar attributes are proximally located on the map. Moreover, our encoder feature vectors can be used to accurately infer downstream task performance of the encoders, such as in retrieval and clustering tasks, demonstrating the faithfulness of our map.

</details>


### [291] [LakeHopper: Cross Data Lakes Column Type Annotation through Model Adaptation](https://arxiv.org/abs/2602.08793)
*Yushi Sun,Xujia Li,Nan Tang,Quanqing Xu,Chuanhui Yang,Lei Chen*

Main category: cs.CL

TL;DR: LakeHopper是一个用于跨数据湖列类型标注的迁移学习框架，通过识别知识鸿沟、聚类选择信息和增量微调机制，实现在目标数据湖上最小化标注需求的高效模型适配。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的列类型标注方法需要大量标注数据，且针对特定数据湖训练的模型难以直接迁移到新数据湖。研究旨在解决源-目标数据湖之间的知识鸿沟问题，减少在新数据湖上的标注需求。

Method: 提出LakeHopper框架：1)通过LM交互识别和解决知识鸿沟；2)采用基于聚类的数据选择方案选择信息量大的未标注列；3)使用增量微调机制逐步将源模型适配到目标数据湖。

Result: 在两个不同数据湖迁移任务上验证了LakeHopper的有效性，在低资源和高资源设置下均表现出色。

Conclusion: LakeHopper成功解决了跨数据湖列类型标注的迁移学习挑战，通过系统性方法实现了模型的有效适配，显著减少了在新数据湖上的标注需求。

Abstract: Column type annotation is vital for tasks like data cleaning, integration, and visualization. Recent solutions rely on resource-intensive language models fine-tuned on well-annotated columns from a particular set of tables, i.e., a source data lake. In this paper, we study whether we can adapt an existing pre-trained LM-based model to a new (i.e., target) data lake to minimize the annotations required on the new data lake. However, challenges include the source-target knowledge gap, selecting informative target data, and fine-tuning without losing shared knowledge exist. We propose LakeHopper, a framework that identifies and resolves the knowledge gap through LM interactions, employs a cluster-based data selection scheme for unannotated columns, and uses an incremental fine-tuning mechanism that gradually adapts the source model to the target data lake. Our experimental results validate the effectiveness of LakeHopper on two different data lake transfers under both low-resource and high-resource settings.

</details>


### [292] [Affective Flow Language Model for Emotional Support Conversation](https://arxiv.org/abs/2602.08826)
*Chenghui Zou,Ning Wang,Tiesunlong Shen,Luwei Xiao,Chuan Ma,Xiangpeng Li,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: AFlow框架通过建模多轮对话中的情感流，为情感支持对话提供细粒度监督，显著提升中间策略决策质量，在多个ESC指标上超越GPT-4o和Claude-3.5等专有模型


<details>
  <summary>Details</summary>
Motivation: 现有情感支持对话系统依赖稀疏的结果级信号，对中间策略决策的监督有限，难以处理复杂的多轮支持对话

Method: 提出AFlow框架，通过建模连续情感流对对话前缀进行细粒度监督，估计搜索轨迹中的中间效用并学习偏好一致的策略转换，采用子路径级流平衡目标提升策略连贯性和共情响应质量

Result: 在多样化情感情境中相比竞争基线取得一致且显著的改进，紧凑开源骨干网络在主要ESC指标上超越专有大型语言模型

Conclusion: AFlow通过引入情感流监督有效解决了多轮情感支持对话中的策略决策问题，为LLM在情感支持应用提供了更有效的对齐方案

Abstract: Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.

</details>


### [293] [WildReward: Learning Reward Models from In-the-Wild Human Interactions](https://arxiv.org/abs/2602.08829)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: WildReward：从WildChat用户交互中直接提取隐式奖励信号训练奖励模型，无需人工标注偏好对，在186k高质量实例上通过序数回归训练，性能媲美甚至超越传统奖励模型。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型依赖大规模人工标注的偏好对，而LLM的广泛部署产生了大量用户交互数据，这些隐式奖励信号能否直接用于训练奖励模型是一个重要研究问题。

Method: 采用WildChat作为交互数据源，提出pipeline提取可靠的人类反馈，通过序数回归直接在用户反馈上训练WildReward，无需偏好对标注。

Result: WildReward在广泛实验中表现出与传统奖励模型相当甚至更优的性能，具有更好的校准性和跨样本一致性，用户多样性直接带来性能提升，在在线DPO训练中显著改进各种任务。

Conclusion: 从野外交互中直接训练奖励模型是可行的，WildReward证明了隐式用户反馈可以替代人工标注偏好对，为奖励模型训练提供了更可扩展和经济高效的方法。

Abstract: Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.

</details>


### [294] [Understanding Dynamic Compute Allocation in Recurrent Transformers](https://arxiv.org/abs/2602.08864)
*Ibraheem Muhammad Moosa,Suhas Lohit,Ye Wang,Moitreya Chatterjee,Wenpeng Yin*

Main category: cs.CL

TL;DR: 本研究提出了一个复杂度控制的评估范式来评估token级自适应计算，开发了ANIRA统一框架进行系统分析，发现计算分配能与任务复杂度对齐但缺乏泛化能力，且早期计算决策依赖静态结构线索而非算法执行状态。


<details>
  <summary>Details</summary>
Motivation: 先前工作主要在自然语言基准上评估token级自适应计算，但token级难度不可观测且与架构因素混淆，无法确定计算分配是否真正与底层复杂度对齐。

Method: 引入复杂度控制的评估范式（算法和合成语言任务），提出ANIRA统一循环Transformer框架支持每token可变深度计算，隔离计算分配决策与其他模型因素，进行系统分析。

Result: 计算分配能在无显式难度监督下与任务复杂度对齐，但这种对齐不意味着算法泛化：模型无法外推到未见输入大小。早期计算决策依赖静态结构线索，而在线停止更接近算法执行状态。

Conclusion: 需要新的评估范式来理解token级自适应计算，计算分配与复杂度的对齐可通过无监督方式实现，但泛化能力有限，决策时机对计算分配质量有重要影响。

Abstract: Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.

</details>


### [295] [Large Language Models for Geolocation Extraction in Humanitarian Crisis Response](https://arxiv.org/abs/2602.08872)
*G. Cafferata,T. Demarco,K. Kalimeri,Y. Mejova,M. G. Beiró*

Main category: cs.CL

TL;DR: LLM-based two-step framework improves location extraction accuracy and fairness in humanitarian texts, particularly for underrepresented regions.


<details>
  <summary>Details</summary>
Motivation: Humanitarian crisis response requires accurate geographic information, but existing automated systems reproduce geographic and socioeconomic biases, leading to uneven visibility of crisis-affected areas.

Method: Two-step framework combining few-shot LLM-based named entity recognition with agent-based geocoding module that uses context to resolve ambiguous place names, benchmarked against state-of-the-art pretrained and rule-based systems.

Result: LLM-based methods significantly improve both precision and fairness of geolocation extraction, especially for underrepresented regions, using extended HumSet dataset with refined toponym annotations.

Conclusion: The work bridges LLM reasoning advances with responsible AI principles, contributing to more equitable geospatial data systems for humanitarian response and advancing the goal of inclusive crisis analytics.

Abstract: Humanitarian crises demand timely and accurate geographic information to inform effective response efforts. Yet, automated systems that extract locations from text often reproduce existing geographic and socioeconomic biases, leading to uneven visibility of crisis-affected regions. This paper investigates whether Large Language Models (LLMs) can address these geographic disparities in extracting location information from humanitarian documents. We introduce a two-step framework that combines few-shot LLM-based named entity recognition with an agent-based geocoding module that leverages context to resolve ambiguous toponyms. We benchmark our approach against state-of-the-art pretrained and rule-based systems using both accuracy and fairness metrics across geographic and socioeconomic dimensions. Our evaluation uses an extended version of the HumSet dataset with refined literal toponym annotations. Results show that LLM-based methods substantially improve both the precision and fairness of geolocation extraction from humanitarian texts, particularly for underrepresented regions. By bridging advances in LLM reasoning with principles of responsible and inclusive AI, this work contributes to more equitable geospatial data systems for humanitarian response, advancing the goal of leaving no place behind in crisis analytics.

</details>


### [296] [Is Reasoning Capability Enough for Safety in Long-Context Language Models?](https://arxiv.org/abs/2602.08874)
*Yu Fu,Haz Sameen Shahgir,Huanli Gong,Zhipeng Wei,N. Benjamin Erichson,Yue Dong*

Main category: cs.CL

TL;DR: 研究发现更强的推理能力并不自动提升大语言模型在长上下文环境中的安全性，反而可能通过组合推理攻击导致安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 测试大型语言模型在长上下文环境中，当有害意图通过推理隐含表达时的安全性能，验证推理能力提升安全性的假设是否成立。

Method: 引入组合推理攻击威胁模型，将有害查询分解为不完整片段散布在长上下文中，通过中性推理查询诱导模型检索和合成，使有害意图在组合后显现。评估14个前沿LLM在长达64K token上下文中的表现。

Result: 1) 更强推理能力的模型对组合推理攻击并不更鲁棒；2) 安全对齐性能随上下文长度增加而下降；3) 增加推理时计算可将攻击成功率降低50%以上。

Conclusion: 安全性不会随推理能力自动扩展，特别是在长上下文推理场景下，需要专门的安全措施来应对组合推理攻击。

Abstract: Large language models (LLMs) increasingly combine long-context processing with advanced reasoning, enabling them to retrieve and synthesize information distributed across tens of thousands of tokens. A hypothesis is that stronger reasoning capability should improve safety by helping models recognize harmful intent even when it is not stated explicitly. We test this hypothesis in long-context settings where harmful intent is implicit and must be inferred through reasoning, and find that it does not hold. We introduce compositional reasoning attacks, a new threat model in which a harmful query is decomposed into incomplete fragments that scattered throughout a long context. The model is then prompted with a neutral reasoning query that induces retrieval and synthesis, causing the harmful intent to emerge only after composition. Evaluating 14 frontier LLMs on contexts up to 64k tokens, we uncover three findings: (1) models with stronger general reasoning capability are not more robust to compositional reasoning attacks, often assembling the intent yet failing to refuse; (2) safety alignment consistently degrades as context length increases; and (3) inference-time reasoning effort is a key mitigating factor: increasing inference-time compute reduces attack success by over 50 percentage points on GPT-oss-120b model. Together, these results suggest that safety does not automatically scale with reasoning capability, especially under long-context inference.

</details>


### [297] [GitSearch: Enhancing Community Notes Generation with Gap-Informed Targeted Search](https://arxiv.org/abs/2602.08945)
*Sahajpreet Singh,Kokil Jaidka,Min-Yen Kan*

Main category: cs.CL

TL;DR: GitSearch框架通过识别信息缺口、实时网络检索和合成平台合规笔记的三阶段流程，有效解决社区审核中的冷启动问题，在政治推文基准测试中实现99%覆盖率和优于人工笔记的质量表现


<details>
  <summary>Details</summary>
Motivation: 社区审核作为可扩展的集中式事实核查替代方案面临结构性挑战，特别是冷启动场景下现有AI方法失效的问题

Method: 提出GitSearch框架，将人类感知的质量缺口（如缺失上下文）作为一级信号，采用三阶段流程：识别信息缺陷、执行实时定向网络检索、合成平台合规笔记

Result: 在包含78,698条美国政治推文的PolBench基准测试中，GitSearch实现99%覆盖率（几乎是现有最佳方法的两倍），以69%的胜率和更高的有用性评分（3.87 vs 3.36）超越人工撰写的有用笔记

Conclusion: GitSearch通过有效平衡规模与质量的权衡，展示了检索效率的优越性，为社区审核系统提供了有效的冷启动解决方案

Abstract: Community-based moderation offers a scalable alternative to centralized fact-checking, yet it faces significant structural challenges, and existing AI-based methods fail in "cold start" scenarios. To tackle these challenges, we introduce GitSearch (Gap-Informed Targeted Search), a framework that treats human-perceived quality gaps, such as missing context, etc., as first-class signals. GitSearch has a three-stage pipeline: identifying information deficits, executing real-time targeted web-retrieval to resolve them, and synthesizing platform-compliant notes. To facilitate evaluation, we present PolBench, a benchmark of 78,698 U.S. political tweets with their associated Community Notes. We find GitSearch achieves 99% coverage, almost doubling coverage over the state-of-the-art. GitSearch surpasses human-authored helpful notes with a 69% win rate and superior helpfulness scores (3.87 vs. 3.36), demonstrating retrieval effectiveness that balanced the trade-off between scale and quality.

</details>


### [298] [How Should We Model the Probability of a Language?](https://arxiv.org/abs/2602.08951)
*Rasul Dent,Pedro Ortiz Suarez,Thibault Clérice,Benoît Sagot*

Main category: cs.CL

TL;DR: 论文认为当前语言识别系统覆盖范围有限的问题主要是由于将LID框架为去上下文的文本分类，忽视了先验概率估计的重要性，并提出应重新将LID视为路由问题并整合环境线索。


<details>
  <summary>Details</summary>
Motivation: 商业语言识别系统仅能可靠识别数百种书面语言，研究级系统在特定情况下扩展了覆盖范围，但大多数语言的覆盖仍然不完整或不存在，这种状况主要是自我强加的。

Method: 通过批判性分析当前LID框架，提出将语言识别重新概念化为路由问题，并开发原则性方法来整合使语言在本地合理的环境线索。

Result: 指出了当前LID方法的局限性，提出了改进尾端语言覆盖的理论框架和方法论方向。

Conclusion: 改进尾端语言的覆盖需要重新思考LID的基本框架，从去上下文的文本分类转向考虑环境线索和先验概率的路由问题，这需要改变机构激励机制和模型设计方法。

Abstract: Of the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persistent framing of LID as decontextualized text classification, which obscures the central role of prior probability estimation and is reinforced by institutional incentives that favor global, fixed-prior models. We argue that improving coverage for tail languages requires rethinking LID as a routing problem and developing principled ways to incorporate environmental cues that make languages locally plausible.

</details>


### [299] [Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models](https://arxiv.org/abs/2602.08984)
*Yuliang Liu,Yunchong Song,Yixuan Wang,Kewen Ge,Alex Lamb,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出Next Concept Prediction (NCP)预训练范式，通过预测跨多个token的离散概念来构建更难的预训练目标，在13个基准测试中相比传统token级模型获得一致性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统Next Token Prediction (NTP)在语言模型预训练中存在局限性，需要更难的预训练任务来提升模型能力。

Method: 开发ConceptLM模型，使用向量量化对隐藏状态进行量化，构建概念词汇表，同时利用NCP和NTP进行参数更新，通过生成概念来指导后续token的生成。

Result: 在70M到1.5B参数规模、300B训练数据的实验中，NCP相比传统模型获得一致性能提升；在8B参数的Llama模型上持续预训练也显示NCP能进一步改进NTP训练的模型。

Conclusion: NCP通过引入更难的预训练任务，为构建更强大的语言模型提供了有前景的路径。

Abstract: We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.

</details>


### [300] [When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents](https://arxiv.org/abs/2602.08995)
*Yuting Ning,Jaylen Jones,Zhehao Zhang,Chentao Ye,Weitong Ruan,Junyi Li,Rahul Gupta,Huan Sun*

Main category: cs.CL

TL;DR: 该研究首次系统定义和分析了计算机使用代理(CUAs)中的行为偏差检测问题，构建了包含人类标注的MisActBench基准，并提出了DeAction防护机制，在离线和在线评估中均显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理经常产生偏离用户原始意图的偏差行为，这些行为可能来自外部攻击(如间接提示注入)或内部限制(如错误推理)，不仅带来安全风险，还降低任务效率和可靠性。

Method: 研究定义了CUA行为偏差检测问题，识别了三种现实部署中的常见类别，构建了MisActBench基准数据集，并提出DeAction防护机制——在执行前检测偏差行为并通过结构化反馈迭代修正。

Result: DeAction在离线评估中F1分数比基线高出15%以上；在线评估中在对抗环境下将攻击成功率降低90%以上，同时在良性环境中保持甚至提高任务成功率，延迟开销适中。

Conclusion: 该工作为CUA行为偏差检测提供了系统框架和有效解决方案，DeAction作为一种实用且通用的防护机制，显著提升了CUA的安全性和可靠性。

Abstract: Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.

</details>
