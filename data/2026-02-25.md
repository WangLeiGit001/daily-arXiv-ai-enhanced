<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 101]
- [cs.CL](#cs.CL) [Total: 33]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VISION-ICE: Video-based Interpretation and Spatial Identification of Arrhythmia Origins via Neural Networks in Intracardiac Echocardiography](https://arxiv.org/abs/2602.20165)
*Dorsa EPMoghaddam,Feng Gao,Drew Bernard,Kavya Sinha,Mehdi Razavi,Behnaam Aazhang*

Main category: cs.CV

TL;DR: AI框架利用心腔内超声心动图(ICE)视频数据进行心律失常定位，通过3D卷积神经网络实现三类分类(正常窦性心律、左侧和右侧心律失常)，准确率达66.2%，显著优于随机基线，展示了该技术在心脏消融手术中的临床潜力。


<details>
  <summary>Details</summary>
Motivation: 传统高密度标测技术和术前CT/MRI在心律失常定位方面耗时耗资源，需要开发更快速准确的实时分析方法来指导电生理手术。

Method: 将心律失常源定位构建为三分类任务，基于ICE视频数据开发3D卷积神经网络，区分正常窦性心律、左侧心律失常和右侧心律失常，采用十折交叉验证评估。

Result: 模型在四个未见患者上的平均准确率达到66.2%，显著超过33.3%的随机基线水平，证明了该方法的可行性。

Conclusion: ICE成像结合深度学习可实现自动化的心律失常定位，有望缩短电生理介入手术时间并减轻手术负担，未来需扩大数据集以提高模型的鲁棒性和泛化能力。

Abstract: Contemporary high-density mapping techniques and preoperative CT/MRI remain time and resource intensive in localizing arrhythmias. AI has been validated as a clinical decision aid in providing accurate, rapid real-time analysis of echocardiographic images. Building on this, we propose an AI-enabled framework that leverages intracardiac echocardiography (ICE), a routine part of electrophysiology procedures, to guide clinicians toward areas of arrhythmogenesis and potentially reduce procedural time. Arrhythmia source localization is formulated as a three-class classification task, distinguishing normal sinus rhythm, left-sided, and right-sided arrhythmias, based on ICE video data. We developed a 3D Convolutional Neural Network trained to discriminate among the three aforementioned classes. In ten-fold cross-validation, the model achieved a mean accuracy of 66.2% when evaluated on four previously unseen patients (substantially outperforming the 33.3% random baseline). These results demonstrate the feasibility and clinical promise of using ICE videos combined with deep learning for automated arrhythmia localization. Leveraging ICE imaging could enable faster, more targeted electrophysiological interventions and reduce the procedural burden of cardiac ablation. Future work will focus on expanding the dataset to improve model robustness and generalizability across diverse patient populations.

</details>


### [2] [OTPrune: Distribution-Aligned Visual Token Pruning via Optimal Transport](https://arxiv.org/abs/2602.20205)
*Xiwen Chen,Wenhui Zhu,Gen Li,Xuanzhao Dong,Yujian Xiong,Hao Wang,Peijie Qiu,Qingquan Song,Zhipeng Wang,Shao Tang,Yalin Wang,Abolfazl Razi*

Main category: cs.CV

TL;DR: OTPrune是一种无需训练的多模态大语言模型视觉token剪枝框架，通过最优传输理论实现分布对齐，在减少计算成本的同时保持视觉表示的局部多样性和全局代表性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉token剪枝方法忽视了视觉表示的分布结构特征，导致剪枝效果不佳。多模态大语言模型推理成本高，需要更高效的剪枝方法。

Method: 提出OTPrune框架，将剪枝问题形式化为最优传输问题，最小化完整token分布与剪枝后分布的2-Wasserstein距离。推导出可处理的子模目标函数，确保单调性和子模性。

Result: 在广泛基准测试中，OTPrune相比最先进方法实现了更好的性能-效率权衡，在减少推理成本的同时保持了语义保真度。

Conclusion: OTPrune通过分布对齐方法为视觉token剪枝提供了理论保证和实际有效性，为多模态大语言模型的高效推理提供了新思路。

Abstract: Multi-modal large language models (MLLMs) achieve strong visual-language reasoning but suffer from high inference cost due to redundant visual tokens. Recent work explores visual token pruning to accelerate inference, while existing pruning methods overlook the underlying distributional structure of visual representations. We propose OTPrune, a training-free framework that formulates pruning as distribution alignment via optimal transport (OT). By minimizing the 2-Wasserstein distance between the full and pruned token distributions, OTPrune preserves both local diversity and global representativeness while reducing inference cost. Moreover, we derive a tractable submodular objective that enables efficient optimization, and theoretically prove its monotonicity and submodularity, providing a principled foundation for stable and efficient pruning. We further provide a comprehensive analysis that explains how distributional alignment contributes to stable and semantically faithful pruning. Comprehensive experiments on wider benchmarks demonstrate that OTPrune achieves superior performance-efficiency tradeoffs compared to state-of-the-art methods. The code is available at https://github.com/xiwenc1/OTPrune.

</details>


### [3] [De-rendering, Reasoning, and Repairing Charts with Vision-Language Models](https://arxiv.org/abs/2602.20291)
*Valentin Bonas,Martin Sinnona,Viviana Siless,Emmanuel Iarussi*

Main category: cs.CV

TL;DR: 提出一个结合图表反渲染、自动化分析和迭代改进的框架，为可视化设计提供可操作的反馈建议，通过视觉语言推理识别设计缺陷并基于可视化原则提出具体修改方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的可视化检查工具缺乏上下文理解，无法提供有意义的改进建议；通用LLM在可视化质量评估方面不可靠，常产生不一致或错误的反馈。

Method: 系统从图像重建图表结构，使用视觉语言推理识别设计缺陷，基于可视化研究原则提出具体修改建议，支持用户选择性应用改进并重新渲染更新后的图表。

Result: 在Chart2Code基准测试的1000个图表上生成了10,452条设计建议，聚类为10个连贯类别（如轴格式化、颜色可访问性、图例一致性等）。

Conclusion: LLM驱动的推荐系统在提供结构化、基于原则的可视化设计反馈方面具有潜力，为开发更智能和易用的创作工具开辟了新途径。

Abstract: Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.

</details>


### [4] [N4MC: Neural 4D Mesh Compression](https://arxiv.org/abs/2602.20312)
*Guodong Chen,Huanshuo Dong,Mallesham Dasari*

Main category: cs.CV

TL;DR: N4MC是首个4D神经网格压缩框架，通过利用时变网格序列的时间冗余性实现高效压缩。该方法将不规则网格帧转换为规则4D张量，使用自解码器捕获时空相关性，并引入基于Transformer的插值模型增强时间一致性，在率失真性能上优于现有方法且支持实时解码。


<details>
  <summary>Details</summary>
Motivation: 现有神经网格压缩方法独立处理每帧网格，忽略了时间冗余性。受2D视频编解码器中帧间压缩的启发，需要开发能够利用网格序列时间相关性的压缩方法。

Method: 1. 将连续不规则网格帧转换为规则4D张量；2. 使用自解码器压缩张量，捕获时空相关性；3. 引入基于Transformer的插值模型，通过跟踪体积中心的潜在嵌入预测中间网格帧，消除运动模糊。

Result: 在广泛评估中，N4MC在率失真性能方面优于现有最先进方法，同时能够实现4D网格序列的实时解码。

Conclusion: N4MC成功实现了首个4D神经网格压缩框架，通过有效利用时间冗余性显著提升了压缩效率，为动态网格序列的高效压缩提供了新思路。

Abstract: We present N4MC, the first 4D neural compression framework to efficiently compress time-varying mesh sequences by exploiting their temporal redundancy. Unlike prior neural mesh compression methods that treat each mesh frame independently, N4MC takes inspiration from inter-frame compression in 2D video codecs, and learns motion compensation in long mesh sequences. Specifically, N4MC converts consecutive irregular mesh frames into regular 4D tensors to provide a uniform and compact representation. These tensors are then condensed using an auto-decoder, which captures both spatial and temporal correlations for redundancy removal. To enhance temporal coherence, we introduce a transformer-based interpolation model that predicts intermediate mesh frames conditioned on latent embeddings derived from tracked volume centers, eliminating motion ambiguities. Extensive evaluations show that N4MC outperforms state-of-the-art in rate-distortion performance, while enabling real-time decoding of 4D mesh sequences. The implementation of our method is available at: https://github.com/frozzzen3/N4MC.

</details>


### [5] [GSNR: Graph Smooth Null-Space Representation for Inverse Problems](https://arxiv.org/abs/2602.20328)
*Romario Gualdrón-Hurtado,Roman Jacome,Rafael S. Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 提出Graph-Smooth Null-Space Representation (GSNR)方法，通过图拉普拉斯算子将结构信息引入逆问题的零空间分量，提高图像重建质量


<details>
  <summary>Details</summary>
Motivation: 传统图像先验（如稀疏性、平滑性）无法约束零空间分量，会导致重建结果产生偏差，因此需要将有意义的零空间信息纳入重建框架

Method: 基于图平滑表示，构建零限制拉普拉斯算子，使用p个最平滑的谱图模式（最低图频率）设计低维投影矩阵，仅对不可见分量施加结构约束

Result: 在图像去模糊、压缩感知、去马赛克和超分辨率四种场景中，相比基线方法提升达4.3 dB，相比端到端学习模型提升达1 dB PSNR

Conclusion: GSNR方法通过有效利用零空间信息，显著提升了逆问题求解的性能，具有理论保证和实践优势，可集成到多种主流求解器中

Abstract: Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.

</details>


### [6] [Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking](https://arxiv.org/abs/2602.20330)
*Jingcheng Yang,Tianhu Xiong,Shengyi Qian,Klara Nahrstedt,Mingyuan Wu*

Main category: cs.CV

TL;DR: 提出了首个视觉语言模型透明电路追踪框架，通过transcoders、归因图和注意力方法系统分析多模态推理机制，揭示VLM如何分层整合视觉和语义概念，发现特定视觉特征电路可处理数学推理并支持跨模态关联


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型虽然强大但仍是黑盒系统，缺乏透明度，需要系统性的分析框架来理解其多模态推理机制

Method: 使用transcoders、归因图和基于注意力的方法进行电路追踪，通过特征引导和电路修补验证方法

Result: 揭示了VLM分层整合视觉和语义概念的方式，发现特定视觉特征电路能够处理数学推理并支持跨模态关联

Conclusion: 该框架证明了这些电路具有因果性和可控性，为构建更可解释和可靠的视觉语言模型奠定了基础

Abstract: Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.

</details>


### [7] [Large-scale Photorealistic Outdoor 3D Scene Reconstruction from UAV Imagery Using Gaussian Splatting Techniques](https://arxiv.org/abs/2602.20342)
*Christos Maikos,Georgios Angelidis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 基于无人机视频流的高效端到端3D高斯溅射重建系统，实现低延迟实时建模与AR/VR可视化


<details>
  <summary>Details</summary>
Motivation: 无人机在实时空中感知应用中广泛使用，3D高斯溅射技术展现出实时神经渲染潜力，但将其集成到端到端无人机重建系统中仍待探索

Method: 提出结合RTMP流媒体实时采集、传感器融合同步、相机位姿估计和3DGS优化的架构，实现连续模型更新和低延迟部署

Result: 相比NeRF方法，在保持4-7%离线参考质量的同时，显著提升渲染性能并大幅降低端到端延迟

Conclusion: 该系统适用于实时、可扩展的空中平台增强感知应用，验证了3DGS在实时无人机重建中的有效性

Abstract: In this study, we present an end-to-end pipeline capable of converting drone-captured video streams into high-fidelity 3D reconstructions with minimal latency. Unmanned aerial vehicles (UAVs) are extensively used in aerial real-time perception applications. Moreover, recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential for real-time neural rendering. However, their integration into end-to-end UAV-based reconstruction and visualization systems remains underexplored. Our goal is to propose an efficient architecture that combines live video acquisition via RTMP streaming, synchronized sensor fusion, camera pose estimation, and 3DGS optimization, achieving continuous model updates and low-latency deployment within interactive visualization environments that supports immersive augmented and virtual reality (AR/VR) applications. Experimental results demonstrate that the proposed method achieves competitive visual fidelity, while delivering significantly higher rendering performance and substantially reduced end-to-end latency, compared to NeRF-based approaches. Reconstruction quality remains within 4-7\% of high-fidelity offline references, confirming the suitability of the proposed system for real-time, scalable augmented perception from aerial platforms.

</details>


### [8] [BiRQA: Bidirectional Robust Quality Assessment for Images](https://arxiv.org/abs/2602.20351)
*Aleksandr Gushchin,Dmitriy S. Vatolin,Anastasia Antsiferova*

Main category: cs.CV

TL;DR: BiRQA是一种紧凑的全参考图像质量评估模型，采用双向多尺度金字塔处理四个快速互补特征，通过锚定对抗训练提升鲁棒性，在保持高精度的同时实现实时处理速度和强大的抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 当前神经图像质量评估指标存在速度慢且易受对抗性扰动影响的问题，需要开发既快速又鲁棒的评估方法。

Method: 使用双向多尺度金字塔结构处理特征，包含自下而上的注意力模块和自上而下的交叉门控块；引入锚定对抗训练策略，使用干净锚点样本和排序损失来约束攻击下的预测误差。

Result: 在五个公开FR IQA基准测试中优于或匹配先前SOTA，速度提升约3倍；在未见白盒攻击下将SROCC从0.30-0.57提升至0.60-0.84，显著增强鲁棒性。

Conclusion: BiRQA是首个将竞争性精度、实时吞吐量和强对抗鲁棒性相结合的FR IQA模型，为图像质量评估提供了高效可靠的解决方案。

Abstract: Full-Reference image quality assessment (FR IQA) is important for image compression, restoration and generative modeling, yet current neural metrics remain slow and vulnerable to adversarial perturbations. We present BiRQA, a compact FR IQA metric model that processes four fast complementary features within a bidirectional multiscale pyramid. A bottom-up attention module injects fine-scale cues into coarse levels through an uncertainty-aware gate, while a top-down cross-gating block routes semantic context back to high resolution. To enhance robustness, we introduce Anchored Adversarial Training, a theoretically grounded strategy that uses clean "anchor" samples and a ranking loss to bound pointwise prediction error under attacks. On five public FR IQA benchmarks BiRQA outperforms or matches the previous state of the art (SOTA) while running ~3x faster than previous SOTA models. Under unseen white-box attacks it lifts SROCC from 0.30-0.57 to 0.60-0.84 on KADID-10k, demonstrating substantial robustness gains. To our knowledge, BiRQA is the only FR IQA model combining competitive accuracy with real-time throughput and strong adversarial resilience.

</details>


### [9] [3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism](https://arxiv.org/abs/2602.20354)
*Bhavik Chandna,Kelsey R. Allen*

Main category: cs.CV

TL;DR: 3DSPA是一种自动视频真实性评估框架，通过3D时空点自编码器整合轨迹、深度和语义特征，无需参考视频即可评估生成视频的真实性、时间一致性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成评估主要依赖人工标注或有限范围的专用数据集，需要开发自动化评估方法来捕捉语义和连贯的3D结构。

Method: 开发3D时空点自编码器(3DSPA)，整合3D点轨迹、深度线索和DINO语义特征，形成统一表示来建模物体运动和场景内容。

Result: 实验表明3DSPA能可靠识别违反物理定律的视频，对运动伪影更敏感，在多个数据集上与人类对视频质量和真实性的判断更一致。

Conclusion: 将3D语义信息融入基于轨迹的表征为生成视频模型提供了更强的基准测试基础，并能隐式捕捉物理规则违反情况。

Abstract: AI video generation is evolving rapidly. For video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos. However, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope. Here we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video. Our method, 3DSPA, is a 3D spatiotemporal point autoencoder which integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations. The code and pretrained model weights will be available at https://github.com/TheProParadox/3dspa_code.

</details>


### [10] [Aesthetic Camera Viewpoint Suggestion with 3D Aesthetic Field](https://arxiv.org/abs/2602.20363)
*Sheyang Tang,Armin Shafiee Sarvestani,Jialu Xu,Xiaoyu Xu,Zhou Wang*

Main category: cs.CV

TL;DR: 提出3D美学场概念，通过稀疏输入视图实现几何基础的美学推理，使用前馈3D高斯溅射网络从预训练2D美学模型蒸馏知识，结合粗采样和梯度优化高效寻找美学视角。


<details>
  <summary>Details</summary>
Motivation: 现有美学视角建议方法要么基于单视图调整无法理解场景几何，要么依赖密集采集或预建3D环境结合昂贵强化学习搜索，需要更高效的3D感知美学建模方法。

Method: 学习3D美学场：使用前馈3D高斯溅射网络从预训练2D美学模型蒸馏知识到3D空间；两阶段搜索：粗采样结合基于梯度的优化，无需密集采集或RL探索。

Result: 实验表明该方法持续推荐出比现有方法具有更优构图和框架的视角，在美学质量上表现优越。

Conclusion: 建立了3D感知美学建模的新方向，通过稀疏输入实现高效美学视角建议，解决了现有方法的局限性。

Abstract: The aesthetic quality of a scene depends strongly on camera viewpoint. Existing approaches for aesthetic viewpoint suggestion are either single-view adjustments, predicting limited camera adjustments from a single image without understanding scene geometry, or 3D exploration approaches, which rely on dense captures or prebuilt 3D environments coupled with costly reinforcement learning (RL) searches. In this work, we introduce the notion of 3D aesthetic field that enables geometry-grounded aesthetic reasoning in 3D with sparse captures, allowing efficient viewpoint suggestions in contrast to costly RL searches. We opt to learn this 3D aesthetic field using a feedforward 3D Gaussian Splatting network that distills high-level aesthetic knowledge from a pretrained 2D aesthetic model into 3D space, enabling aesthetic prediction for novel viewpoints from only sparse input views. Building on this field, we propose a two-stage search pipeline that combines coarse viewpoint sampling with gradient-based refinement, efficiently identifying aesthetically appealing viewpoints without dense captures or RL exploration. Extensive experiments show that our method consistently suggests viewpoints with superior framing and composition compared to existing approaches, establishing a new direction toward 3D-aware aesthetic modeling.

</details>


### [11] [CLIPoint3D: Language-Grounded Few-Shot Unsupervised 3D Point Cloud Domain Adaptation](https://arxiv.org/abs/2602.20409)
*Mainak Singha,Sarthak Mehrotra,Paolo Casari,Subhasis Chaudhuri,Elisa Ricci,Biplab Banerjee*

Main category: cs.CV

TL;DR: CLIPoint3D是一个基于CLIP的少样本无监督3D点云域自适应框架，通过多视角深度图投影、知识驱动的提示调优和参数高效微调，在合成到真实点云域迁移中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在3D感知中表现出色但对域偏移脆弱，传统3D域自适应方法依赖重型可训练编码器导致效率低下，需要开发既高效又鲁棒的解决方案。

Method: 将3D样本投影为多深度图，利用冻结CLIP主干，通过知识驱动提示调优整合语言先验和几何线索，采用参数高效微调、熵引导视图采样、最优传输对齐损失和不确定性感知原型对齐损失。

Result: 在PointDA-10和GraspNetPC-10基准测试中，相比基于CLIP和传统编码器的基线方法，准确率提升3-16%。

Conclusion: CLIPoint3D成功实现了高效且有效的3D点云域自适应，通过结合语言先验和几何信息，在少样本设置下显著提升了跨域性能。

Abstract: Recent vision-language models (VLMs) such as CLIP demonstrate impressive cross-modal reasoning, extending beyond images to 3D perception. Yet, these models remain fragile under domain shifts, especially when adapting from synthetic to real-world point clouds. Conventional 3D domain adaptation approaches rely on heavy trainable encoders, yielding strong accuracy but at the cost of efficiency. We introduce CLIPoint3D, the first framework for few-shot unsupervised 3D point cloud domain adaptation built upon CLIP. Our approach projects 3D samples into multiple depth maps and exploits the frozen CLIP backbone, refined through a knowledge-driven prompt tuning scheme that integrates high-level language priors with geometric cues from a lightweight 3D encoder. To adapt task-specific features effectively, we apply parameter-efficient fine-tuning to CLIP's encoders and design an entropy-guided view sampling strategy for selecting confident projections. Furthermore, an optimal transport-based alignment loss and an uncertainty-aware prototype alignment loss collaboratively bridge source-target distribution gaps while maintaining class separability. Extensive experiments on PointDA-10 and GraspNetPC-10 benchmarks show that CLIPoint3D achieves consistent 3-16% accuracy gains over both CLIP-based and conventional encoder-based baselines. Codes are available at https://github.com/SarthakM320/CLIPoint3D.

</details>


### [12] [SimLBR: Learning to Detect Fake Images by Learning to Detect Real Images](https://arxiv.org/abs/2602.20412)
*Aayush Dhakal,Subash Khanal,Srikumar Sastry,Jacob Arndt,Philipe Ambrozio Dias,Dalton Lunga,Nathan Jacobs*

Main category: cs.CV

TL;DR: SimLBR：一种基于潜在混合正则化的简单高效假图像检测框架，通过围绕真实图像分布学习紧决策边界，将假图像视为汇类，显著提升跨生成器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展使AI生成图像检测成为关键挑战，现有方法在训练数据上过拟合，在分布偏移的困难测试集上表现灾难性失败。需要更原则性的方法来学习真实图像分布的紧边界。

Method: 提出SimLBR框架，使用潜在混合正则化(LBR)技术，通过围绕真实图像分布学习紧决策边界，将假图像类别作为汇类处理。

Result: 在Chameleon基准测试中显著提升跨生成器泛化能力，准确率提升达+24.85%，召回率提升+69.62%。训练速度比现有方法快数个数量级。

Conclusion: SimLBR提供了一种简单高效的假图像检测解决方案，同时强调需要可靠性导向的评估方法，引入风险调整指标和最坏情况估计来更好评估模型鲁棒性。

Abstract: The rapid advancement of generative models has made the detection of AI-generated images a critical challenge for both research and society. Recent works have shown that most state-of-the-art fake image detection methods overfit to their training data and catastrophically fail when evaluated on curated hard test sets with strong distribution shifts. In this work, we argue that it is more principled to learn a tight decision boundary around the real image distribution and treat the fake category as a sink class. To this end, we propose SimLBR, a simple and efficient framework for fake image detection using Latent Blending Regularization (LBR). Our method significantly improves cross-generator generalization, achieving up to +24.85\% accuracy and +69.62\% recall on the challenging Chameleon benchmark. SimLBR is also highly efficient, training orders of magnitude faster than existing approaches. Furthermore, we emphasize the need for reliability-oriented evaluation in fake image detection, introducing risk-adjusted metrics and worst-case estimates to better assess model robustness. All code and models will be released on HuggingFace and GitHub.

</details>


### [13] [gQIR: Generative Quanta Image Reconstruction](https://arxiv.org/abs/2602.20417)
*Aryan Garg,Sizhuo Ma,Mohit Gupta*

Main category: cs.CV

TL;DR: 该研究提出了一种将大型文本到图像潜在扩散模型适应于量子爆发成像光子受限领域的方法，通过整合潜在空间恢复和爆发级时空推理，在极低光子条件下实现高质量图像重建。


<details>
  <summary>Details</summary>
Motivation: 单光子雪崩二极管(SPAD)传感器在传统相机无法工作的场景中具有潜力，但原始量子帧包含稀疏、噪声的二进制光子探测，需要处理对齐、去噪和去马赛克等挑战，而传统恢复流程或现代生成模型无法处理这种极端噪声统计。

Method: 利用互联网规模扩散模型的结构和语义先验，引入处理伯努利光子统计的机制，通过潜在空间恢复与爆发级时空推理相结合的方法。

Result: 在合成基准和新现实世界数据集(包括首个彩色SPAD爆发数据集和XD视频基准)上评估，该方法在所有设置中都显著提高了感知质量，超越了经典和现代学习基线的性能。

Conclusion: 该研究展示了将大型生成先验适应于极端光子受限传感的潜力，能够产生既保持光度保真度又具有良好感知质量的重建结果，即使在高速运动条件下也能表现优异。

Abstract: Capturing high-quality images from only a few detected photons is a fundamental challenge in computational imaging. Single-photon avalanche diode (SPAD) sensors promise high-quality imaging in regimes where conventional cameras fail, but raw \emph{quanta frames} contain only sparse, noisy, binary photon detections. Recovering a coherent image from a burst of such frames requires handling alignment, denoising, and demosaicing (for color) under noise statistics far outside those assumed by standard restoration pipelines or modern generative models. We present an approach that adapts large text-to-image latent diffusion models to the photon-limited domain of quanta burst imaging. Our method leverages the structural and semantic priors of internet-scale diffusion models while introducing mechanisms to handle Bernoulli photon statistics. By integrating latent-space restoration with burst-level spatio-temporal reasoning, our approach produces reconstructions that are both photometrically faithful and perceptually pleasing, even under high-speed motion. We evaluate the method on synthetic benchmarks and new real-world datasets, including the first color SPAD burst dataset and a challenging \textit{Deforming (XD)} video benchmark. Across all settings, the approach substantially improves perceptual quality over classical and modern learning-based baselines, demonstrating the promise of adapting large generative priors to extreme photon-limited sensing. Code at \href{https://github.com/Aryan-Garg/gQIR}{https://github.com/Aryan-Garg/gQIR}.

</details>


### [14] [MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation](https://arxiv.org/abs/2602.20423)
*Taha Koleilat,Hojat Asgariandehkordi,Omid Nejati Manzari,Berardino Barile,Yiming Xiao,Hassan Rivaz*

Main category: cs.CV

TL;DR: MedCLIPSeg：基于CLIP的概率视觉语言模型，用于数据高效、不确定性感知的医学图像分割，通过补丁级跨模态注意力和对比学习提升准确性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临标注数据有限、解剖特征模糊和域偏移等挑战，现有视觉语言模型在密集文本引导分割方面潜力尚未充分挖掘

Method: 提出MedCLIPSeg框架，利用补丁级CLIP嵌入，通过概率跨模态注意力实现图像与文本标记的双向交互和预测不确定性建模，结合软补丁级对比损失优化语义学习

Result: 在5种成像模态、6个器官的16个数据集上验证，MedCLIPSeg在准确性、效率和鲁棒性方面优于现有方法，并提供可解释的不确定性图谱

Conclusion: 该工作展示了概率视觉语言模型在文本驱动医学图像分割中的潜力，为数据高效和不确定性感知的分割提供了有效解决方案

Abstract: Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, a novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with a soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation.

</details>


### [15] [SceMoS: Scene-Aware 3D Human Motion Synthesis by Planning with Geometry-Grounded Tokens](https://arxiv.org/abs/2602.20476)
*Anindita Ghosh,Vladislav Golyanik,Taku Komura,Philipp Slusallek,Christian Theobalt,Rishabh Dabral*

Main category: cs.CV

TL;DR: SceMoS是一个基于2D场景表示的3D人体运动合成框架，通过分离全局规划和局部执行，使用BEV图像进行语义规划和高度图进行物理约束，在保持运动真实性的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要昂贵的3D场景数据（如点云、体素网格）同时学习高层规划和底层接触推理，计算成本高。研究旨在探索结构化2D场景表示能否替代完整3D监督，实现物理基础的运动合成。

Method: 1) 文本条件自回归全局运动规划器：使用DINOv2特征编码的鸟瞰图(BEV)作为场景表示；2) 几何基础运动分词器：通过条件VQ-VAE训练，使用2D局部场景高度图将表面物理嵌入离散词汇表

Result: 在TRUMANS基准测试中达到最先进的运动真实性和接触准确性，场景编码可训练参数减少50%以上

Conclusion: 2D场景线索（BEV语义和局部高度图）能够有效支撑3D人-场景交互，实现效率与保真度的平衡，证明2D结构化表示是完整3D监督的强大替代方案

Abstract: Synthesizing text-driven 3D human motion within realistic scenes requires learning both semantic intent ("walk to the couch") and physical feasibility (e.g., avoiding collisions). Current methods use generative frameworks that simultaneously learn high-level planning and low-level contact reasoning, and rely on computationally expensive 3D scene data such as point clouds or voxel occupancy grids. We propose SceMoS, a scene-aware motion synthesis framework that shows that structured 2D scene representations can serve as a powerful alternative to full 3D supervision in physically grounded motion synthesis. SceMoS disentangles global planning from local execution using lightweight 2D cues and relying on (1) a text-conditioned autoregressive global motion planner that operates on a bird's-eye-view (BEV) image rendered from an elevated corner of the scene, encoded with DINOv2 features, as the scene representation, and (2) a geometry-grounded motion tokenizer trained via a conditional VQ-VAE, that uses 2D local scene heightmap, thus embedding surface physics directly into a discrete vocabulary. This 2D factorization reaches an efficiency-fidelity trade-off: BEV semantics capture spatial layout and affordance for global reasoning, while local heightmaps enforce fine-grained physical adherence without full 3D volumetric reasoning. SceMoS achieves state-of-the-art motion realism and contact accuracy on the TRUMANS benchmark, reducing the number of trainable parameters for scene encoding by over 50%, showing that 2D scene cues can effectively ground 3D human-scene interaction.

</details>


### [16] [Path-Decoupled Hyperbolic Flow Matching for Few-Shot Adaptation](https://arxiv.org/abs/2602.20479)
*Lin Li,Ziqi Jiang,Gefan Ye,Zhenqi He,Jiahui Li,Jun Xiao,Kwang-Ting Cheng,Long Chen*

Main category: cs.CV

TL;DR: HFM提出路径解耦的双曲流匹配方法，通过洛伦兹流形的指数扩展特性解决欧几里得流匹配中的路径纠缠问题，在11个基准测试中达到新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 欧几里得流匹配在处理跨模态少样本适应时存在局限性，平坦几何的多项式体积增长无法适应多样化的特征分布，导致严重的路径纠缠问题。

Method: 1) 向心双曲对齐：通过锚定文本根构建向心层次结构，将视觉叶子推向边界以初始化有序流；2) 路径解耦目标：作为"语义护栏"，通过逐步监督将轨迹严格限制在孤立的类特定测地走廊内；3) 基于直径的自适应停止机制：根据内在语义尺度防止过度传输到拥挤的源点。

Result: 在11个基准测试上的广泛消融实验表明，HFM建立了新的最先进性能，始终优于其欧几里得对应方法。

Conclusion: HFM通过利用双曲几何的指数扩展特性有效解决了特征传输中的路径纠缠问题，为跨模态少样本适应提供了更优的解决方案。

Abstract: Recent advances in cross-modal few-shot adaptation treat visual-semantic alignment as a continuous feature transport problem via Flow Matching (FM). However, we argue that Euclidean-based FM overlooks fundamental limitations of flat geometry, where polynomial volume growth fails to accommodate diverse feature distributions, leading to severe path entanglement. To this end, we propose path-decoupled Hyperbolic Flow Matching (HFM), leveraging the Lorentz manifold's exponential expansion for trajectory decoupling. HFM structures the transport via two key designs: 1) Centripetal hyperbolic alignment: It constructs a centripetal hierarchy by anchoring textual roots, which pushes visual leaves to the boundary to initialize orderly flows. 2) Path-decoupled objective: It acts as a ``semantic guardrail'' rigidly confining trajectories within isolated class-specific geodesic corridors via step-wise supervision. Furthermore, we devise an adaptive diameter-based stopping to prevent over-transportation into the crowded origin based on the intrinsic semantic scale. Extensive ablations on 11 benchmarks have shown that HFM establishes a new state-of-the-art, consistently outperforming its Euclidean counterparts. Our codes and models will be released.

</details>


### [17] [Pip-Stereo: Progressive Iterations Pruner for Iterative Optimization based Stereo Matching](https://arxiv.org/abs/2602.20496)
*Jintu Zheng,Qizhe Liu,HuangXin Xu,Zhuojie Chen*

Main category: cs.CV

TL;DR: PipStereo是一个创新的实时立体匹配系统，通过渐进式迭代剪枝、单目先验转移框架和硬件优化的FlashGRU算子，在边缘设备上实现了高精度实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有迭代式立体匹配方法依赖RNN网络，计算复杂度高，难以在边缘设备上部署，且存在空间稀疏性和时间冗余性问题。

Method: 1. 渐进式迭代剪枝策略抑制冗余更新步骤；2. 协作式单目先验转移框架隐式嵌入深度先验；3. 硬件感知的FlashGRU算子利用结构化稀疏性和I/O优化设计。

Result: 在NVIDIA Jetson Orin NX上处理320×640帧仅需75ms，RTX 4090上仅需19ms，速度提升7.28倍，内存峰值减少76.6%，全局内存请求减少80.9%，精度与大型迭代模型相当。

Conclusion: 该方法成功解决了迭代式立体匹配在边缘设备部署的挑战，实现了实时高精度性能，泛化能力和准确性远超现有实时方法。

Abstract: While iterative stereo matching achieves high accuracy, its dependence on Recurrent Neural Networks (RNN) hinders edge deployment, a challenge underexplored in existing researches. We analyze iterative refinement and reveal that disparity updates are spatially sparse and temporally redundant. First, we introduce a progressive iteration pruning strategy that suppresses redundant update steps, effectively collapsing the recursive computation into a near-single-pass inference. Second, we propose a collaborative monocular prior transfer framework that implicitly embeds depth priors without requiring a dedicated monocular encoder, thereby eliminating its associated computational burden. Third, we develop FlashGRU, a hardware-aware RNN operator leveraging structured sparsity and I/O-conscious design, achieving a 7.28$\times$ speedup, 76.6\% memory peak reduction and 80.9\% global memory requests reduction over natvie ConvGRUs under 2K resolution. Our PipStereo enables real-time, high-fidelity stereo matching on edge hardware: it processes 320$\times$640 frames in just 75ms on an NVIDIA Jetson Orin NX (FP16) and 19ms on RTX 4090, matching the accuracy of large iterative based models, and our generalization ability and accuracy far exceeds that of existing real-time methods. Our embedded AI projects will be updated at: https://github.com/XPENG-Aridge-AI.

</details>


### [18] [LESA: Learnable Stage-Aware Predictors for Diffusion Model Acceleration](https://arxiv.org/abs/2602.20497)
*Peiliang Cai,Jiacheng Liu,Haowen Xu,Xinyu Wang,Chang Zou,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出可学习的阶段感知（LESA）预测器框架，通过两阶段训练和KAN网络学习扩散过程的时序特征映射，实现5-6倍加速同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散变换器（DiTs）计算需求高，基于简单重用或无训练预测的特征缓存方法难以适应扩散过程的复杂阶段依赖性动态，导致质量下降和去噪过程不一致。

Method: 采用两阶段训练框架，使用Kolmogorov-Arnold网络（KAN）学习时序特征映射，引入多阶段多专家架构为不同噪声级别阶段分配专门预测器。

Result: 在FLUX.1-dev上实现5.00倍加速（质量仅下降1.0%），在Qwen-Image上实现6.25倍加速（质量提升20.2%），在HunyuanVideo上实现5.00倍加速（PSNR提升24.7%）。

Conclusion: 训练型框架在文本到图像和文本到视频合成任务上均达到最先进性能，验证了方法的有效性和跨模型泛化能力。

Abstract: Diffusion models have achieved remarkable success in image and video generation tasks. However, the high computational demands of Diffusion Transformers (DiTs) pose a significant challenge to their practical deployment. While feature caching is a promising acceleration strategy, existing methods based on simple reusing or training-free forecasting struggle to adapt to the complex, stage-dependent dynamics of the diffusion process, often resulting in quality degradation and failing to maintain consistency with the standard denoising process. To address this, we propose a LEarnable Stage-Aware (LESA) predictor framework based on two-stage training. Our approach leverages a Kolmogorov-Arnold Network (KAN) to accurately learn temporal feature mappings from data. We further introduce a multi-stage, multi-expert architecture that assigns specialized predictors to different noise-level stages, enabling more precise and robust feature forecasting. Extensive experiments show our method achieves significant acceleration while maintaining high-fidelity generation. Experiments demonstrate 5.00x acceleration on FLUX.1-dev with minimal quality degradation (1.0% drop), 6.25x speedup on Qwen-Image with a 20.2% quality improvement over the previous SOTA (TaylorSeer), and 5.00x acceleration on HunyuanVideo with a 24.7% PSNR improvement over TaylorSeer. State-of-the-art performance on both text-to-image and text-to-video synthesis validates the effectiveness and generalization capability of our training-based framework across different models. Our code is included in the supplementary materials and will be released on GitHub.

</details>


### [19] [Probing and Bridging Geometry-Interaction Cues for Affordance Reasoning in Vision Foundation Models](https://arxiv.org/abs/2602.20501)
*Qing Zhang,Xuesong Li,Jing Zhang*

Main category: cs.CV

TL;DR: 该研究提出视觉系统理解功能可供性需要几何感知和交互感知两个互补能力，通过系统分析视觉基础模型发现DINO编码几何结构而Flux包含交互先验，两者融合可实现零样本竞争性功能可供性估计。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉系统真正理解功能可供性的本质含义，认为这需要几何感知（识别物体结构部分）和交互感知（建模动作与结构交互）两种能力的结合。

Method: 系统探测视觉基础模型（VFMs），分析DINO模型的几何结构编码能力和Flux生成模型的动词条件空间注意力映射，然后将两者的几何原型与交互映射进行无训练零样本融合。

Result: 发现DINO编码部件级几何结构，Flux包含丰富的动词条件空间注意力映射作为隐式交互先验，两者融合后的功能可供性估计性能可与弱监督方法竞争。

Conclusion: 几何感知和交互感知是视觉基础模型中功能可供性理解的基本构建模块，两者不仅是相关而且是可组合的，为感知如何支撑行动提供了机制性解释。

Abstract: What does it mean for a visual system to truly understand affordance? We argue that this understanding hinges on two complementary capacities: geometric perception, which identifies the structural parts of objects that enable interaction, and interaction perception, which models how an agent's actions engage with those parts. To test this hypothesis, we conduct a systematic probing of Visual Foundation Models (VFMs). We find that models like DINO inherently encode part-level geometric structures, while generative models like Flux contain rich, verb-conditioned spatial attention maps that serve as implicit interaction priors. Crucially, we demonstrate that these two dimensions are not merely correlated but are composable elements of affordance. By simply fusing DINO's geometric prototypes with Flux's interaction maps in a training-free and zero-shot manner, we achieve affordance estimation competitive with weakly-supervised methods. This final fusion experiment confirms that geometric and interaction perception are the fundamental building blocks of affordance understanding in VFMs, providing a mechanistic account of how perception grounds action.

</details>


### [20] [Leveraging Causal Reasoning Method for Explaining Medical Image Segmentation Models](https://arxiv.org/abs/2602.20511)
*Limai Jiang,Ruitao Xie,Bokai Yang,Huazhen Huang,Juan He,Yufu Huo,Zikai Wang,Yang Wei,Yunpeng Cai*

Main category: cs.CV

TL;DR: 提出基于因果推理框架的医学图像分割可解释性方法，通过平均处理效应量化输入区域和网络组件对分割结果的影响，相比现有方法提供更忠实解释，并揭示不同模型感知策略的异质性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在临床决策中至关重要，但深度学习模型的黑盒特性影响其在高风险医疗场景中的可信度。现有解释技术主要关注分类任务，分割领域的可解释性研究相对不足。

Method: 采用因果推理框架，通过反向传播平均处理效应(ATE)构建量化指标，评估输入区域和网络组件对目标分割区域的影响程度。

Result: 在两个代表性医学影像数据集上与现有分割可解释性技术对比，证明该方法提供更忠实的解释；系统性因果分析揭示了不同模型间以及同一模型不同输入间感知策略的显著异质性。

Conclusion: 该方法不仅能提供比现有方法更准确的分割解释，还能为优化分割模型提供重要洞见，具有促进医学图像分割模型可信度提升的潜力。

Abstract: Medical image segmentation plays a vital role in clinical decision-making, enabling precise localization of lesions and guiding interventions. Despite significant advances in segmentation accuracy, the black-box nature of most deep models has raised growing concerns about their trustworthiness in high-stakes medical scenarios. Current explanation techniques have primarily focused on classification tasks, leaving the segmentation domain relatively underexplored. We introduced an explanation model for segmentation task which employs the causal inference framework and backpropagates the average treatment effect (ATE) into a quantification metric to determine the influence of input regions, as well as network components, on target segmentation areas. Through comparison with recent segmentation explainability techniques on two representative medical imaging datasets, we demonstrated that our approach provides more faithful explanations than existing approaches. Furthermore, we carried out a systematic causal analysis of multiple foundational segmentation models using our method, which reveals significant heterogeneity in perceptual strategies across different models, and even between different inputs for the same model. Suggesting the potential of our method to provide notable insights for optimizing segmentation models. Our code can be found at https://github.com/lcmmai/PdCR.

</details>


### [21] [How Do Inpainting Artifacts Propagate to Language?](https://arxiv.org/abs/2602.20520)
*Pratham Yashwante,Davit Abrahamyan,Shresth Grover,Sukruth Rao*

Main category: cs.CV

TL;DR: 研究扩散修复方法引入的视觉伪影如何影响视觉语言模型的语言生成，通过两阶段诊断框架分析修复保真度与图像描述质量的关系


<details>
  <summary>Details</summary>
Motivation: 扩散修复技术在图像处理中会产生视觉伪影，但这些伪影对下游视觉语言任务（如图像描述）的影响尚未被系统研究

Method: 采用两阶段诊断设置：首先对掩码图像区域进行重建修复，然后将修复后的图像输入到图像描述模型中，通过对比原始图像和修复图像的描述结果进行控制比较

Result: 发现像素级和感知层面的重建质量指标与词汇和语义层面的描述性能存在一致关联；中间视觉表示和注意力模式分析显示修复伪影导致模型行为的系统性、层级依赖性变化

Conclusion: 研究提供了一个实用的诊断框架，可用于检验视觉重建质量如何影响多模态系统中的语言生成过程

Abstract: We study how visual artifacts introduced by diffusion-based inpainting affect language generation in vision-language models. We use a two-stage diagnostic setup in which masked image regions are reconstructed and then provided to captioning models, enabling controlled comparisons between captions generated from original and reconstructed inputs. Across multiple datasets, we analyze the relationship between reconstruction fidelity and downstream caption quality. We observe consistent associations between pixel-level and perceptual reconstruction metrics and both lexical and semantic captioning performance. Additional analysis of intermediate visual representations and attention patterns shows that inpainting artifacts lead to systematic, layer-dependent changes in model behavior. Together, these results provide a practical diagnostic framework for examining how visual reconstruction quality influences language generation in multimodal systems.

</details>


### [22] [A Lightweight Vision-Language Fusion Framework for Predicting App Ratings from User Interfaces and Metadata](https://arxiv.org/abs/2602.20531)
*Azrin Sultana,Firoz Ahmed*

Main category: cs.CV

TL;DR: 提出一种轻量级视觉-语言框架，融合移动UI布局特征和语义信息进行应用评分预测，结合MobileNetV3和DistilBERT提取多模态特征，通过门控融合模块和MLP回归头实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 现有应用评分预测模型主要局限于文本数据或UI特征，忽视了联合利用UI和语义信息的重要性，需要开发能同时处理视觉和文本信息的多模态框架。

Method: 使用MobileNetV3提取UI布局的视觉特征，DistilBERT提取文本特征；通过带Swish激活函数的门控融合模块融合多模态特征；采用MLP回归头进行评分预测；使用MAE、RMSE、MSE、R2和Pearson相关系数进行评估。

Result: 训练20个epoch后，模型达到MAE 0.1060、RMSE 0.1433、MSE 0.0205、R2 0.8529、Pearson相关系数0.9251的优秀性能。消融研究验证了不同视觉和文本编码器组合的有效性。

Conclusion: 该轻量级框架为开发者和最终用户提供了有价值的见解，支持可持续的应用开发，并能在边缘设备上高效部署，有效解决了现有模型的局限性。

Abstract: App ratings are among the most significant indicators of the quality, usability, and overall user satisfaction of mobile applications. However, existing app rating prediction models are largely limited to textual data or user interface (UI) features, overlooking the importance of jointly leveraging UI and semantic information. To address these limitations, this study proposes a lightweight vision--language framework that integrates both mobile UI and semantic information for app rating prediction. The framework combines MobileNetV3 to extract visual features from UI layouts and DistilBERT to extract textual features. These multimodal features are fused through a gated fusion module with Swish activations, followed by a multilayer perceptron (MLP) regression head. The proposed model is evaluated using mean absolute error (MAE), root mean square error (RMSE), mean squared error (MSE), coefficient of determination (R2), and Pearson correlation. After training for 20 epochs, the model achieves an MAE of 0.1060, an RMSE of 0.1433, an MSE of 0.0205, an R2 of 0.8529, and a Pearson correlation of 0.9251. Extensive ablation studies further demonstrate the effectiveness of different combinations of visual and textual encoders. Overall, the proposed lightweight framework provides valuable insights for developers and end users, supports sustainable app development, and enables efficient deployment on edge devices.

</details>


### [23] [PFGNet: A Fully Convolutional Frequency-Guided Peripheral Gating Network for Efficient Spatiotemporal Predictive Learning](https://arxiv.org/abs/2602.20537)
*Xinyong Cai,Changbin Sun,Yong Wang,Hongyu Yang,Yuankai Wu*

Main category: cs.CV

TL;DR: PFGNet是一个完全卷积的时空预测框架，通过像素级频率引导门控动态调制感受野，实现高效的自适应时空建模，在多个基准测试中达到SOTA性能且计算成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 纯卷积模型在时空预测中具有高效和完全并行的优势，但其固定感受野限制了自适应捕捉空间变化运动模式的能力。受生物中心-周围组织和频率选择信号处理的启发，需要开发能够动态调整感受野的高效模型。

Method: 提出PFGNet框架，核心是外围频率门控(PFG)块：提取局部频谱线索，自适应融合多尺度大核外围响应与可学习中心抑制，形成空间自适应带通滤波器。使用可分离1D卷积(1×k后接k×1)将每通道计算成本从O(k²)降至O(2k)。

Result: 在Moving MNIST、TaxiBJ、Human3.6M和KTH数据集上的实验表明，PFGNet以显著更少的参数和FLOPs实现了SOTA或接近SOTA的预测性能。

Conclusion: PFGNet通过频率引导的动态感受野调制，在不使用循环或注意力机制的情况下实现了结构感知的时空建模，为高效时空预测提供了新的解决方案。

Abstract: Spatiotemporal predictive learning (STPL) aims to forecast future frames from past observations and is essential across a wide range of applications. Compared with recurrent or hybrid architectures, pure convolutional models offer superior efficiency and full parallelism, yet their fixed receptive fields limit their ability to adaptively capture spatially varying motion patterns. Inspired by biological center-surround organization and frequency-selective signal processing, we propose PFGNet, a fully convolutional framework that dynamically modulates receptive fields through pixel-wise frequency-guided gating. The core Peripheral Frequency Gating (PFG) block extracts localized spectral cues and adaptively fuses multi-scale large-kernel peripheral responses with learnable center suppression, effectively forming spatially adaptive band-pass filters. To maintain efficiency, all large kernels are decomposed into separable 1D convolutions ($1 \times k$ followed by $k \times 1$), reducing per-channel computational cost from $O(k^2)$ to $O(2k)$. PFGNet enables structure-aware spatiotemporal modeling without recurrence or attention. Experiments on Moving MNIST, TaxiBJ, Human3.6M, and KTH show that PFGNet delivers SOTA or near-SOTA forecasting performance with substantially fewer parameters and FLOPs. Our code is available at https://github.com/fhjdqaq/PFGNet.

</details>


### [24] [Beyond Human Performance: A Vision-Language Multi-Agent Approach for Quality Control in Pharmaceutical Manufacturing](https://arxiv.org/abs/2602.20543)
*Subhra Jyoti Mandal,Lara Rachidi,Puneet Jain,Matthieu Duvinage,Sander W. Timmer*

Main category: cs.CV

TL;DR: 基于深度学习和视觉语言模型的多智能体框架，用于制药行业菌落形成单位检测，实现99%检测率并减少85%人工验证工作量


<details>
  <summary>Details</summary>
Motivation: 传统人工菌落计数劳动密集且易出错，现有深度学习方案虽准确但受样本质量变异和伪影影响，无法满足制药级质量要求

Method: 结合Detectron2深度学习模型和视觉语言模型(VLM)：VLM先分类平板有效性；有效样本由DL和VLM分别计数；预测一致时自动记录，不一致时专家审核；通过反馈实现持续重训练

Result: 检测率达到99%，假阳性2%，假阴性0.6%；DL自动化减少50%人工验证，VLM集成后提升至85%减少；提供可扩展、可审计的法规合规解决方案

Conclusion: 该多智能体框架成功解决了制药微生物质量控制中的自动化挑战，显著提升操作效率并满足严格监管要求，推动了生物制药生产的自动化进程

Abstract: Colony-forming unit (CFU) detection is critical in pharmaceutical manufacturing, serving as a key component of Environmental Monitoring programs and ensuring compliance with stringent quality standards. Manual counting is labor-intensive and error-prone, while deep learning (DL) approaches, though accurate, remain vulnerable to sample quality variations and artifacts. Building on our earlier CNN-based framework (Beznik et al., 2020), we evaluated YOLOv5, YOLOv7, and YOLOv8 for CFU detection; however, these achieved only 97.08 percent accuracy, insufficient for pharmaceutical-grade requirements. A custom Detectron2 model trained on GSK's dataset of over 50,000 Petri dish images achieved 99 percent detection rate with 2 percent false positives and 0.6 percent false negatives. Despite high validation accuracy, Detectron2 performance degrades on outlier cases including contaminated plates, plastic artifacts, or poor optical clarity. To address this, we developed a multi-agent framework combining DL with vision-language models (VLMs). The VLM agent first classifies plates as valid or invalid. For valid samples, both DL and VLM agents independently estimate colony counts. When predictions align within 5 percent, results are automatically recorded in Postgres and SAP; otherwise, samples are routed for expert review. Expert feedback enables continuous retraining and self-improvement. Initial DL-based automation reduced human verification by 50 percent across vaccine manufacturing sites. With VLM integration, this increased to 85 percent, delivering significant operational savings. The proposed system provides a scalable, auditable, and regulation-ready solution for microbiological quality control, advancing automation in biopharmaceutical production.

</details>


### [25] [Robust Spiking Neural Networks Against Adversarial Attacks](https://arxiv.org/abs/2602.20548)
*Shuai Wang,Malu Zhang,Yulin Jiang,Dehao Zhang,Ammar Belatreche,Yu Liang,Yimeng Shan,Zijian Zhou,Yang Yang,Haizhou Li*

Main category: cs.CV

TL;DR: 提出阈值守卫优化(TGO)方法增强脉冲神经网络(SNN)的鲁棒性，通过使神经元膜电位远离阈值和引入噪声神经元来降低对抗攻击的理论上限和状态翻转概率。


<details>
  <summary>Details</summary>
Motivation: SNN在复杂对抗环境中的鲁棒性受限，研究发现阈值邻近的脉冲神经元是限制直接训练SNN鲁棒性的关键因素，这些神经元设定了对抗攻击的最大强度上限且容易在微小扰动下发生状态翻转。

Method: 提出阈值守卫优化(TGO)方法：1)在损失函数中加入约束使神经元膜电位远离阈值，增加梯度稀疏性；2)引入噪声脉冲神经元将神经元发放机制从确定性转为概率性，降低状态翻转概率。

Result: 在标准对抗场景下的广泛实验证明，该方法显著增强了直接训练SNN的鲁棒性。

Conclusion: 该研究为推进现实应用中更可靠和安全的神经形态计算铺平了道路。

Abstract: Spiking Neural Networks (SNNs) represent a promising paradigm for energy-efficient neuromorphic computing due to their bio-plausible and spike-driven characteristics. However, the robustness of SNNs in complex adversarial environments remains significantly constrained. In this study, we theoretically demonstrate that those threshold-neighboring spiking neurons are the key factors limiting the robustness of directly trained SNNs. We find that these neurons set the upper limits for the maximum potential strength of adversarial attacks and are prone to state-flipping under minor disturbances. To address this challenge, we propose a Threshold Guarding Optimization (TGO) method, which comprises two key aspects. First, we incorporate additional constraints into the loss function to move neurons' membrane potentials away from their thresholds. It increases SNNs' gradient sparsity, thereby reducing the theoretical upper bound of adversarial attacks. Second, we introduce noisy spiking neurons to transition the neuronal firing mechanism from deterministic to probabilistic, decreasing their state-flipping probability due to minor disturbances. Extensive experiments conducted in standard adversarial scenarios prove that our method significantly enhances the robustness of directly trained SNNs. These findings pave the way for advancing more reliable and secure neuromorphic computing in real-world applications.

</details>


### [26] [The Finite Primitive Basis Theorem for Computational Imaging: Formal Foundations of the OperatorGraph Representation](https://arxiv.org/abs/2602.20550)
*Chengshuai Yang*

Main category: cs.CV

TL;DR: 本文证明了所有计算成像前向模型都可以用11个基本原语的DAG图进行ε近似表示，该基是最小的，无法进一步缩减。通过构造性算法为31种线性模态和9种非线性模态提供了具体实现，误差低于0.01。


<details>
  <summary>Details</summary>
Motivation: 传统计算成像前向模型采用单一、特定模态的实现方式，缺乏统一的数学框架和标准化表示，限制了跨模态方法的通用性和可复用性。

Method: 提出了有限基元定理，定义了包含11个标准原语的算子类Cimg。通过构造性算法将任何前向模型H∈Cimg转换为DAG图表示，并证明该基元库的最小性。将非线性问题分为点式标量函数和自洽迭代两类进行处理。

Result: 在31种线性模态上实现ε_img<0.01，最多使用5个节点和深度5的DAG。为9种非线性模态提供了构造性DAG分解。验证了基元库的最小性。

Conclusion: 建立了物理世界模型(PWM)框架的数学基础，为计算成像提供了统一的标准化表示方法，实现了跨不同成像模态的通用前向模型描述。

Abstract: Computational imaging forward models, from coded aperture spectral cameras to MRI scanners, are traditionally implemented as monolithic, modality-specific codes. We prove that every forward model in a broad, precisely defined operator class Cimg (encompassing clinical, scientific, and industrial imaging modalities, both linear and nonlinear) admits an epsilon-approximate representation as a typed directed acyclic graph (DAG) whose nodes are drawn from a library of exactly 11 canonical primitives: Propagate, Modulate, Project, Encode, Convolve, Accumulate, Detect, Sample, Disperse, Scatter, and Transform. We call this the Finite Primitive Basis Theorem. The proof is constructive: we provide an algorithm that, given any H in Cimg, produces a DAG G with relative operator error at most epsilon and graph complexity within prescribed bounds. We further prove that the library is minimal: removing any single primitive causes at least one modality to lose its epsilon-approximate representation. A systematic analysis of nonlinearities in imaging physics shows they fall into two structural categories: pointwise scalar functions (handled by Transform) and self-consistent iterations (unrolled into existing linear primitives). Empirical validation on 31 linear modalities confirms eimg below 0.01 with at most 5 nodes and depth 5, and we provide constructive DAG decompositions for 9 additional nonlinear modalities. These results establish mathematical foundations for the Physics World Model (PWM) framework.

</details>


### [27] [CAD-Prompted SAM3: Geometry-Conditioned Instance Segmentation for Industrial Objects](https://arxiv.org/abs/2602.20551)
*Zhenran Tang,Rohan Nagabhirava,Changliu Liu*

Main category: cs.CV

TL;DR: 提出基于CAD模型多视角渲染的几何提示分割框架SAM3，解决语言和外观提示在工业场景中对复杂几何物体分割的局限性


<details>
  <summary>Details</summary>
Motivation: 语言提示受自然语言表达能力限制，难以描述不常见或特定实例的物体；外观提示在工业环境中不可靠，因为同一部件可能有不同材质、表面处理或颜色；而CAD模型能精确捕捉物体的规范几何特征

Method: 基于SAM3构建CAD提示分割框架，使用CAD模型的多视角渲染作为提示输入，通过合成数据训练模型，渲染在不同视角和场景上下文下的网格

Result: 实现了单阶段的CAD提示掩码预测，扩展了可提示分割到无法通过语言或外观单独可靠描述的物体

Conclusion: CAD几何提示为工业制造和3D打印环境中的物体分割提供了可靠解决方案，克服了语言和外观提示的局限性

Abstract: Verbal-prompted segmentation is inherently limited by the expressiveness of natural language and struggles with uncommon, instance-specific, or difficult-to-describe objects: scenarios frequently encountered in manufacturing and 3D printing environments. While image exemplars provide an alternative, they primarily encode appearance cues such as color and texture, which are often unrelated to a part's geometric identity. In industrial settings, a single component may be produced in different materials, finishes, or colors, making appearance-based prompting unreliable. In contrast, such objects are typically defined by precise CAD models that capture their canonical geometry. We propose a CAD-prompted segmentation framework built on SAM3 that uses canonical multi-view renderings of a CAD model as prompt input. The rendered views provide geometry-based conditioning independent of surface appearance. The model is trained using synthetic data generated from mesh renderings in simulation under diverse viewpoints and scene contexts. Our approach enables single-stage, CAD-prompted mask prediction, extending promptable segmentation to objects that cannot be robustly described by language or appearance alone.

</details>


### [28] [WildGHand: Learning Anti-Perturbation Gaussian Hand Avatars from Monocular In-the-Wild Videos](https://arxiv.org/abs/2602.20556)
*Hanhui Li,Xuan Huang,Wanquan Liu,Yuhao Cheng,Long Chen,Yiqiang Yan,Xiaodan Liang,Chenqiang Gao*

Main category: cs.CV

TL;DR: WildGHand是一个基于优化的框架，用于从单目视频中重建高保真手部化身，专门针对野外环境中的严重扰动（如手-物体交互、极端姿态、光照变化和运动模糊）进行了优化。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部重建方法在受控环境中表现良好，但在具有严重扰动的真实世界环境中性能下降，需要一种能够处理这些扰动的鲁棒方法。

Method: 框架包含两个关键组件：(1)动态扰动解耦模块，将扰动表示为3D高斯属性上的时变偏差；(2)扰动感知优化策略，生成每帧各向异性加权掩码来指导优化。这些组件共同在空间和时间维度上识别和抑制扰动。

Result: 在多个数据集上的广泛实验表明，WildGHand实现了最先进的性能，在多个指标上相比基线模型有显著提升（PSNR相对增益达15.8%，LPIPS相对减少23.1%）。

Conclusion: WildGHand通过创新的扰动处理机制，成功解决了野外环境下3D手部重建的挑战，为真实世界应用提供了高质量的解决方案，并提供了相应的数据集供研究使用。

Abstract: Despite recent progress in 3D hand reconstruction from monocular videos, most existing methods rely on data captured in well-controlled environments and therefore degrade in real-world settings with severe perturbations, such as hand-object interactions, extreme poses, illumination changes, and motion blur. To tackle these issues, we introduce WildGHand, an optimization-based framework that enables self-adaptive 3D Gaussian splatting on in-the-wild videos and produces high-fidelity hand avatars. WildGHand incorporates two key components: (i) a dynamic perturbation disentanglement module that explicitly represents perturbations as time-varying biases on 3D Gaussian attributes during optimization, and (ii) a perturbation-aware optimization strategy that generates per-frame anisotropic weighted masks to guide optimization. Together, these components allow the framework to identify and suppress perturbations across both spatial and temporal dimensions. We further curate a dataset of monocular hand videos captured under diverse perturbations to benchmark in-the-wild hand avatar reconstruction. Extensive experiments on this dataset and two public datasets demonstrate that WildGHand achieves state-of-the-art performance and substantially improves over its base model across multiple metrics (e.g., up to a $15.8\%$ relative gain in PSNR and a $23.1\%$ relative reduction in LPIPS). Our implementation and dataset are available at https://github.com/XuanHuang0/WildGHand.

</details>


### [29] [AIForge-Doc: A Benchmark for Detecting AI-Forged Tampering in Financial and Form Documents](https://arxiv.org/abs/2602.20569)
*Jiaqi Wu,Yuchen Zhou,Muduo Xu,Zisheng Liang,Simiao Ren,Jiayu Xue,Meige Yang,Siying Chen,Jingheng Huan*

Main category: cs.CV

TL;DR: AIForge-Doc是首个专门针对金融和表单文档中基于扩散模型的修复伪造的基准测试集，包含4061张AI伪造图像，测试发现现有检测器性能大幅下降，表明AI伪造文档对现有取证技术构成新挑战。


<details>
  <summary>Details</summary>
Motivation: 现有文档伪造数据集依赖传统数字编辑工具，无法应对AI伪造文档的快速增长威胁，需要专门针对AI修复伪造的基准测试集。

Method: 使用Gemini 2.5 Flash Image和Ideogram v2 Edit两种AI修复API，在四个公共文档数据集（CORD、WildReceipt、SROIE、XFUND）中系统伪造数字字段，生成4061张伪造图像，并提供像素级篡改区域标注。

Result: 测试三种代表性检测器：TruFor AUC=0.751（零样本、分布外），DocTamper AUC=0.563（像素级IoU=0.020），GPT-4o AUC=0.509，所有方法性能均显著下降，接近随机猜测水平。

Conclusion: AIForge-Doc代表了文档取证领域一个全新的、尚未解决的挑战，AI伪造的数值在自动检测器和视觉语言模型中几乎无法区分。

Abstract: We present AIForge-Doc, the first dedicated benchmark targeting exclusively diffusion-model-based inpainting in financial and form documents with pixel-level annotation. Existing document forgery datasets rely on traditional digital editing tools (e.g., Adobe Photoshop, GIMP), creating a critical gap: state-of-the-art detectors are blind to the rapidly growing threat of AI-forged document fraud. AIForge-Doc addresses this gap by systematically forging numeric fields in real-world receipt and form images using two AI inpainting APIs -- Gemini 2.5 Flash Image and Ideogram v2 Edit -- yielding 4,061 forged images from four public document datasets (CORD, WildReceipt, SROIE, XFUND) across nine languages, annotated with pixel-precise tampered-region masks in DocTamper-compatible format. We benchmark three representative detectors -- TruFor, DocTamper, and a zero-shot GPT-4o judge -- and find that all existing methods degrade substantially: TruFor achieves AUC=0.751 (zero-shot, out-of-distribution) vs. AUC=0.96 on NIST16; DocTamper achieves AUC=0.563 vs. AUC=0.98 in-distribution, with pixel-level IoU=0.020; GPT-4o achieves only 0.509 -- essentially at chance -- confirming that AI-forged values are indistinguishable to automated detectors and VLMs. These results demonstrate that AIForge-Doc represents a qualitatively new and unsolved challenge for document forensics.

</details>


### [30] [An interactive enhanced driving dataset for autonomous driving](https://arxiv.org/abs/2602.20575)
*Haojie Feng,Peizhi Zhang,Mengjie Tian,Xinrui Zhang,Zhuoren Li,Junpeng Huang,Xiurong Wang,Junfan Zhu,Jianzhou Wang,Dongxiao Yin,Lu Xiong*

Main category: cs.CV

TL;DR: 提出Interactive Enhanced Driving Dataset (IEDD)和IEDD-VQA数据集，解决自动驾驶VLA模型发展中交互场景稀疏和多模态对齐不足的问题，通过大规模交互片段挖掘和BEV视频生成来增强模型推理能力评估。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶向全自动化发展需要强大的交互能力，但现有数据中交互场景稀疏且多模态对齐不足，制约了Vision-Language-Action模型的发展。

Method: 开发可扩展的流水线从自然驾驶数据中挖掘百万级交互片段，基于交互轨迹设计量化指标；构建IEDD-VQA数据集，生成语义动作与结构化语言严格对齐的合成鸟瞰图视频。

Result: 提供了对10个主流视觉语言模型的基准测试结果，展示了数据集在评估和微调自动驾驶模型推理能力方面的重用价值。

Conclusion: IEDD数据集为解决自动驾驶交互能力评估提供了有效的数据基础，能够支持VLA模型的性能评估和优化，推动自动驾驶交互能力的发展。

Abstract: The evolution of autonomous driving towards full automation demands robust interactive capabilities; however, the development of Vision-Language-Action (VLA) models is constrained by the sparsity of interactive scenarios and inadequate multimodal alignment in existing data. To this end, this paper proposes the Interactive Enhanced Driving Dataset (IEDD). We develop a scalable pipeline to mine million-level interactive segments from naturalistic driving data based on interactive trajectories, and design metrics to quantify the interaction processes. Furthermore, the IEDD-VQA dataset is constructed by generating synthetic Bird's Eye View (BEV) videos where semantic actions are strictly aligned with structured language. Benchmark results evaluating ten mainstream Vision Language Models (VLMs) are provided to demonstrate the dataset's reuse value in assessing and fine-tuning the reasoning capabilities of autonomous driving models.

</details>


### [31] [Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion](https://arxiv.org/abs/2602.20577)
*Jiaru Zhang,Manav Gagvani,Can Cui,Juntong Peng,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: MVLAD-AD是一个新颖的自动驾驶框架，通过掩码视觉-语言-动作扩散模型解决现有LLM/VLM在推理延迟、动作精度和可解释性方面的挑战，采用离散动作标记化和几何感知嵌入学习，在nuScenes基准上实现了优越的效率和规划精度。


<details>
  <summary>Details</summary>
Motivation: 现有自回归方法存在逐token生成速度慢的问题，而扩散规划器依赖缺乏明确几何结构的通用语言token，需要解决高效规划与语义可解释性之间的差距。

Method: 提出离散动作标记化策略构建紧凑的运动学可行路径点码本；采用几何感知嵌入学习确保潜在空间嵌入近似物理几何度量；引入动作优先解码策略优先生成轨迹。

Result: 在nuScenes及其衍生基准上的广泛实验表明，MVLAD-AD实现了优越的效率，在规划精度上超越了最先进的自回归和扩散基线方法，同时提供高保真和可解释的推理。

Conclusion: MVLAD-AD框架成功弥合了高效规划与语义可解释性之间的差距，通过创新的离散动作表示和几何感知方法，为自动驾驶提供了更精确、高效且可解释的规划解决方案。

Abstract: Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.

</details>


### [32] [PropFly: Learning to Propagate via On-the-Fly Supervision from Pre-trained Video Diffusion Models](https://arxiv.org/abs/2602.20583)
*Wonyong Seo,Jaeho Moon,Jaehyup Lee,Soo Ye Kim,Munchurl Kim*

Main category: cs.CV

TL;DR: PropFly：基于预训练视频扩散模型的即时监督训练框架，无需成对视频编辑数据集即可实现高质量的视频传播编辑


<details>
  <summary>Details</summary>
Motivation: 传统传播式视频编辑需要大规模成对视频数据集进行训练，但这类数据集获取成本高且复杂，限制了模型的可扩展性和应用范围

Method: 利用预训练视频扩散模型，通过不同CFG尺度下的单步潜空间估计生成多样化的'源'（低CFG）和'编辑后'（高CFG）潜空间对作为即时监督信号，采用Guidance-Modulated Flow Matching损失训练适配器学习编辑传播

Result: 在多个视频编辑任务上显著超越现有最先进方法，能够产生高质量、时间一致的编辑结果

Conclusion: PropFly提供了一种无需成对训练数据的有效视频编辑训练范式，通过预训练模型的即时监督实现了高质量的视频编辑传播，具有很好的实用性和扩展性

Abstract: Propagation-based video editing enables precise user control by propagating a single edited frame into following frames while maintaining the original context such as motion and structures. However, training such models requires large-scale, paired (source and edited) video datasets, which are costly and complex to acquire. Hence, we propose the PropFly, a training pipeline for Propagation-based video editing, relying on on-the-Fly supervision from pre-trained video diffusion models (VDMs) instead of requiring off-the-shelf or precomputed paired video editing datasets. Specifically, our PropFly leverages one-step clean latent estimations from intermediate noised latents with varying Classifier-Free Guidance (CFG) scales to synthesize diverse pairs of 'source' (low-CFG) and 'edited' (high-CFG) latents on-the-fly. The source latent serves as structural information of the video, while the edited latent provides the target transformation for learning propagation. Our pipeline enables an additional adapter attached to the pre-trained VDM to learn to propagate edits via Guidance-Modulated Flow Matching (GMFM) loss, which guides the model to replicate the target transformation. Our on-the-fly supervision ensures the model to learn temporally consistent and dynamic transformations. Extensive experiments demonstrate that our PropFly significantly outperforms the state-of-the-art methods on various video editing tasks, producing high-quality editing results.

</details>


### [33] [Long-Term Multi-Session 3D Reconstruction Under Substantial Appearance Change](https://arxiv.org/abs/2602.20584)
*Beverley Gorry,Tobias Fischer,Michael Milford,Alejandro Fontan*

Main category: cs.CV

TL;DR: 提出一种联合SfM重建方法，通过跨会话对应关系直接处理长期环境监测中的大时间跨度图像对齐问题，结合手工和深度学习特征在显著外观变化下实现稳健的3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有SfM方法假设图像近同时捕获且外观变化有限，在长期环境监测（如珊瑚礁调查）中因大时间跨度的视觉和结构变化而失效，需要解决独立重建会话后对齐的局限性。

Method: 在联合SfM重建中直接强制执行跨会话对应关系，结合手工和深度学习特征建立大时间间隔下的稳健对应，并通过视觉位置识别筛选可能匹配的图像对以降低计算成本。

Result: 在经历显著真实世界变化的长期珊瑚礁数据集上评估，该方法在现有方法失效的情况下能够实现一致的联合重建，展示了优于标准独立和联合SfM管道的性能。

Conclusion: 通过跨会话对应关系的直接强制执行和混合特征策略，该方法成功解决了长期环境监测中的3D重建挑战，为处理大时间跨度外观变化提供了有效解决方案。

Abstract: Long-term environmental monitoring requires the ability to reconstruct and align 3D models across repeated site visits separated by months or years. However, existing Structure-from-Motion (SfM) pipelines implicitly assume near-simultaneous image capture and limited appearance change, and therefore fail when applied to long-term monitoring scenarios such as coral reef surveys, where substantial visual and structural change is common. In this paper, we show that the primary limitation of current approaches lies in their reliance on post-hoc alignment of independently reconstructed sessions, which is insufficient under large temporal appearance change. We address this limitation by enforcing cross-session correspondences directly within a joint SfM reconstruction. Our approach combines complementary handcrafted and learned visual features to robustly establish correspondences across large temporal gaps, enabling the reconstruction of a single coherent 3D model from imagery captured years apart, where standard independent and joint SfM pipelines break down. We evaluate our method on long-term coral reef datasets exhibiting significant real-world change, and demonstrate consistent joint reconstruction across sessions in cases where existing methods fail to produce coherent reconstructions. To ensure scalability to large datasets, we further restrict expensive learned feature matching to a small set of likely cross-session image pairs identified via visual place recognition, which reduces computational cost and improves alignment robustness.

</details>


### [34] [Interaction-aware Representation Modeling with Co-occurrence Consistency for Egocentric Hand-Object Parsing](https://arxiv.org/abs/2602.20597)
*Yuejiao Su,Yi Wang,Lei Yao,Yawen Cui,Lap-Pui Chau*

Main category: cs.CV

TL;DR: InterFormer：一种端到端的交互感知Transformer模型，通过动态查询生成器、双上下文特征选择器和条件共现损失，解决了手-物交互解析中的查询初始化、噪声抑制和物理一致性等关键问题，在EgoHOS和mini-HOI4D数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决第一人称视角人机交互解析中的三个关键挑战：1)现有查询初始化机制对变化场景适应性差；2)像素级语义特征可能引入交互无关噪声；3)模型易产生物理不一致的"交互幻觉"预测。

Method: 提出InterFormer框架，包含三个核心组件：动态查询生成器(DQG)基于手-物接触空间动态生成交互感知查询；双上下文特征选择器(DFS)融合交互线索与语义特征，抑制噪声并强调交互关系学习；条件共现(CoCo)损失引入手-物关系约束增强物理一致性。

Result: 在EgoHOS和更具挑战性的out-of-distribution mini-HOI4D数据集上均取得最先进性能，证明了方法的有效性和强泛化能力。

Conclusion: InterFormer通过创新的查询初始化机制、特征选择策略和物理约束损失，成功解决了手-物交互解析中的关键问题，为下一代具身智能体的发展提供了重要技术基础。

Abstract: A fine-grained understanding of egocentric human-environment interactions is crucial for developing next-generation embodied agents. One fundamental challenge in this area involves accurately parsing hands and active objects. While transformer-based architectures have demonstrated considerable potential for such tasks, several key limitations remain unaddressed: 1) existing query initialization mechanisms rely primarily on semantic cues or learnable parameters, demonstrating limited adaptability to changing active objects across varying input scenes; 2) previous transformer-based methods utilize pixel-level semantic features to iteratively refine queries during mask generation, which may introduce interaction-irrelevant content into the final embeddings; and 3) prevailing models are susceptible to "interaction illusion", producing physically inconsistent predictions. To address these issues, we propose an end-to-end Interaction-aware Transformer (InterFormer), which integrates three key components, i.e., a Dynamic Query Generator (DQG), a Dual-context Feature Selector (DFS), and the Conditional Co-occurrence (CoCo) loss. The DQG explicitly grounds query initialization in the spatial dynamics of hand-object contact, enabling targeted generation of interaction-aware queries for hands and various active objects. The DFS fuses coarse interactive cues with semantic features, thereby suppressing interaction-irrelevant noise and emphasizing the learning of interactive relationships. The CoCo loss incorporates hand-object relationship constraints to enhance physical consistency in prediction. Our model achieves state-of-the-art performance on both the EgoHOS and the challenging out-of-distribution mini-HOI4D datasets, demonstrating its effectiveness and strong generalization ability. Code and models are publicly available at https://github.com/yuggiehk/InterFormer.

</details>


### [35] [VAGNet: Grounding 3D Affordance from Human-Object Interactions in Videos](https://arxiv.org/abs/2602.20608)
*Aihua Mao,Kaihang Huang,Yong-Jin Liu,Chee Seng Chan,Ying He*

Main category: cs.CV

TL;DR: VAGNet：基于视频引导的3D物体可供性定位框架，通过动态交互序列提供功能监督，解决静态方法无法处理的模糊性，在PVAD数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有3D物体可供性定位方法依赖静态视觉或文本线索，忽略了可供性本质由动态动作定义，难以准确定位真实交互接触区域。人类通过观察和模仿动作学习物体使用方式，而非仅通过形状检查

Method: 提出视频引导的3D可供性定位方法VAGNet，将视频衍生的交互线索与3D结构对齐，解决静态线索无法处理的模糊性。创建首个HOI视频-3D配对可供性数据集PVAD提供功能监督

Result: 在PVAD数据集上的广泛实验表明，VAGNet达到最先进性能，显著优于基于静态方法的基线模型

Conclusion: 动态交互序列能为3D可供性定位提供关键功能监督，VAGNet框架有效解决了静态方法的局限性，代码和数据集将公开

Abstract: 3D object affordance grounding aims to identify regions on 3D objects that support human-object interaction (HOI), a capability essential to embodied visual reasoning. However, most existing approaches rely on static visual or textual cues, neglecting that affordances are inherently defined by dynamic actions. As a result, they often struggle to localize the true contact regions involved in real interactions. We take a different perspective. Humans learn how to use objects by observing and imitating actions, not just by examining shapes. Motivated by this intuition, we introduce video-guided 3D affordance grounding, which leverages dynamic interaction sequences to provide functional supervision. To achieve this, we propose VAGNet, a framework that aligns video-derived interaction cues with 3D structure to resolve ambiguities that static cues cannot address. To support this new setting, we introduce PVAD, the first HOI video-3D pairing affordance dataset, providing functional supervision unavailable in prior works. Extensive experiments on PVAD show that VAGNet achieves state-of-the-art performance, significantly outperforming static-based baselines. The code and dataset will be open publicly.

</details>


### [36] [Knowing the Unknown: Interpretable Open-World Object Detection via Concept Decomposition Model](https://arxiv.org/abs/2602.20616)
*Xueqiang Lv,Shizhou Zhang,Yinghui Xing,Di Xu,Peng Wang,Yanning Zhang*

Main category: cs.CV

TL;DR: IPOW是一个可解释的开放世界目标检测框架，通过概念分解模型将特征分解为判别性、共享和背景概念，并使用概念引导修正来解决已知-未知类别混淆问题，显著提高了未知类别检测性能并提供了概念级可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有开放世界目标检测方法主要关注提高未知类别召回率，但忽视了可解释性，导致已知-未知类别混淆和预测可靠性降低。本文旨在使整个OWOD框架可解释，让检测器真正'了解未知'。

Method: 提出概念驱动的可解释OWOD框架(IPOW)，引入概念分解模型(CDM)将Faster R-CNN中的耦合RoI特征显式分解为判别性概念、共享概念和背景概念。判别性概念用于区分已知类别，共享和背景概念因其强泛化能力可迁移到未知类别检测。还提出概念引导修正(CGR)来解决已知-未知混淆问题。

Result: 大量实验表明，IPOW显著提高了未知类别召回率，同时缓解了类别混淆问题，为已知和未知类别的预测提供了概念级的可解释性。

Conclusion: IPOW框架通过概念分解和概念引导修正，成功实现了开放世界目标检测的可解释性，有效解决了已知-未知类别混淆问题，为检测器提供了真正的'了解未知'能力。

Abstract: Open-world object detection (OWOD) requires incrementally detecting known categories while reliably identifying unknown objects. Existing methods primarily focus on improving unknown recall, yet overlook interpretability, often leading to known-unknown confusion and reduced prediction reliability. This paper aims to make the entire OWOD framework interpretable, enabling the detector to truly "knowing the unknown". To this end, we propose a concept-driven InterPretable OWOD framework(IPOW) by introducing a Concept Decomposition Model (CDM) for OWOD, which explicitly decomposes the coupled RoI features in Faster R-CNN into discriminative, shared, and background concepts. Discriminative concepts identify the most discriminative features to enlarge the distances between known categories, while shared and background concepts, due to their strong generalization ability, can be readily transferred to detect unknown categories. Leveraging the interpretable framework, we identify that known-unknown confusion arises when unknown objects fall into the discriminative space of known classes. To address this, we propose Concept-Guided Rectification (CGR) to further resolve such confusion. Extensive experiments show that IPOW significantly improves unknown recall while mitigating confusion, and provides concept-level interpretability for both known and unknown predictions.

</details>


### [37] [RecoverMark: Robust Watermarking for Localization and Recovery of Manipulated Faces](https://arxiv.org/abs/2602.20618)
*Haonan An,Xiaohui Ye,Guang Hua,Yihang Tao,Hangcheng Cao,Xiangyu Yu,Yuguang Fang*

Main category: cs.CV

TL;DR: RecoverMark是一个创新的水印框架，通过将人脸内容本身作为水印嵌入背景区域，同时实现鲁棒的篡改定位、内容恢复和所有权验证，解决了传统脆弱水印易被移除的问题。


<details>
  <summary>Details</summary>
Motivation: AI生成内容的泛滥导致人脸篡改技术日益复杂，传统脆弱水印方法假设攻击者不知道水印存在，忽视了水印移除攻击的脆弱性。双水印策略中相互干扰和有限嵌入容量进一步降低了脆弱水印的有效性。

Method: 利用攻击者必须保持背景语义一致性的现实约束，将受保护的人脸内容作为水印嵌入周围背景。采用两阶段训练范式，通过精心设计的失真层模拟全面潜在攻击，实现鲁棒的水印嵌入。

Result: 大量实验证明RecoverMark对已知和未知攻击都具有鲁棒性，且在分布内和分布外数据上具有良好的泛化能力。

Conclusion: RecoverMark通过创新性地将内容本身作为水印，有效解决了传统水印方法的脆弱性问题，为图像完整性保护和知识产权验证提供了更可靠的解决方案。

Abstract: The proliferation of AI-generated content has facilitated sophisticated face manipulation, severely undermining visual integrity and posing unprecedented challenges to intellectual property. In response, a common proactive defense leverages fragile watermarks to detect, localize, or even recover manipulated regions. However, these methods always assume an adversary unaware of the embedded watermark, overlooking their inherent vulnerability to watermark removal attacks. Furthermore, this fragility is exacerbated in the commonly used dual-watermark strategy that adds a robust watermark for image ownership verification, where mutual interference and limited embedding capacity reduce the fragile watermark's effectiveness. To address the gap, we propose RecoverMark, a watermarking framework that achieves robust manipulation localization, content recovery, and ownership verification simultaneously. Our key insight is twofold. First, we exploit a critical real-world constraint: an adversary must preserve the background's semantic consistency to avoid visual detection, even if they apply global, imperceptible watermark removal attacks. Second, using the image's own content (face, in this paper) as the watermark enhances extraction robustness. Based on these insights, RecoverMark treats the protected face content itself as the watermark and embeds it into the surrounding background. By designing a robust two-stage training paradigm with carefully crafted distortion layers that simulate comprehensive potential attacks and a progressive training strategy, RecoverMark achieves a robust watermark embedding in no fragile manner for image manipulation localization, recovery, and image IP protection simultaneously. Extensive experiments demonstrate the proposed RecoverMark's robustness against both seen and unseen attacks and its generalizability to in-distribution and out-of-distribution data.

</details>


### [38] [Object-Scene-Camera Decomposition and Recomposition for Data-Efficient Monocular 3D Object Detection](https://arxiv.org/abs/2602.20627)
*Zhaonian Kuang,Rui Ding,Meng Yang,Xinhu Zheng,Gang Hua*

Main category: cs.CV

TL;DR: 提出一种在线对象-场景-相机分解与重组的数据增强方案，通过将训练图像分解为纹理化3D对象点模型和背景场景，然后在每个epoch中重新组合新的训练图像，插入3D对象到背景场景的自由空间并使用扰动相机姿态进行渲染，从而解决M3OD训练数据中对象、场景和相机姿态紧密耦合的问题。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测本质上是病态问题，需要大量标注数据。但现有训练数据中对象、场景和相机姿态三者紧密耦合，缺乏多样性，导致数据利用不足和过拟合问题。

Method: 1) 将训练图像高效分解为纹理化3D对象点模型和背景场景；2) 在每个训练epoch中重新组合新图像，将3D对象插入背景场景的自由空间；3) 使用扰动相机姿态从纹理化3D点表示进行渲染；4) 支持全监督和稀疏监督设置。

Result: 方法在KITTI和更复杂的Waymo数据集上应用于五个代表性M3OD模型进行验证，证明能够有效提升模型性能。

Conclusion: 该方法作为即插即用组件能够有效提升M3OD模型的性能，通过数据分解和重组解决了训练数据多样性不足的问题，同时支持灵活的标注成本控制。

Abstract: Monocular 3D object detection (M3OD) is intrinsically ill-posed, hence training a high-performance deep learning based M3OD model requires a humongous amount of labeled data with complicated visual variation from diverse scenes, variety of objects and camera poses.However, we observe that, due to strong human bias, the three independent entities, i.e., object, scene, and camera pose, are always tightly entangled when an image is captured to construct training data. More specifically, specific 3D objects are always captured in particular scenes with fixed camera poses, and hence lacks necessary diversity. Such tight entanglement induces the challenging issues of insufficient utilization and overfitting to uniform training data. To mitigate this, we propose an online object-scene-camera decomposition and recomposition data manipulation scheme to more efficiently exploit the training data. We first fully decompose training images into textured 3D object point models and background scenes in an efficient computation and storage manner. We then continuously recompose new training images in each epoch by inserting the 3D objects into the freespace of the background scenes, and rendering them with perturbed camera poses from textured 3D point representation. In this way, the refreshed training data in all epochs can cover the full spectrum of independent object, scene, and camera pose combinations. This scheme can serve as a plug-and-play component to boost M3OD models, working flexibly with both fully and sparsely supervised settings. In the sparsely-supervised setting, objects closest to the ego-camera for all instances are sparsely annotated. We then can flexibly increase the annotated objects to control annotation cost. For validation, our method is widely applied to five representative M3OD models and evaluated on both the KITTI and the more complicated Waymo datasets.

</details>


### [39] [VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation](https://arxiv.org/abs/2602.21054)
*Seongheon Park,Changdae Oh,Hyeong Kyu Choi,Xuefeng Du,Sharon Li*

Main category: cs.CV

TL;DR: VAUQ是一个针对大型视觉语言模型的视觉感知不确定性量化框架，通过图像信息分数和核心区域掩码策略来评估模型输出对视觉证据的依赖程度，显著提升自我评估性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我评估方法过度依赖语言先验，不适用于视觉条件预测的评估，LVLM模型存在幻觉问题限制实际应用部署。

Method: 提出图像信息分数(IS)量化视觉输入带来的预测不确定性减少，采用无监督核心区域掩码策略增强显著区域影响，结合预测熵构建无需训练的打分函数。

Result: 在多个数据集上，VAUQ持续优于现有自我评估方法，可靠反映答案正确性。

Conclusion: VAUQ框架通过显式测量模型输出对视觉证据的依赖程度，有效解决了LVLM幻觉问题，为模型安全部署提供了可靠的自我评估方案。

Abstract: Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.

</details>


### [40] [From Pairs to Sequences: Track-Aware Policy Gradients for Keypoint Detection](https://arxiv.org/abs/2602.20630)
*Yepeng Liu,Hao Li,Liwen Yang,Fangzhen Li,Xudi Ge,Yuliang Gu,kuang Gao,Bing Wang,Guang Chen,Hangjun Ye,Yongchao Xu*

Main category: cs.CV

TL;DR: TraqPoint是一个基于强化学习的端到端关键点检测框架，通过序列决策优化关键点在多视角下的跟踪质量，在稀疏匹配基准测试中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的关键点匹配方法主要在图像对上训练，未能显式优化关键点在挑战性视角和光照变化下的长期可跟踪性

Method: 将关键点检测重构为序列决策问题，引入基于策略梯度方法的轨迹感知奖励机制，联合优化多视角下关键点的一致性和区分性

Result: 在相对姿态估计和3D重建等稀疏匹配基准测试中，TraqPoint显著优于最先进的关键点检测和描述方法

Conclusion: 通过强化学习直接优化关键点跟踪质量的新范式，为3D视觉系统中的关键点匹配提供了更有效的解决方案

Abstract: Keypoint-based matching is a fundamental component of modern 3D vision systems, such as Structure-from-Motion (SfM) and SLAM. Most existing learning-based methods are trained on image pairs, a paradigm that fails to explicitly optimize for the long-term trackability of keypoints across sequences under challenging viewpoint and illumination changes. In this paper, we reframe keypoint detection as a sequential decision-making problem. We introduce TraqPoint, a novel, end-to-end Reinforcement Learning (RL) framework designed to optimize the \textbf{Tra}ck-\textbf{q}uality (Traq) of keypoints directly on image sequences. Our core innovation is a track-aware reward mechanism that jointly encourages the consistency and distinctiveness of keypoints across multiple views, guided by a policy gradient method. Extensive evaluations on sparse matching benchmarks, including relative pose estimation and 3D reconstruction, demonstrate that TraqPoint significantly outperforms some state-of-the-art (SOTA) keypoint detection and description methods.

</details>


### [41] [Boosting Instance Awareness via Cross-View Correlation with 4D Radar and Camera for 3D Object Detection](https://arxiv.org/abs/2602.20632)
*Xiaokai Bai,Lianqing Zheng,Si-Yuan Cao,Xiaohan Zhang,Zhe Wu,Beinan Yu,Fang Wang,Jie Bai,Hui-Liang Shen*

Main category: cs.CV

TL;DR: SIFormer是一种场景-实例感知的Transformer架构，通过分割和深度引导的定位抑制背景噪声，利用跨视图激活机制将2D实例线索注入BEV空间，结合Transformer融合模块聚合图像语义和雷达几何信息，解决了4D毫米波雷达稀疏几何线索导致的实例激活难题。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达在自动驾驶中具有鲁棒性和成本优势，但其稀疏的几何线索使得实例激活困难，现有雷达-相机融合范式效果有限。BEV级融合缺乏实例关注，透视级融合缺乏全局上下文，需要新的融合方法来结合两者的互补优势。

Method: 1. 通过分割和深度引导的定位在视图变换中抑制背景噪声
2. 引入跨视图激活机制，将2D实例线索注入BEV空间
3. 使用基于Transformer的融合模块聚合图像语义和雷达几何信息

Result: 在View-of-Delft、TJ4DRadSet和NuScenes数据集上实现了最先进的性能表现

Conclusion: SIFormer成功弥合了BEV级和透视级融合范式之间的差距，通过增强实例感知能力，有效解决了雷达数据稀疏性问题，提高了检测精度，为雷达-相机融合提供了新的有效解决方案。

Abstract: 4D millimeter-wave radar has emerged as a promising sensing modality for autonomous driving due to its robustness and affordability. However, its sparse and weak geometric cues make reliable instance activation difficult, limiting the effectiveness of existing radar-camera fusion paradigms. BEV-level fusion offers global scene understanding but suffers from weak instance focus, while perspective-level fusion captures instance details but lacks holistic context. To address these limitations, we propose SIFormer, a scene-instance aware transformer for 3D object detection using 4D radar and camera. SIFormer first suppresses background noise during view transformation through segmentation- and depth-guided localization. It then introduces a cross-view activation mechanism that injects 2D instance cues into BEV space, enabling reliable instance awareness under weak radar geometry. Finally, a transformer-based fusion module aggregates complementary image semantics and radar geometry for robust perception. As a result, with the aim of enhancing instance awareness, SIFormer bridges the gap between the two paradigms, combining their complementary strengths to address inherent sparse nature of radar and improve detection accuracy. Experiments demonstrate that SIFormer achieves state-of-the-art performance on View-of-Delft, TJ4DRadSet and NuScenes datasets. Source code is available at github.com/shawnnnkb/SIFormer.

</details>


### [42] [SurgAtt-Tracker: Online Surgical Attention Tracking via Temporal Proposal Reranking and Motion-Aware Refinement](https://arxiv.org/abs/2602.20636)
*Rulin Zhou,Guankun Wang,An Wang,Yujie Ma,Lixin Ouyang,Bolin Cui,Junyan Li,Chaowei Zhu,Mingyang Li,Ming Chen,Xiaopin Zhong,Peng Lu,Jiankun Wang,Xianming Liu,Hongliang Ren*

Main category: cs.CV

TL;DR: SurgAtt-Tracker是一个通过时空学习建模外科医生注意力热图的手术视野引导框架，结合提案级重排序和运动感知细化来鲁棒跟踪手术注意力，并在大规模数据集上实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往将视觉注意力估计与相机控制混为一谈，或依赖直接的对象中心假设，需要更准确稳定的手术视野引导来确保微创手术的安全性和效率。

Method: 将手术注意力跟踪建模为时空学习问题，通过提案级重排序和运动感知细化来利用时间一致性，而非直接回归，同时构建了大规模标注数据集SurgAtt-1.16M用于训练和评估。

Result: 在多个手术数据集上的广泛实验表明，SurgAtt-Tracker在遮挡、多器械干扰和跨域设置下均实现了最先进的性能和强鲁棒性。

Conclusion: 该方法不仅提供了精确的注意力跟踪，还能为下游机器人视野规划和自动相机控制提供帧级视野引导信号，具有重要的临床应用价值。

Abstract: Accurate and stable field-of-view (FoV) guidance is critical for safe and efficient minimally invasive surgery, yet existing approaches often conflate visual attention estimation with downstream camera control or rely on direct object-centric assumptions. In this work, we formulate surgical attention tracking as a spatio-temporal learning problem and model surgeon focus as a dense attention heatmap, enabling continuous and interpretable frame-wise FoV guidance. We propose SurgAtt-Tracker, a holistic framework that robustly tracks surgical attention by exploiting temporal coherence through proposal-level reranking and motion-aware refinement, rather than direct regression. To support systematic training and evaluation, we introduce SurgAtt-1.16M, a large-scale benchmark with a clinically grounded annotation protocol that enables comprehensive heatmap-based attention analysis across procedures and institutions. Extensive experiments on multiple surgical datasets demonstrate that SurgAtt-Tracker consistently achieves state-of-the-art performance and strong robustness under occlusion, multi-instrument interference, and cross-domain settings. Beyond attention tracking, our approach provides a frame-wise FoV guidance signal that can directly support downstream robotic FoV planning and automatic camera control.

</details>


### [43] [Dataset Color Quantization: A Training-Oriented Framework for Dataset-Level Compression](https://arxiv.org/abs/2602.20650)
*Chenyue Yu,Lingao Xiao,Jinhong Deng,Ivor W. Tsang,Yang He*

Main category: cs.CV

TL;DR: DCQ是一种通过减少颜色空间冗余来压缩视觉数据集的新框架，通过跨图像统一调色板表示、基于模型感知保留语义重要颜色、保持结构细节，在保持训练性能的同时显著减少存储需求。


<details>
  <summary>Details</summary>
Motivation: 大规模图像数据集对深度学习至关重要，但其高存储需求在资源受限环境中部署面临挑战。现有方法通过丢弃样本来减少数据集大小，但忽略了图像内部（特别是颜色空间）的显著冗余。

Method: 提出Dataset Color Quantization (DCQ)统一框架，通过跨相似图像实施一致的调色板表示、基于模型感知选择性地保留语义重要颜色、保持有效特征学习所需的结构细节来压缩视觉数据集。

Result: 在CIFAR-10、CIFAR-100、Tiny-ImageNet和ImageNet-1K上的广泛实验表明，DCQ在激进压缩条件下显著提高训练性能，为数据集级存储减少提供了可扩展且鲁棒的解决方案。

Conclusion: DCQ通过减少颜色空间冗余有效压缩视觉数据集，在保持模型训练性能的同时显著降低存储需求，为资源受限环境中的深度学习部署提供了实用解决方案。

Abstract: Large-scale image datasets are fundamental to deep learning, but their high storage demands pose challenges for deployment in resource-constrained environments. While existing approaches reduce dataset size by discarding samples, they often ignore the significant redundancy within each image -- particularly in the color space. To address this, we propose Dataset Color Quantization (DCQ), a unified framework that compresses visual datasets by reducing color-space redundancy while preserving information crucial for model training. DCQ achieves this by enforcing consistent palette representations across similar images, selectively retaining semantically important colors guided by model perception, and maintaining structural details necessary for effective feature learning. Extensive experiments across CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show that DCQ significantly improves training performance under aggressive compression, offering a scalable and robust solution for dataset-level storage reduction. Code is available at \href{https://github.com/he-y/Dataset-Color-Quantization}{https://github.com/he-y/Dataset-Color-Quantization}.

</details>


### [44] [SD4R: Sparse-to-Dense Learning for 3D Object Detection with 4D Radar](https://arxiv.org/abs/2602.20653)
*Xiaokai Bai,Jiahao Cheng,Songkai Wang,Yixuan Luo,Lianqing Zheng,Xiaohan Zhang,Si-Yuan Cao,Hui-Liang Shen*

Main category: cs.CV

TL;DR: SD4R是一个新颖的4D雷达点云稠密化框架，通过前景点生成器和逻辑查询编码器，有效解决雷达点云稀疏性和噪声问题，在View-of-Delft数据集上实现了最先进的3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 4D雷达提供经济且天气鲁棒的3D感知方案，但点云固有的稀疏性和噪声给准确的目标检测带来挑战，现有稠密化方法难以处理极端稀疏场景且鲁棒性有限。

Method: 提出SD4R框架：1）前景点生成器(FPG)减少噪声传播并生成稠密点云；2）逻辑查询编码器(LQE)增强传统柱状化处理，获得鲁棒的特征表示。

Result: 在公开的View-of-Delft数据集上进行广泛实验，SD4R在噪声抑制和前景点稠密化方面表现出强大能力，达到了最先进的性能水平。

Conclusion: SD4R通过创新的FPG和LQE组件，成功解决了4D雷达点云的稀疏性和噪声问题，为雷达点云稠密化提供了有效的解决方案，代码已开源。

Abstract: 4D radar measurements offer an affordable and weather-robust solution for 3D perception. However, the inherent sparsity and noise of radar point clouds present significant challenges for accurate 3D object detection, underscoring the need for effective and robust point clouds densification. Despite recent progress, existing densification methods often fail to address the extreme sparsity of 4D radar point clouds and exhibit limited robustness when processing scenes with a small number of points. In this paper, we propose SD4R, a novel framework that transforms sparse radar point clouds into dense representations. SD4R begins by utilizing a foreground point generator (FPG) to mitigate noise propagation and produce densified point clouds. Subsequently, a logit-query encoder (LQE) enhances conventional pillarization, resulting in robust feature representations. Through these innovations, our SD4R demonstrates strong capability in both noise reduction and foreground point densification. Extensive experiments conducted on the publicly available View-of-Delft dataset demonstrate that SD4R achieves state-of-the-art performance. Source code is available at https://github.com/lancelot0805/SD4R.

</details>


### [45] [Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video](https://arxiv.org/abs/2602.20658)
*Mohammad Sadra Rajabi,Aanuoluwapo Ojelade,Sunwook Kim,Maury A. Nussbaum*

Main category: cs.CV

TL;DR: 评估使用视觉语言模型从RGB视频流非侵入式估计NIOSH举升方程中水平和垂直距离参数的可行性，开发了两种多阶段VLM管道并比较性能，发现基于分割的多视角管道误差最小。


<details>
  <summary>Details</summary>
Motivation: 手动举升任务是职业性肌肉骨骼疾病的主要诱因，传统RNLE工具的距离测量依赖人工或专用传感系统，难以在真实环境中应用，需要非侵入式的自动化解决方案。

Method: 开发了两种多阶段VLM管道：文本引导的纯检测管道和检测+分割管道。通过文本引导定位感兴趣区域，提取视觉特征，使用基于transformer的时间回归来估计举升开始和结束时的H和V距离。

Result: 分割基础的多视角管道表现最佳，H和V的平均绝对误差分别为6-8cm和5-8cm。分割相比纯检测管道将H误差降低20-30%，V误差降低35-40%。

Conclusion: VLM基础管道在视频基RNLE距离参数估计方面具有可行性，分割方法显著提升估计精度，为实际应用提供了有前景的技术路径。

Abstract: Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.

</details>


### [46] [AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?](https://arxiv.org/abs/2602.20664)
*Hailong Yan,Shice Liu,Tao Wang,Xiangtao Zhang,Yijie Zhong,Jinwei Chen,Le Zhang,Bo Li*

Main category: cs.CV

TL;DR: AnimeAgent是首个基于图像到视频(I2V)的多智能体框架，通过结合主观-客观评审机制解决传统静态扩散模型在自定义故事板生成中的局限性，实现了最先进的一致性和风格化效果。


<details>
  <summary>Details</summary>
Motivation: 解决当前静态扩散模型在故事板生成中的三个关键限制：缺乏动态表现力、无法迭代修正缺失属性、以及多智能体框架依赖不稳健的评估器。

Method: 采用Disney"直进与姿态结合"工作流启发的方法，利用I2V的隐式运动先验增强一致性，通过混合主观-客观评审器实现可靠的迭代优化。

Result: 实验表明AnimeAgent在一致性、提示忠实度和风格化方面达到了最先进的性能表现。

Conclusion: AnimeAgent框架成功解决了CSG领域的核心挑战，为高质量多角色一致性故事叙述提供了有效的解决方案，并通过人工标注基准验证了其优越性。

Abstract: Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to "copy-paste" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's "Combination of Straight Ahead and Pose to Pose" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.

</details>


### [47] [BoxSplitGen: A Generative Model for 3D Part Bounding Boxes in Varying Granularity](https://arxiv.org/abs/2602.20666)
*Juil Koo,Wei-Tung Lin,Chanho Park,Chanhyeok Park,Minhyuk Sung*

Main category: cs.CV

TL;DR: 提出了一个交互式3D形状生成框架，通过迭代分割边界框从粗到细细化形状。包含两个生成模型：BoxSplitGen用于生成多粒度部件边界框，以及边界框到形状的生成模型。


<details>
  <summary>Details</summary>
Motivation: 人类创造力遵循从抽象到细节的感知过程，但现有3D生成模型缺乏专门支持这种从粗到细创作流程的方法。

Method: BoxSplitGen模型学习合并过程的逆向分割序列，包含两个组件：学习要分割框的类别分布和学习新框的分布。边界框到形状模型利用现有3D扩散模型先验并加入边界框条件。

Result: 实验显示BoxSplitGen优于标记预测模型和无条件扩散模型的修复方法，边界框到形状模型相比先前模型提供更优结果。

Conclusion: 该框架为3D创作提供了直观的从粗到细生成方法，通过迭代边界框分割有效支持人类想象力在3D创作中的应用。

Abstract: Human creativity follows a perceptual process, moving from abstract ideas to finer details during creation. While 3D generative models have advanced dramatically, models specifically designed to assist human imagination in 3D creation -- particularly for detailing abstractions from coarse to fine -- have not been explored. We propose a framework that enables intuitive and interactive 3D shape generation by iteratively splitting bounding boxes to refine the set of bounding boxes. The main technical components of our framework are two generative models: the box-splitting generative model and the box-to-shape generative model. The first model, named BoxSplitGen, generates a collection of 3D part bounding boxes with varying granularity by iteratively splitting coarse bounding boxes. It utilizes part bounding boxes created through agglomerative merging and learns the reverse of the merging process -- the splitting sequences. The model consists of two main components: the first learns the categorical distribution of the box to be split, and the second learns the distribution of the two new boxes, given the set of boxes and the indication of which box to split. The second model, the box-to-shape generative model, is trained by leveraging the 3D shape priors learned by an existing 3D diffusion model while adapting the model to incorporate bounding box conditioning. In our experiments, we demonstrate that the box-splitting generative model outperforms token prediction models and the inpainting approach with an unconditional diffusion model. Also, we show that our box-to-shape model, based on a state-of-the-art 3D diffusion model, provides superior results compared to a previous model.

</details>


### [48] [BBQ-to-Image: Numeric Bounding Box and Qolor Control in Large-Scale Text-to-Image Models](https://arxiv.org/abs/2602.20672)
*Eliran Kachlon,Alexander Visheratin,Nimrod Sarid,Tal Hacham,Eyal Gutflaish,Saar Huberman,Hezi Zisman,David Ruppin,Ron Mokady*

Main category: cs.CV

TL;DR: BBQ是一个基于结构化文本框架的大规模文本到图像模型，通过直接使用数值边界框和RGB三元组作为条件输入，实现了对物体位置、大小和颜色的精确数值控制，无需架构修改或推理时优化。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型依赖描述性语言，但专业工作流程需要精确的数值控制（物体位置、尺寸和颜色），存在参数化差距。

Method: 通过在带有参数化标注的丰富标注数据上训练，使用统一的结构化文本框架直接处理数值边界框和RGB三元组。

Result: BBQ在综合评估中实现了强边界框对齐，并相比最先进基线提高了RGB颜色保真度。

Conclusion: 研究支持一种新范式：将用户意图转换为中间结构化语言，由基于流的transformer作为渲染器使用，自然适应数值参数。

Abstract: Text-to-image models have rapidly advanced in realism and controllability, with recent approaches leveraging long, detailed captions to support fine-grained generation. However, a fundamental parametric gap remains: existing models rely on descriptive language, whereas professional workflows require precise numeric control over object location, size, and color. In this work, we introduce BBQ, a large-scale text-to-image model that directly conditions on numeric bounding boxes and RGB triplets within a unified structured-text framework. We obtain precise spatial and chromatic control by training on captions enriched with parametric annotations, without architectural modifications or inference-time optimization. This also enables intuitive user interfaces such as object dragging and color pickers, replacing ambiguous iterative prompting with precise, familiar controls. Across comprehensive evaluations, BBQ achieves strong box alignment and improves RGB color fidelity over state-of-the-art baselines. More broadly, our results support a new paradigm in which user intent is translated into an intermediate structured language, consumed by a flow-based transformer acting as a renderer and naturally accommodating numeric parameters.

</details>


### [49] [GA-Drive: Geometry-Appearance Decoupled Modeling for Free-viewpoint Driving Scene Generatio](https://arxiv.org/abs/2602.20673)
*Hao Zhang,Lue Fan,Qitai Wang,Wenbo Li,Zehuan Wu,Lewei Lu,Zhaoxiang Zhang,Hongsheng Li*

Main category: cs.CV

TL;DR: GA-Drive是一个创新的驾驶模拟框架，通过几何-外观解耦和扩散模型生成技术，能够沿用户指定轨迹生成高质量的可编辑自由视角视图。


<details>
  <summary>Details</summary>
Motivation: 需要构建一个自由视角、可编辑且高保真的驾驶模拟器，用于训练和评估端到端自动驾驶系统。现有方法在生成新轨迹视图时存在质量不足和编辑能力有限的问题。

Method: 采用几何-外观解耦方法：首先利用几何信息从记录轨迹图像生成伪视图，然后通过训练的视频扩散模型将伪视图转换为逼真的照片级视图。支持通过先进的视频到视频编辑技术进行外观编辑。

Result: 大量实验表明，GA-Drive在NTA-IoU、NTL-IoU和FID评分方面显著优于现有方法，证明了其优越的性能。

Conclusion: GA-Drive通过创新的几何-外观解耦框架，成功实现了高质量、可编辑的自由视角驾驶模拟，为自动驾驶系统的训练和评估提供了有效的解决方案。

Abstract: A free-viewpoint, editable, and high-fidelity driving simulator is crucial for training and evaluating end-to-end autonomous driving systems. In this paper, we present GA-Drive, a novel simulation framework capable of generating camera views along user-specified novel trajectories through Geometry-Appearance Decoupling and Diffusion-Based Generation. Given a set of images captured along a recorded trajectory and the corresponding scene geometry, GA-Drive synthesizes novel pseudo-views using geometry information. These pseudo-views are then transformed into photorealistic views using a trained video diffusion model. In this way, we decouple the geometry and appearance of scenes. An advantage of such decoupling is its support for appearance editing via state-of-the-art video-to-video editing techniques, while preserving the underlying geometry, enabling consistent edits across both original and novel trajectories. Extensive experiments demonstrate that GA-Drive substantially outperforms existing methods in terms of NTA-IoU, NTL-IoU, and FID scores.

</details>


### [50] [RAYNOVA: 3D-Geometry-Free Auto-Regressive Driving World Modeling with Unified Spatio-Temporal Representation](https://arxiv.org/abs/2602.20685)
*Yichen Xie,Chensheng Peng,Mazen Abdelfattah,Yihan Hu,Jiezhi Yang,Eric Higgins,Ryan Brigden,Masayoshi Tomizuka,Wei Zhan*

Main category: cs.CV

TL;DR: RAYNOVA是一个几何无关的世界模型，采用双因果自回归框架，通过全局注意力实现统一的4D时空推理，无需强3D几何先验即可实现多视角视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法分别处理空间和时间相关性，且依赖强3D几何先验，限制了在不同相机设置和运动条件下的泛化能力。

Method: 使用双因果自回归框架，遵循尺度级和时间拓扑顺序，基于相对Plücker射线位置编码构建各向同性时空表示，引入循环训练范式缓解长时视频生成的分布漂移。

Result: 在nuScenes数据集上实现了最先进的多视角视频生成效果，具有更高的吞吐量和强可控性，能够泛化到新视角和相机配置。

Conclusion: RAYNOVA通过几何无关的方法实现了鲁棒的4D时空推理，为世界建模提供了新的解决方案，无需显式3D场景表示即可处理多样化输入条件。

Abstract: World foundation models aim to simulate the evolution of the real world with physically plausible behavior. Unlike prior methods that handle spatial and temporal correlations separately, we propose RAYNOVA, a geometry-free world model that employs a dual-causal autoregressive framework. It follows both scale-wise and temporal topological orders in the autoregressive process, and leverages global attention for unified 4D spatio-temporal reasoning. Different from existing works that impose strong 3D geometric priors, RAYNOVA constructs an isotropic spatio-temporal representation across views, frames, and scales based on relative Plücker-ray positional encoding, enabling robust generalization to diverse camera setups and ego motions. We further introduce a recurrent training paradigm to alleviate distribution drift in long-horizon video generation. RAYNOVA achieves state-of-the-art multi-view video generation results on nuScenes, while offering higher throughput and strong controllability under diverse input conditions, generalizing to novel views and camera configurations without explicit 3D scene representation. Our code will be released at http://yichen928.github.io/raynova.

</details>


### [51] [MatchED: Crisp Edge Detection Using End-to-End, Matching-based Supervision](https://arxiv.org/abs/2602.20689)
*Bedrettin Cetinkaya,Sinan Kalkan,Emre Akbas*

Main category: cs.CV

TL;DR: LPP是一个轻量级即插即用的匹配监督模块，可附加到任何边缘检测模型上，通过端到端学习实现单像素宽度的清晰边缘检测，无需依赖传统的非可微分后处理算法。


<details>
  <summary>Details</summary>
Motivation: 现有清晰边缘检测方法依赖非可微分的后处理算法（如NMS和骨架细化），阻碍端到端优化，且所有方法都需要此类后处理才能获得满意结果。

Method: 提出LPP模块（仅约21K参数），在每个训练迭代中基于空间距离和置信度对预测边缘和真实边缘进行一对一匹配，确保训练与测试协议的一致性。

Result: 在四个流行数据集上的实验表明，LPP显著提升现有边缘检测模型性能：平均清晰度指标提升2-4倍，在强调清晰度的评估中ODS提升20-35%，OIS和AP也有类似增益，首次达到或超越标准后处理的SOTA性能。

Conclusion: LPP模块解决了清晰边缘检测中的端到端优化问题，通过可微分的匹配监督实现了无需传统后处理的单像素宽度边缘检测，为边缘检测领域提供了新的解决方案。

Abstract: Generating crisp, i.e., one-pixel-wide, edge maps remains one of the fundamental challenges in edge detection, affecting both traditional and learning-based methods. To obtain crisp edges, most existing approaches rely on two hand-crafted post-processing algorithms, Non-Maximum Suppression (NMS) and skeleton-based thinning, which are non-differentiable and hinder end-to-end optimization. Moreover, all existing crisp edge detection methods still depend on such post-processing to achieve satisfactory results. To address this limitation, we propose \MethodLPP, a lightweight, only $\sim$21K additional parameters, and plug-and-play matching-based supervision module that can be appended to any edge detection model for joint end-to-end learning of crisp edges. At each training iteration, \MethodLPP performs one-to-one matching between predicted and ground-truth edges based on spatial distance and confidence, ensuring consistency between training and testing protocols. Extensive experiments on four popular datasets demonstrate that integrating \MethodLPP substantially improves the performance of existing edge detection models. In particular, \MethodLPP increases the Average Crispness (AC) metric by up to 2--4$\times$ compared to baseline models. Under the crispness-emphasized evaluation (CEval), \MethodLPP further boosts baseline performance by up to 20--35\% in ODS and achieves similar gains in OIS and AP, achieving SOTA performance that matches or surpasses standard post-processing for the first time. Code is available at https://cvpr26-matched.github.io.

</details>


### [52] [NGL-Prompter: Training-Free Sewing Pattern Estimation from a Single Image](https://arxiv.org/abs/2602.20700)
*Anna Badalyan,Pratheba Selvaraju,Giorgio Becherini,Omid Taheri,Victoria Fernandez Abrevaya,Michael Black*

Main category: cs.CV

TL;DR: NGL-Prompter是一个无需训练的流程，通过提出NGL中间语言将服装描述转换为VLMs可理解的形式，从图像中提取结构化服装参数并映射到GarmentCode，实现了多层级服装的精确缝纫图案重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于合成数据微调VLMs，但难以泛化到真实图像，无法捕捉真实服装部件关联，且仅限于单层服装。观察到VLMs擅长自然语言描述但直接回归参数效果差。

Method: 提出NGL中间语言重构GarmentCode表示，开发NGL-Prompter流程查询大型VLMs提取结构化服装参数，确定性映射到有效GarmentCode。

Result: 在Dress4D、CloSe和5000张真实时尚图像数据集上达到SOTA几何指标性能，在人类和GPT感知评估中显著优于基线，能重建多层服装而基线仅限单层。

Conclusion: 无需昂贵模型训练即可实现精确缝纫图案重建，NGL-Prompter对真实图像（即使有遮挡部分）具有强泛化能力。

Abstract: Estimating sewing patterns from images is a practical approach for creating high-quality 3D garments. Due to the lack of real-world pattern-image paired data, prior approaches fine-tune large vision language models (VLMs) on synthetic garment datasets generated by randomly sampling from a parametric garment model GarmentCode. However, these methods often struggle to generalize to in-the-wild images, fail to capture real-world correlations between garment parts, and are typically restricted to single-layer outfits. In contrast, we observe that VLMs are effective at describing garments in natural language, yet perform poorly when asked to directly regress GarmentCode parameters from images. To bridge this gap, we propose NGL (Natural Garment Language), a novel intermediate language that restructures GarmentCode into a representation more understandable to language models. Leveraging this language, we introduce NGL-Prompter, a training-free pipeline that queries large VLMs to extract structured garment parameters, which are then deterministically mapped to valid GarmentCode. We evaluate our method on the Dress4D, CloSe and a newly collected dataset of approximately 5,000 in-the-wild fashion images. Our approach achieves state-of-the-art performance on standard geometry metrics and is strongly preferred in both human and GPT-based perceptual evaluations compared to existing baselines. Furthermore, NGL-prompter can recover multi-layer outfits whereas competing methods focus mostly on single-layer garments, highlighting its strong generalization to real-world images even with occluded parts. These results demonstrate that accurate sewing pattern reconstruction is possible without costly model training. Our code and data will be released for research use.

</details>


### [53] [Onboard-Targeted Segmentation of Straylight in Space Camera Sensors](https://arxiv.org/abs/2602.20709)
*Riccardo Gallon,Fabian Schiemenz,Alessandra Menicucci,Eberhard Gill*

Main category: cs.CV

TL;DR: 基于AI的语义分割方法用于空间相机杂散光故障检测，使用DeepLabV3+MobileNetV3架构，通过Flare7k++数据集预训练实现泛化，针对航天器资源受限硬件优化，并开发系统级性能评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决空间相机在太阳光照射下产生的杂散光效应分割问题，克服空间特定数据稀缺的挑战，实现航天器在轨实时故障检测。

Method: 采用DeepLabV3模型配合MobileNetV3骨干网络进行语义分割，利用公开数据集Flare7k++进行预训练以增强对不同耀斑纹理的泛化能力，针对航天器硬件资源约束进行模型优化。

Result: 开发了能够有效分割空间相机杂散光效应的AI模型，实现了在资源受限硬件上的部署，并建立了与导航管道的接口及系统级性能评估体系。

Conclusion: 该方法成功解决了空间相机杂散光分割的技术难题，为航天器在轨自主故障检测提供了可行的AI解决方案，具有实际工程应用价值。

Abstract: This study details an artificial intelligence (AI)-based methodology for the semantic segmentation of space camera faults. Specifically, we address the segmentation of straylight effects induced by solar presence around the camera's Field of View (FoV). Anomalous images are sourced from our published dataset. Our approach emphasizes generalization across diverse flare textures, leveraging pre-training on a public dataset (Flare7k++) including flares in various non-space contexts to mitigate the scarcity of realistic space-specific data. A DeepLabV3 model with MobileNetV3 backbone performs the segmentation task. The model design targets deployment in spacecraft resource-constrained hardware. Finally, based on a proposed interface between our model and the onboard navigation pipeline, we develop custom metrics to assess the model's performance in the system-level context.

</details>


### [54] [Monocular Endoscopic Tissue 3D Reconstruction with Multi-Level Geometry Regularization](https://arxiv.org/abs/2602.20718)
*Yangsen Chen,Hao Wang*

Main category: cs.CV

TL;DR: 提出基于3D高斯泼溅的变形内窥镜组织重建方法，通过SDF网格约束重建过程并加入局部刚性和全局非刚性约束，实现实时渲染和平滑表面重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在组织表面重建一致性方面存在挑战，而NeRF方法缺乏实时渲染能力，需要同时实现平滑可变形表面和实时渲染。

Method: 采用表面感知重建方法：首先使用符号距离场构建网格，然后用该网格约束高斯泼溅重建过程；加入局部刚性和全局非刚性约束来指导高斯变形，适应软组织的可变形特性。

Result: 定量和定性分析表明，该方法在纹理和几何重建质量方面表现优异，实现了快速渲染过程和平滑表面外观。

Conclusion: 基于3D高斯泼溅的新方法成功解决了内窥镜组织重建中的一致性和实时性问题，为机器人辅助手术提供了有效的组织重建解决方案。

Abstract: Reconstructing deformable endoscopic tissues is crucial for achieving robot-assisted surgery. However, 3D Gaussian Splatting-based approaches encounter challenges in achieving consistent tissue surface reconstruction, while existing NeRF-based methods lack real-time rendering capabilities. In pursuit of both smooth deformable surfaces and real-time rendering, we introduce a novel approach based on 3D Gaussian Splatting. Specifically, we introduce surface-aware reconstruction, initially employing a Sign Distance Field-based method to construct a mesh, subsequently utilizing this mesh to constrain the Gaussian Splatting reconstruction process. Furthermore, to ensure the generation of physically plausible deformations, we incorporate local rigidity and global non-rigidity restrictions to guide Gaussian deformation, tailored for the highly deformable nature of soft endoscopic tissue. Based on 3D Gaussian Splatting, our proposed method delivers a fast rendering process and smooth surface appearances. Quantitative and qualitative analysis against alternative methodologies shows that our approach achieves solid reconstruction quality in both textures and geometries.

</details>


### [55] [CleanStyle: Plug-and-Play Style Conditioning Purification for Text-to-Image Stylization](https://arxiv.org/abs/2602.20721)
*Xiaoman Feng,Mingkun Lei,Yang Wang,Dingwen Fu,Chi Zhang*

Main category: cs.CV

TL;DR: CleanStyle是一个即插即用的框架，通过SVD分解和动态抑制尾部成分来消除风格嵌入中的内容泄漏，同时提出SS-CFG方法利用被抑制成分构建风格感知的无条件输入，显著提升风格化质量和提示词对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的基于编码器的扩散模型风格迁移方法存在内容泄漏问题，即风格图像的语义元素会不期望地出现在输出中，损害提示词保真度和风格一致性。

Method: 1. 使用SVD分解分离风格嵌入的尾部成分；2. 提出CS-SVD方法，通过时间感知指数调度动态抑制尾部成分；3. 提出SS-CFG方法，利用被抑制的尾部成分构建风格感知的无条件输入。

Result: 实验表明CleanStyle显著减少了内容泄漏，提高了风格化质量和提示词对齐效果，适用于广泛的风格参考和提示词。

Conclusion: CleanStyle是一个轻量级、可解释的框架，无需重新训练即可无缝集成到现有的基于编码器的扩散模型中，有效解决了内容泄漏问题。

Abstract: Style transfer in diffusion models enables controllable visual generation by injecting the style of a reference image. However, recent encoder-based methods, while efficient and tuning-free, often suffer from content leakage, where semantic elements from the style image undesirably appear in the output, impairing prompt fidelity and stylistic consistency. In this work, we introduce CleanStyle, a plug-and-play framework that filters out content-related noise from the style embedding without retraining. Motivated by empirical analysis, we observe that such leakage predominantly stems from the tail components of the style embedding, which are isolated via Singular Value Decomposition (SVD). To address this, we propose CleanStyleSVD (CS-SVD), which dynamically suppresses tail components using a time-aware exponential schedule, providing clean, style-preserving conditional embeddings throughout the denoising process. Furthermore, we present Style-Specific Classifier-Free Guidance (SS-CFG), which reuses the suppressed tail components to construct style-aware unconditional inputs. Unlike conventional methods that use generic negative embeddings (e.g., zero vectors), SS-CFG introduces targeted negative signals that reflect style-specific but prompt-irrelevant visual elements. This enables the model to effectively suppress these distracting patterns during generation, thereby improving prompt fidelity and enhancing the overall visual quality of stylized outputs. Our approach is lightweight, interpretable, and can be seamlessly integrated into existing encoder-based diffusion models without retraining. Extensive experiments demonstrate that CleanStyle substantially reduces content leakage, improves stylization quality and improves prompt alignment across a wide range of style references and prompts.

</details>


### [56] [Bridging Physically Based Rendering and Diffusion Models with Stochastic Differential Equation](https://arxiv.org/abs/2602.20725)
*Junwei Shu,Wenjie Liu,Changgu Chen,Hantang Liu,Yang Li,Changbo Wang*

Main category: cs.CV

TL;DR: 本文提出了一种统一的随机公式，将蒙特卡洛渲染与基于扩散的生成建模相结合，通过物理路径追踪的随机微分方程表示，实现对扩散生成结果的物理基础控制。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本或图像条件下能生成逼真内容，但缺乏对底层物理着色和材质属性的显式控制；而基于物理的渲染(PBR)提供精细物理控制但缺乏提示驱动的灵活性。两种范式都遵循从噪声观察到清晰图像的共同演化过程。

Method: 1) 建立蒙特卡洛积分在中心极限定理下的通用随机微分方程(SDE)公式；2) 通过基于物理的路径追踪实例化，转换为物理基础的SDE表示；3) 从噪声方差角度系统分析路径追踪物理特性如何扩展到现有扩散模型。

Result: 在多个任务上的广泛实验表明，该方法能够对扩散生成结果施加物理基础的控制，涵盖渲染和材质编辑等任务。

Conclusion: 成功构建了连接蒙特卡洛渲染和扩散生成模型的统一随机框架，实现了物理精确控制与提示驱动灵活性的结合，为计算机图形学和计算机视觉的交叉领域提供了新思路。

Abstract: Diffusion-based image generators excel at producing realistic content from text or image conditions, but they offer only limited explicit control over low-level, physically grounded shading and material properties. In contrast, physically based rendering (PBR) offers fine-grained physical control but lacks prompt-driven flexibility. Although these two paradigms originate from distinct communities, both share a common evolution -- from noisy observations to clean images. In this paper, we propose a unified stochastic formulation that bridges Monte Carlo rendering and diffusion-based generative modeling. First, a general stochastic differential equation (SDE) formulation for Monte Carlo integration under the Central Limit Theorem is modeled. Through instantiation via physically based path tracing, we convert it into a physically grounded SDE representation. Moreover, we provide a systematic analysis of how the physical characteristics of path tracing can be extended to existing diffusion models from the perspective of noise variance. Extensive experiments across multiple tasks show that our method can exert physically grounded control over diffusion-generated results, covering tasks such as rendering and material editing.

</details>


### [57] [Communication-Inspired Tokenization for Structured Image Representations](https://arxiv.org/abs/2602.20731)
*Aram Davtyan,Yusuf Sahin,Yasaman Haghighi,Sebastian Stapf,Pablo Acuaviva,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: COMiT是一个受人类交流启发的离散视觉分词框架，通过迭代观察局部图像块和循环更新离散表示来构建结构化token序列，在固定token预算下实现对象级语义结构捕获。


<details>
  <summary>Details</summary>
Motivation: 现有图像分词方法主要针对重建和压缩优化，产生的token往往捕获局部纹理而非对象级语义结构。受人类增量式、组合性交流方式启发，需要开发能更好捕捉语义结构的视觉分词方法。

Method: 使用单一transformer模型实现端到端训练，通过迭代编码过程观察局部图像块并循环更新离散表示，结合流匹配重建损失和语义表示对齐损失进行训练。

Result: 实验表明语义对齐提供了基础，而注意力顺序分词对于诱导可解释的对象中心token结构至关重要，显著提高了组合泛化和关系推理能力。

Conclusion: COMiT框架通过模仿人类交流的增量组合特性，成功实现了结构化视觉token表示学习，在语义结构捕获和组合推理方面优于现有方法。

Abstract: Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.

</details>


### [58] [OrthoDiffusion: A Generalizable Multi-Task Diffusion Foundation Model for Musculoskeletal MRI Interpretation](https://arxiv.org/abs/2602.20752)
*Tian Lan,Lei Xu,Zimu Yuan,Shanggui Liu,Jiajun Liu,Jiaxin Liu,Weilai Xiang,Hongyu Yang,Dong Jiang,Jianxin Yin,Dingyu Wang*

Main category: cs.CV

TL;DR: OrthoDiffusion是一个基于扩散模型的统一框架，用于多任务肌肉骨骼MRI分析，在膝关节MRI的解剖分割和异常检测中表现出色，具有优秀的泛化能力和标签效率。


<details>
  <summary>Details</summary>
Motivation: 肌肉骨骼疾病是全球主要的致残原因，MRI诊断需要识别复杂解剖结构中的多个潜在异常，这一过程需要专业知识且存在变异性，因此需要开发自动化工具来提高诊断效率和准确性。

Method: 开发了OrthoDiffusion框架，使用三个方向特定的3D扩散模型，在15,948个未标记的膝关节MRI扫描上进行自监督预训练，学习矢状面、冠状面和轴状面的稳健解剖特征，并整合这些视图特定表示来支持解剖分割和多标签诊断。

Result: 模型在11个膝关节结构分割和8个膝关节异常检测中表现优异，在不同临床中心和MRI场强下展现出显著稳健性，仅使用10%训练标签时仍保持高诊断精度，且学习到的解剖表示可迁移到踝关节和肩关节的11种疾病诊断。

Conclusion: 基于扩散模型的基础模型可以作为多疾病诊断和解剖分割的统一平台，有望提高肌肉骨骼MRI解读在真实临床工作流程中的效率和准确性。

Abstract: Musculoskeletal disorders represent a significant global health burden and are a leading cause of disability worldwide. While MRI is essential for accurate diagnosis, its interpretation remains exceptionally challenging. Radiologists must identify multiple potential abnormalities within complex anatomical structures across different imaging planes, a process that requires significant expertise and is prone to variability. We developed OrthoDiffusion, a unified diffusion-based foundation model designed for multi-task musculoskeletal MRI interpretation. The framework utilizes three orientation-specific 3D diffusion models, pre-trained in a self-supervised manner on 15,948 unlabeled knee MRI scans, to learn robust anatomical features from sagittal, coronal, and axial views. These view-specific representations are integrated to support diverse clinical tasks, including anatomical segmentation and multi-label diagnosis. Our evaluation demonstrates that OrthoDiffusion achieves excellent performance in the segmentation of 11 knee structures and the detection of 8 knee abnormalities. The model exhibited remarkable robustness across different clinical centers and MRI field strengths, consistently outperforming traditional supervised models. Notably, in settings where labeled data was scarce, OrthoDiffusion maintained high diagnostic precision using only 10\% of training labels. Furthermore, the anatomical representations learned from knee imaging proved highly transferable to other joints, achieving strong diagnostic performance across 11 diseases of the ankle and shoulder. These findings suggest that diffusion-based foundation models can serve as a unified platform for multi-disease diagnosis and anatomical segmentation, potentially improving the efficiency and accuracy of musculoskeletal MRI interpretation in real-world clinical workflows.

</details>


### [59] [Federated Learning for Cross-Modality Medical Image Segmentation via Augmentation-Driven Generalization](https://arxiv.org/abs/2602.20773)
*Sachin Dudda Nagaraju,Ashkan Moradi,Bendik Skarre Abrahamsen,Mattijs Elschot*

Main category: cs.CV

TL;DR: 本研究探讨了在联邦学习框架下通过数据增强策略解决跨模态医学图像分割问题的有效方法，特别是在各医疗机构仅拥有单一模态数据（CT或MRI）的现实场景中。


<details>
  <summary>Details</summary>
Motivation: 医学影像分析中，由于数据隐私和机构间数据孤岛问题，开发鲁棒的跨模态分割模型面临挑战。现有方法需要配对的多模态数据或复杂架构，在临床实践中不实用。

Method: 系统评估了多种数据增强策略：基于卷积的空间增强、频域操作、域特定归一化和全局强度非线性（GIN）增强，使用腹部器官分割和全心分割作为基准测试。

Result: GIN增强在中心化和联邦学习设置中均表现最佳，能有效模拟跨模态外观变化同时保持解剖结构。胰腺分割Dice分数从0.073提升至0.437（提升498%），联邦学习方法达到中心化训练精度的93-98%。

Conclusion: GIN增强策略在联邦学习框架下实现了强大的跨模态泛化能力，不损害数据隐私，为在不同医疗系统中部署可行的联邦AI解决方案提供了有效途径。

Abstract: Artificial intelligence has emerged as a transformative tool in medical image analysis, yet developing robust and generalizable segmentation models remains difficult due to fragmented, privacy-constrained imaging data siloed across institutions. While federated learning (FL) enables collaborative model training without centralizing data, cross-modality domain shifts pose a critical challenge, particularly when models trained on one modality fail to generalize to another. Many existing solutions require paired multimodal data per patient or rely on complex architectures, both of which are impractical in real clinical settings. In this work, we consider a realistic FL scenario where each client holds single-modality data (CT or MRI), and systematically investigate augmentation strategies for cross-modality generalization. Using abdominal organ segmentation and whole-heart segmentation as representative multi-class and binary segmentation benchmarks, we evaluate convolution-based spatial augmentation, frequency-domain manipulation, domain-specific normalization, and global intensity nonlinear (GIN) augmentation. Our results show that GIN consistently outperforms alternatives in both centralized and federated settings by simulating cross-modality appearance variations while preserving anatomical structure. For the pancreas, Dice score improved from 0.073 to 0.437, a 498% gain. Our federated approach achieves 93-98% of centralized training accuracy, demonstrating strong cross-modality generalization without compromising data privacy, pointing toward feasible federated AI deployment across diverse healthcare systems.

</details>


### [60] [Real-time Motion Segmentation with Event-based Normal Flow](https://arxiv.org/abs/2602.20790)
*Sheng Zhong,Zhongyang Ren,Xiya Zhu,Dehao Yuan,Cornelia Fermuller,Yi Zhou*

Main category: cs.CV

TL;DR: 提出基于法向光流的运动分割框架，通过图割能量最小化方法处理事件相机数据，实现实时性能并比现有方法快800倍


<details>
  <summary>Details</summary>
Motivation: 事件相机具有微秒级分辨率但数据稀疏，直接处理原始事件数据效率低下，限制了在实时运动分割等任务中的应用。法向光流作为中间表示能更有效地压缩运动信息

Method: 利用从事件邻域学习的密集法向光流作为输入，将运动分割建模为图割能量最小化问题，通过法向光流聚类和运动模型拟合进行迭代优化，使用基于法向光流的运动模型初始化和拟合方法

Result: 在多个公共数据集上的广泛评估证明了框架的准确性和效率，实现了近800倍的速度提升

Conclusion: 该方法通过法向光流表示和图割优化，显著降低了计算复杂度，确保了实时性能，为事件相机的动态场景理解提供了有效解决方案

Abstract: Event-based cameras are bio-inspired sensors with pixels that independently and asynchronously respond to brightness changes at microsecond resolution, offering the potential to handle visual tasks in challenging scenarios. However, due to the sparse information content in individual events, directly processing the raw event data to solve vision tasks is highly inefficient, which severely limits the applicability of state-of-the-art methods in real-time tasks, such as motion segmentation, a fundamental task for dynamic scene understanding. Incorporating normal flow as an intermediate representation to compress motion information from event clusters within a localized region provides a more effective solution. In this work, we propose a normal flow-based motion segmentation framework for event-based vision. Leveraging the dense normal flow directly learned from event neighborhoods as input, we formulate the motion segmentation task as an energy minimization problem solved via graph cuts, and optimize it iteratively with normal flow clustering and motion model fitting. By using a normal flow-based motion model initialization and fitting method, the proposed system is able to efficiently estimate the motion models of independently moving objects with only a limited number of candidate models, which significantly reduces the computational complexity and ensures real-time performance, achieving nearly a 800x speedup in comparison to the open-source state-of-the-art method. Extensive evaluations on multiple public datasets fully demonstrate the accuracy and efficiency of our framework.

</details>


### [61] [SIMSPINE: A Biomechanics-Aware Simulation Framework for 3D Spine Motion Annotation and Benchmarking](https://arxiv.org/abs/2602.20792)
*Muhammad Saif Ullah Khan,Didier Stricker*

Main category: cs.CV

TL;DR: SIMSPINE：首个大规模3D脊柱运动数据集，通过生物力学感知的关键点模拟框架增强现有姿态数据集，提供210万帧无约束自然运动的椎骨级3D标注，并建立了脊柱运动估计的统一基准。


<details>
  <summary>Details</summary>
Motivation: 脊柱运动建模对人类生物力学研究至关重要，但由于脊柱复杂的多关节运动学和缺乏大规模3D标注，在计算机视觉领域尚未得到充分探索。

Method: 提出生物力学感知的关键点模拟框架，从肌肉骨骼建模推导解剖学一致的3D脊柱关键点，增强现有人体姿态数据集。创建包含2.14百万帧的SIMSPINE数据集，提供室内多相机捕获的无约束自然全身运动的稀疏椎骨级3D标注。

Result: 2D脊柱基线在受控环境中将最先进水平从0.63 AUC提升至0.80 AUC，在野外脊柱跟踪中从0.91 AP提升至0.93 AP。发布了预训练基线模型，包括微调的2D检测器、单目3D姿态提升模型和多视角重建流程。

Conclusion: SIMSPINE数据集和模拟框架通过实现自然条件下可重复、解剖学基础的3D脊柱估计，推动了基于视觉的生物力学、运动分析和数字人体建模研究，弥合了肌肉骨骼模拟与计算机视觉之间的差距。

Abstract: Modeling spinal motion is fundamental to understanding human biomechanics, yet remains underexplored in computer vision due to the spine's complex multi-joint kinematics and the lack of large-scale 3D annotations. We present a biomechanics-aware keypoint simulation framework that augments existing human pose datasets with anatomically consistent 3D spinal keypoints derived from musculoskeletal modeling. Using this framework, we create the first open dataset, named SIMSPINE, which provides sparse vertebra-level 3D spinal annotations for natural full-body motions in indoor multi-camera capture without external restraints. With 2.14 million frames, this enables data-driven learning of vertebral kinematics from subtle posture variations and bridges the gap between musculoskeletal simulation and computer vision. In addition, we release pretrained baselines covering fine-tuned 2D detectors, monocular 3D pose lifting models, and multi-view reconstruction pipelines, establishing a unified benchmark for biomechanically valid spine motion estimation. Specifically, our 2D spine baselines improve the state-of-the-art from 0.63 to 0.80 AUC in controlled environments, and from 0.91 to 0.93 AP for in-the-wild spine tracking. Together, the simulation framework and SIMSPINE dataset advance research in vision-based biomechanics, motion analysis, and digital human modeling by enabling reproducible, anatomically grounded 3D spine estimation under natural conditions.

</details>


### [62] [VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving](https://arxiv.org/abs/2602.20794)
*Jie Wang,Guang Li,Zhijian Huang,Chenxu Dang,Hangjun Ye,Yahong Han,Long Chen*

Main category: cs.CV

TL;DR: VGGDrive是一个新颖的架构，通过跨视图几何赋能器(CVGE)将成熟的3D基础模型的几何特征注入到视觉语言模型中，解决了现有VLM在自动驾驶场景中缺乏跨视图3D几何建模能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型(VLMs)缺乏跨视图3D几何建模能力，导致在自动驾驶任务中表现平庸，虽然有些方法尝试通过构建问答数据进行辅助训练，但仍无法从根本上解决这一问题。

Method: 提出VGGDrive架构，包含一个即插即用的跨视图3D几何赋能器(CVGE)，通过分层自适应注入机制，将冻结的3D视觉模型的跨视图几何特征与VLM的2D视觉特征进行桥接。

Result: 在五个自动驾驶基准测试中，VGGDrive显著提升了基础VLM的性能，包括跨视图风险感知、运动预测和轨迹规划等任务。

Conclusion: 成熟的3D基础模型可以通过有效集成来赋能自动驾驶任务，这项工作展示了这一范式在自动驾驶领域的巨大潜力。

Abstract: The significance of cross-view 3D geometric modeling capabilities for autonomous driving is self-evident, yet existing Vision-Language Models (VLMs) inherently lack this capability, resulting in their mediocre performance. While some promising approaches attempt to mitigate this by constructing Q&A data for auxiliary training, they still fail to fundamentally equip VLMs with the ability to comprehensively handle diverse evaluation protocols. We thus chart a new course, advocating for the infusion of VLMs with the cross-view geometric grounding of mature 3D foundation models, closing this critical capability gap in autonomous driving. In this spirit, we propose a novel architecture, VGGDrive, which empowers Vision-language models with cross-view Geometric Grounding for autonomous Driving. Concretely, to bridge the cross-view 3D geometric features from the frozen visual 3D model with the VLM's 2D visual features, we introduce a plug-and-play Cross-View 3D Geometric Enabler (CVGE). The CVGE decouples the base VLM architecture and effectively empowers the VLM with 3D features through a hierarchical adaptive injection mechanism. Extensive experiments show that VGGDrive enhances base VLM performance across five autonomous driving benchmarks, including tasks like cross-view risk perception, motion prediction, and trajectory planning. It's our belief that mature 3D foundation models can empower autonomous driving tasks through effective integration, and we hope our initial exploration demonstrates the potential of this paradigm to the autonomous driving community.

</details>


### [63] [RU4D-SLAM: Reweighting Uncertainty in Gaussian Splatting SLAM for 4D Scene Reconstruction](https://arxiv.org/abs/2602.20807)
*Yangfan Zhao,Hanwei Zhang,Ke Huang,Qiufeng Wang,Zhenzhou Shao,Dengyu Wu*

Main category: cs.CV

TL;DR: RU4D-SLAM是一个将4D高斯溅射与SLAM结合的鲁棒框架，通过引入时间因素、不确定性感知和运动模糊渲染，在动态环境中显著提升了轨迹精度和4D场景重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射SLAM方法在动态环境中表现不佳，特别是移动物体干扰3D重建和跟踪可靠性。4D重建技术为解决这些问题提供了新方向，但其在4D感知SLAM中的应用尚未充分探索。

Method: 提出RU4D-SLAM框架：1）将时间因素融入空间3D表示；2）集成不确定性感知的场景变化感知和模糊图像合成；3）引入运动模糊渲染增强动态场景表示；4）扩展每像素不确定性建模处理模糊图像；5）提出语义引导的重加权机制；6）引入可学习不透明度权重支持自适应4D映射。

Result: 在标准基准测试中，该方法在轨迹精度和4D场景重建方面显著优于现有最先进方法，特别是在包含移动物体的动态环境和低质量输入条件下表现突出。

Conclusion: RU4D-SLAM成功地将4D高斯溅射与SLAM相结合，通过不确定性感知和动态场景处理机制，为动态环境下的连续3D重建和可靠跟踪提供了有效的解决方案。

Abstract: Combining 3D Gaussian splatting with Simultaneous Localization and Mapping (SLAM) has gained popularity as it enables continuous 3D environment reconstruction during motion. However, existing methods struggle in dynamic environments, particularly moving objects complicate 3D reconstruction and, in turn, hinder reliable tracking. The emergence of 4D reconstruction, especially 4D Gaussian splatting, offers a promising direction for addressing these challenges, yet its potential for 4D-aware SLAM remains largely underexplored. Along this direction, we propose a robust and efficient framework, namely Reweighting Uncertainty in Gaussian Splatting SLAM (RU4D-SLAM) for 4D scene reconstruction, that introduces temporal factors into spatial 3D representation while incorporating uncertainty-aware perception of scene changes, blurred image synthesis, and dynamic scene reconstruction. We enhance dynamic scene representation by integrating motion blur rendering, and improve uncertainty-aware tracking by extending per-pixel uncertainty modeling, which is originally designed for static scenarios, to handle blurred images. Furthermore, we propose a semantic-guided reweighting mechanism for per-pixel uncertainty estimation in dynamic scenes, and introduce a learnable opacity weight to support adaptive 4D mapping. Extensive experiments on standard benchmarks demonstrate that our method substantially outperforms state-of-the-art approaches in both trajectory accuracy and 4D scene reconstruction, particularly in dynamic environments with moving objects and low-quality inputs. Code available: https://ru4d-slam.github.io

</details>


### [64] [GatedCLIP: Gated Multimodal Fusion for Hateful Memes Detection](https://arxiv.org/abs/2602.20818)
*Yingying Guo,Ke Zhang,Zirong Zeng*

Main category: cs.CV

TL;DR: GatedCLIP是一种改进的视觉-语言模型，通过专门架构增强CLIP的多模态能力，在仇恨表情包检测任务上显著优于CLIP基线，AUROC达到0.66。


<details>
  <summary>Details</summary>
Motivation: 多模态表情包中的仇恨内容检测面临独特挑战，因为有害信息往往来自良性图像和文本的复杂交互作用，需要更有效的多模态理解模型。

Method: 引入学习投影头将CLIP嵌入映射到任务优化的语义空间，采用动态门控融合机制自适应加权视觉和文本特征，并使用对比学习目标保持跨模态语义对齐。

Result: 在Hateful Memes数据集上，GatedCLIP达到AUROC 0.66，显著优于CLIP基线（AUROC 0.49），仅使用35万个可训练参数保持计算效率。

Conclusion: GatedCLIP通过专门的架构改进有效提升了多模态仇恨内容检测性能，证明了在保持效率的同时显著提升CLIP模型在特定任务上表现的可能性。

Abstract: Detecting hateful content in multimodal memes presents unique challenges, as harmful messages often emerge from the complex interplay between benign images and text. We propose GatedCLIP, a Vision-Language model that enhances CLIP's multimodal capabilities with specialized architectural improvements for hateful memes detection. Our approach introduces learned projection heads that map CLIP embeddings to a task-optimized semantic space, a dynamic gated fusion mechanism that adaptively weights visual and textual features, and a contrastive learning objective that maintains cross-modal semantic alignment. Experiments on the Hateful Memes dataset demonstrate that GatedCLIP achieves an AUROC of 0.66, substantially outperforming the CLIP baseline (AUROC 0.49) while maintaining computational efficiency with only 350K trainable parameters.

</details>


### [65] [Training-Free Multi-Concept Image Editing](https://arxiv.org/abs/2602.20839)
*Niki Foteinopoulou,Ignas Budvytis,Stephan Liwicki*

Main category: cs.CV

TL;DR: 提出无需训练的基于概念的图像编辑框架，结合优化DDS与LoRA驱动的概念组合，通过预训练概念适配器实现文本语义引导与低层视觉线索的整合，在保持稳定性和可控性的同时提升编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的零-shot文本编辑方法难以保持图像身份特征或表达语言无法完全描述的视觉概念（如面部结构、材质纹理、物体几何等），需要解决这一局限性。

Method: 统一优化DDS与LoRA驱动的概念组合框架，利用LoRA的训练数据表示概念；通过有序时间步、正则化和负提示引导改进DDS的稳定性和可控性；整合文本语义引导和预训练概念适配器的低层线索。

Result: 在InstructPix2Pix和ComposLoRA基准测试中，定量和定性结果均显示相比现有无需训练扩散编辑方法有持续改进。

Conclusion: 该框架成功实现了无需训练的基于概念的图像编辑，能够有效结合和控制多个视觉概念，为扩散模型编辑提供了更精确和稳定的解决方案。

Abstract: Editing images with diffusion models without training remains challenging. While recent optimisation-based methods achieve strong zero-shot edits from text, they struggle to preserve identity or capture details that language alone cannot express. Many visual concepts such as facial structure, material texture, or object geometry are impossible to express purely through text prompts alone. To address this gap, we introduce a training-free framework for concept-based image editing, which unifies Optimised DDS with LoRA-driven concept composition, where the training data of the LoRA represent the concept. Our approach enables combining and controlling multiple visual concepts directly within the diffusion process, integrating semantic guidance from text with low-level cues from pretrained concept adapters. We further refine DDS for stability and controllability through ordered timesteps, regularisation, and negative-prompt guidance. Quantitative and qualitative results demonstrate consistent improvements over existing training-free diffusion editing methods on InstructPix2Pix and ComposLoRA benchmarks. Code will be made publicly available.

</details>


### [66] [FLIM Networks with Bag of Feature Points](https://arxiv.org/abs/2602.20845)
*João Deltregia Martinelli,Marcelo Luis Rodrigues Filho,Felipe Crispim da Rocha Salvagnini,Gilson Junior Soares,Jefersson A. dos Santos,Alexandre X. Falcão*

Main category: cs.CV

TL;DR: FLIM-BoFP是一种无需反向传播的快速卷积滤波器估计方法，通过在输入块执行单次聚类创建特征点包，显著提升FLIM网络的训练效率，并在寄生虫检测任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统卷积网络需要大量图像标注，成本高昂且耗时。FLIM方法通过用户标记的判别性区域估计滤波器权重，但之前的FLIM-Cluster方法存在计算开销大和滤波器位置控制不足的问题。

Method: 提出FLIM-BoFP方法：在输入块执行单次聚类创建特征点包，然后直接基于映射到所有块的特征点定义滤波器，避免了逐块聚类的计算开销。

Result: 相比FLIM-Cluster，FLIM-BoFP在效率、效果和泛化能力方面都有显著提升，特别是在光学显微镜图像寄生虫检测任务中表现出色。

Conclusion: FLIM-BoFP提供了一种更快速、高效的滤波器估计方法，为无需反向传播的卷积网络训练开辟了新途径，在计算效率和模型控制方面都有重要改进。

Abstract: Convolutional networks require extensive image annotation, which can be costly and time-consuming. Feature Learning from Image Markers (FLIM) tackles this challenge by estimating encoder filters (i.e., kernel weights) from user-drawn markers on discriminative regions of a few representative images without traditional optimization. Such an encoder combined with an adaptive decoder comprises a FLIM network fully trained without backpropagation. Prior research has demonstrated their effectiveness in Salient Object Detection (SOD), being significantly lighter than existing lightweight models. This study revisits FLIM SOD and introduces FLIM-Bag of Feature Points (FLIM-BoFP), a considerably faster filter estimation method. The previous approach, FLIM-Cluster, derives filters through patch clustering at each encoder's block, leading to computational overhead and reduced control over filter locations. FLIM-BoFP streamlines this process by performing a single clustering at the input block, creating a bag of feature points, and defining filters directly from mapped feature points across all blocks. The paper evaluates the benefits in efficiency, effectiveness, and generalization of FLIM-BoFP compared to FLIM-Cluster and other state-of-the-art baselines for parasite detection in optical microscopy images.

</details>


### [67] [Hybrid Fusion: One-Minute Efficient Training for Zero-Shot Cross-Domain Image Fusion](https://arxiv.org/abs/2602.20851)
*Ran Zhang,Xuanhua He,Liu Liu*

Main category: cs.CV

TL;DR: 提出一种新颖的混合图像融合框架，结合学习型U-Net生成动态引导图和固定拉普拉斯金字塔融合核，实现高效全分辨率训练，在保持SOTA性能的同时极大提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统图像融合方法速度快但性能有限，深度学习方法性能优越但存在训练推理差距大、计算资源消耗高的问题，需要解决这一效率与性能的权衡问题。

Method: 使用可学习的U-Net生成动态引导图来指导固定的拉普拉斯金字塔融合核，将策略学习与像素合成解耦，实现全分辨率高效训练。

Result: 在RTX 4090上约1分钟、消费级笔记本GPU上约2分钟即可达到与SOTA相当的性能，无需外部模型，并在红外-可见光到医学成像等多种任务中表现出强大的零样本泛化能力。

Conclusion: 该方法成功解决了图像融合中效率与性能的权衡问题，通过混合框架实现了高效全分辨率训练，输出结果完全基于源信息线性构建，确保了关键应用中的高保真度。

Abstract: Image fusion seeks to integrate complementary information from multiple sources into a single, superior image. While traditional methods are fast, they lack adaptability and performance. Conversely, deep learning approaches achieve state-of-the-art (SOTA) results but suffer from critical inefficiencies: their reliance on slow, resource-intensive, patch-based training introduces a significant gap with full-resolution inference. We propose a novel hybrid framework that resolves this trade-off. Our method utilizes a learnable U-Net to generate a dynamic guidance map that directs a classic, fixed Laplacian pyramid fusion kernel. This decoupling of policy learning from pixel synthesis enables remarkably efficient full-resolution training, eliminating the train-inference gap. Consequently, our model achieves SOTA-comparable performance in about one minute on a RTX 4090 or two minutes on a consumer laptop GPU from scratch without any external model and demonstrates powerful zero-shot generalization across diverse tasks, from infrared-visible to medical imaging. By design, the fused output is linearly constructed solely from source information, ensuring high faithfulness for critical applications. The codes are available at https://github.com/Zirconium233/HybridFusion

</details>


### [68] [On the Explainability of Vision-Language Models in Art History](https://arxiv.org/abs/2602.20853)
*Stefanie Schneider*

Main category: cs.CV

TL;DR: 本研究评估了7种XAI方法在艺术史背景下解释CLIP视觉语言模型视觉推理的能力，通过零样本定位实验和人类可解释性研究发现这些方法的效果取决于概念稳定性和表征可用性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型将视觉和文本数据映射到共享嵌入空间，引发了关于机器'理解'本质的关键问题，需要研究如何通过可解释AI方法在艺术史背景下使VLMs的视觉推理过程可读。

Method: 结合零样本定位实验和人类可解释性研究，评估了7种不同的XAI方法在CLIP模型上的表现。

Result: 研究结果表明，虽然这些XAI方法能够捕捉人类解释的某些方面，但其有效性高度依赖于所考察类别的概念稳定性和表征可用性。

Conclusion: 在艺术史等复杂领域中，XAI方法对VLMs的解释能力存在局限性，需要进一步考虑概念特征和表征质量对解释效果的影响。

Abstract: Vision-Language Models (VLMs) transfer visual and textual data into a shared embedding space. In so doing, they enable a wide range of multimodal tasks, while also raising critical questions about the nature of machine 'understanding.' In this paper, we examine how Explainable Artificial Intelligence (XAI) methods can render the visual reasoning of a VLM - namely, CLIP - legible in art-historical contexts. To this end, we evaluate seven methods, combining zero-shot localization experiments with human interpretability studies. Our results indicate that, while these methods capture some aspects of human interpretation, their effectiveness hinges on the conceptual stability and representational availability of the examined categories.

</details>


### [69] [DA-Cal: Towards Cross-Domain Calibration in Semantic Segmentation](https://arxiv.org/abs/2602.20860)
*Wangkai Li,Rui Sun,Zhaoyang Li,Yujia Chen,Tianzhu Zhang*

Main category: cs.CV

TL;DR: DA-Cal是一个专门的无监督域自适应校准框架，通过元温度网络生成像素级校准参数，将目标域校准转化为软伪标签优化问题，显著提升语义分割中的域自适应性能和校准质量。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应方法在语义分割中忽视了网络校准质量，导致预测置信度与实际准确度不匹配，这在安全关键应用中存在重大风险。研究发现，在跨域场景中，由于校准不佳，使用软伪标签替代硬伪标签会导致性能显著下降。

Method: 提出DA-Cal框架：1）引入元温度网络生成像素级校准参数；2）采用双层优化建立软伪标签与UDA监督之间的关系；3）利用互补的域混合策略防止过拟合并减少域差异。

Result: 实验表明DA-Cal能无缝集成到现有的自训练框架中，在多个UDA分割基准测试中显著改善目标域校准，同时带来性能提升且无推理开销。

Conclusion: DA-Cal通过将目标域校准转化为软伪标签优化问题，有效解决了跨域语义分割中的校准问题，为安全关键应用提供了更可靠的域自适应解决方案。

Abstract: While existing unsupervised domain adaptation (UDA) methods greatly enhance target domain performance in semantic segmentation, they often neglect network calibration quality, resulting in misalignment between prediction confidence and actual accuracy -- a significant risk in safety-critical applications. Our key insight emerges from observing that performance degrades substantially when soft pseudo-labels replace hard pseudo-labels in cross-domain scenarios due to poor calibration, despite the theoretical equivalence of perfectly calibrated soft pseudo-labels to hard pseudo-labels. Based on this finding, we propose DA-Cal, a dedicated cross-domain calibration framework that transforms target domain calibration into soft pseudo-label optimization. DA-Cal introduces a Meta Temperature Network to generate pixel-level calibration parameters and employs bi-level optimization to establish the relationship between soft pseudo-labels and UDA supervision, while utilizing complementary domain-mixing strategies to prevent overfitting and reduce domain discrepancies. Experiments demonstrate that DA-Cal seamlessly integrates with existing self-training frameworks across multiple UDA segmentation benchmarks, significantly improving target domain calibration while delivering performance gains without inference overhead. The code will be released.

</details>


### [70] [MUSE: Harnessing Precise and Diverse Semantics for Few-Shot Whole Slide Image Classification](https://arxiv.org/abs/2602.20873)
*Jiahao Xu,Sheng Huang,Xin Zhang,Zhixiong Nan,Jiajun Dong,Nankun Mu*

Main category: cs.CV

TL;DR: MUSE是一个用于计算病理学中少样本全切片图像分类的新框架，通过样本级别的语义细化和检索增强的多视角生成来解决现有方法语义对齐精度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言的方法将大模型生成的文本语义视为静态的类别级别先验，缺乏样本级别的细化，限制了视觉语义对齐的多样性和精度，在有限监督下泛化能力受限。

Method: 提出随机多视角语义增强框架MUSE：1) SFSE模块通过MoE机制实现样本级别的细粒度语义先验生成；2) SMMO模块构建LLM生成的知识库，在训练中检索并随机整合多个匹配的文本视角作为丰富的语义监督。

Result: 在三个基准WSI数据集上的实验表明，MUSE在少样本设置下持续优于现有的视觉语言基线方法。

Conclusion: 有效的少样本病理学习不仅需要更丰富的语义来源，还需要主动和样本感知的语义优化策略。

Abstract: In computational pathology, few-shot whole slide image classification is primarily driven by the extreme scarcity of expert-labeled slides. Recent vision-language methods incorporate textual semantics generated by large language models, but treat these descriptions as static class-level priors that are shared across all samples and lack sample-wise refinement. This limits both the diversity and precision of visual-semantic alignment, hindering generalization under limited supervision. To overcome this, we propose the stochastic MUlti-view Semantic Enhancement (MUSE), a framework that first refines semantic precision via sample-wise adaptation and then enhances semantic richness through retrieval-augmented multi-view generation. Specifically, MUSE introduces Sample-wise Fine-grained Semantic Enhancement (SFSE), which yields a fine-grained semantic prior for each sample through MoE-based adaptive visual-semantic interaction. Guided by this prior, Stochastic Multi-view Model Optimization (SMMO) constructs an LLM-generated knowledge base of diverse pathological descriptions per class, then retrieves and stochastically integrates multiple matched textual views during training. These dynamically selected texts serve as enriched semantic supervisions to stochastically optimize the vision-language model, promoting robustness and mitigating overfitting. Experiments on three benchmark WSI datasets show that MUSE consistently outperforms existing vision-language baselines in few-shot settings, demonstrating that effective few-shot pathology learning requires not only richer semantic sources but also their active and sample-aware semantic optimization. Our code is available at: https://github.com/JiahaoXu-god/CVPR2026_MUSE.

</details>


### [71] [When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance](https://arxiv.org/abs/2602.20880)
*Yongli Xiang,Ziming Hong,Zhaoqing Wang,Xiangyu Zhao,Bo Han,Tongliang Liu*

Main category: cs.CV

TL;DR: CASG是一个无需训练的文本到图像扩散模型安全引导框架，通过动态识别与生成状态最相关的有害类别并应用针对性安全引导，解决多类别有害内容间的冲突问题，显著降低有害生成率。


<details>
  <summary>Details</summary>
Motivation: 现有基于安全引导的方法在处理多类别有害内容时存在"有害冲突"问题，即抑制一种有害类型可能无意中放大另一种有害类型，导致总体有害率增加。

Method: 提出冲突感知自适应安全引导(CASG)框架，包含两个组件：冲突感知类别识别(CaCI)动态识别与生成状态最对齐的有害类别；冲突解决引导应用(CrGA)仅沿识别出的类别应用安全引导以避免多类别干扰。

Result: 在T2I安全基准测试中，CASG达到最先进性能，相比现有方法将有害率降低高达15.4%。

Conclusion: CASG有效解决了多类别有害内容生成中的冲突问题，提供了一种更精确和有效的安全引导方法，可应用于潜在空间和文本空间的安全保障。

Abstract: Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to "harmful conflicts" where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model's evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG's state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.

</details>


### [72] [SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models](https://arxiv.org/abs/2602.20901)
*Yuechen Xie,Xiaoyan Zhang,Yicheng Shan,Hao Zhu,Rui Tang,Rong Wei,Mingli Song,Yuanyu Wan,Jie Song*

Main category: cs.CV

TL;DR: 本文提出了SpatiaLQA基准测试来评估视觉语言模型的空间逻辑推理能力，通过递归场景图辅助推理方法显著提升了模型在复杂空间逻辑推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在常见视觉问答和逻辑推理任务中表现出色，但在复杂现实环境中缺乏合理的空间逻辑推理能力，需要同时理解复杂场景中物体的空间关系和多步骤任务中的逻辑依赖关系。

Method: 提出递归场景图辅助推理方法，利用视觉基础模型将复杂场景逐步分解为任务相关的场景图，从而增强视觉语言模型的空间逻辑推理能力。

Result: 在41个主流视觉语言模型上的实验表明，即使最先进的模型在空间逻辑推理方面仍然存在困难，而提出的方法优于所有先前的方法。

Conclusion: SpatiaLQA基准测试填补了视觉语言模型在空间逻辑推理能力评估方面的空白，递归场景图辅助推理方法有效提升了模型性能，为复杂现实环境中的应用提供了重要支撑。

Abstract: Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects in complex scenes, but also the logical dependencies between steps in multi-step tasks. To bridge this gap, we introduce Spatial Logical Question Answering (SpatiaLQA), a benchmark designed to evaluate the spatial logical reasoning capabilities of VLMs. SpatiaLQA consists of 9,605 question answer pairs derived from 241 real-world indoor scenes. We conduct extensive experiments on 41 mainstream VLMs, and the results show that even the most advanced models still struggle with spatial logical reasoning. To address this issue, we propose a method called recursive scene graph assisted reasoning, which leverages visual foundation models to progressively decompose complex scenes into task-relevant scene graphs, thereby enhancing the spatial logical reasoning ability of VLMs, outperforming all previous methods. Code and dataset are available at https://github.com/xieyc99/SpatiaLQA.

</details>


### [73] [TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering](https://arxiv.org/abs/2602.20903)
*Hanshen Zhu,Yuliang Liu,Xuecheng Wu,An-Lan Wang,Hao Feng,Dingkang Yang,Chao Feng,Can Huang,Jingqun Tang,Xiang Bai*

Main category: cs.CV

TL;DR: TextPecker是一个即插即用的结构异常感知强化学习策略，用于改进文本到图像生成中的视觉文本渲染质量，通过字符级结构异常标注和笔画编辑合成引擎来解决现有模型无法感知文本结构异常的问题。


<details>
  <summary>Details</summary>
Motivation: 当前先进的文本到图像生成模型在视觉文本渲染(VTR)中存在结构异常问题(如扭曲、模糊、错位)，而主流多模态大模型和OCR模型无法有效感知这些结构异常，这成为VTR评估和基于强化学习的优化的关键瓶颈。

Method: 提出了TextPecker方法，包括：1)构建具有字符级结构异常标注的识别数据集；2)开发笔画编辑合成引擎以扩展结构错误覆盖范围；3)设计即插即用的结构异常感知强化学习策略，可适用于任何文本到图像生成器。

Result: 实验表明TextPecker能持续改进各种文本到图像模型，在优化良好的Qwen-Image模型上，中文文本渲染的结构保真度平均提升4%，语义对齐度提升8.7%，在高保真VTR方面建立了新的最先进水平。

Conclusion: TextPecker填补了VTR优化领域的空白，为可靠且结构保真的视觉文本生成提供了基础性步骤，解决了现有方法无法感知文本结构异常的关键问题。

Abstract: Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.

</details>


### [74] [LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding](https://arxiv.org/abs/2602.20913)
*Jihao Qiu,Lingxi Xie,Xinyue Huo,Qi Tian,Qixiang Ye*

Main category: cs.CV

TL;DR: LongVideo-R1是一个主动推理的多模态大语言模型代理，通过层次化视频导航和智能停止机制，在低计算预算下实现高效长视频理解，在准确性和效率之间取得优越平衡。


<details>
  <summary>Details</summary>
Motivation: 解决长视频理解中计算成本高的问题，避免冗余的穷举搜索，需要在有限计算预算下实现高效视频内容分析。

Method: 采用推理模块利用高层视觉线索推断信息量最大的视频片段，从顶层视觉摘要开始迭代细化关注点，使用两阶段训练范式（监督微调+强化学习）和专门设计的奖励函数。

Result: 在多个长视频基准测试中验证了有效性，在QA准确性和效率之间实现了优越的权衡。

Conclusion: LongVideo-R1通过主动推理和智能导航机制，为低计算预算下的长视频理解提供了有效解决方案，具有实际应用价值。

Abstract: This paper addresses the critical and underexplored challenge of long video understanding with low computational budgets. We propose LongVideo-R1, an active, reasoning-equipped multimodal large language model (MLLM) agent designed for efficient video context navigation, avoiding the redundancy of exhaustive search. At the core of LongVideo-R1 lies a reasoning module that leverages high-level visual cues to infer the most informative video clip for subsequent processing. During inference, the agent initiates traversal from top-level visual summaries and iteratively refines its focus, immediately halting the exploration process upon acquiring sufficient knowledge to answer the query. To facilitate training, we first extract hierarchical video captions from CGBench, a video corpus with grounding annotations, and guide GPT-5 to generate 33K high-quality chain-of-thought-with-tool trajectories. The LongVideo-R1 agent is fine-tuned upon the Qwen-3-8B model through a two-stage paradigm: supervised fine-tuning (SFT) followed by reinforcement learning (RL), where RL employs a specifically designed reward function to maximize selective and efficient clip navigation. Experiments on multiple long video benchmarks validate the effectiveness of name, which enjoys superior tradeoff between QA accuracy and efficiency. All curated data and source code are provided in the supplementary material and will be made publicly available. Code and data are available at: https://github.com/qiujihao19/LongVideo-R1

</details>


### [75] [Computing a Characteristic Orientation for Rotation-Independent Image Analysis](https://arxiv.org/abs/2602.20930)
*Cristian Valero-Abundio,Emilio Sansano-Sansano,Raúl Montoliu,Marina Martínez García*

Main category: cs.CV

TL;DR: 提出了一种名为GID的预处理方法，通过估计图像全局方向并进行对齐来提高标准神经网络对旋转的鲁棒性，无需修改网络架构，在MNIST和CIFAR-10数据集上表现优于现有旋转不变架构。


<details>
  <summary>Details</summary>
Motivation: 深度学习在处理几何变换特别是旋转时面临挑战，现有方法如数据增强或架构修改会增加计算成本、需要专门实现或改变网络结构，限制了应用范围。

Method: GID预处理方法：估计每张图像的全局方向，将其对齐到规范参考框架，保持空间结构的同时使标准模型能更一致地处理不同旋转的输入，与卷积网络兼容。

Result: 在旋转MNIST数据集上达到比最先进旋转不变架构更高的准确率；在CIFAR-10数据集上验证了该方法在更复杂条件下的有效性。

Conclusion: GID方法提供了一种有效的预处理解决方案，能够显著提升神经网络对旋转变换的鲁棒性，同时保持网络架构不变，具有良好的通用性和实用性。

Abstract: Handling geometric transformations, particularly rotations, remains a challenge in deep learning for computer vision. Standard neural networks lack inherent rotation invariance and typically rely on data augmentation or architectural modifications to improve robustness. Although effective, these approaches increase computational demands, require specialised implementations, or alter network structures, limiting their applicability. This paper introduces General Intensity Direction (GID), a preprocessing method that improves rotation robustness without modifying the network architecture. The method estimates a global orientation for each image and aligns it to a canonical reference frame, allowing standard models to process inputs more consistently across different rotations. Unlike moment-based approaches that extract invariant descriptors, this method directly transforms the image while preserving spatial structure, making it compatible with convolutional networks. Experimental evaluation on the rotated MNIST dataset shows that the proposed method achieves higher accuracy than state-of-the-art rotation-invariant architectures. Additional experiments on the CIFAR-10 dataset, confirm that the method remains effective under more complex conditions.

</details>


### [76] [Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting](https://arxiv.org/abs/2602.20933)
*Shuangkang Fang,I-Chao Shen,Xuanyang Zhang,Zesheng Wang,Yufeng Wang,Wenrui Ding,Gang Yu,Takeo Igarashi*

Main category: cs.CV

TL;DR: DropAnSH-GS是一种新颖的基于锚点的3D高斯泼溅Dropout方法，通过同时移除锚点高斯的空间邻居来打破局部冗余，并扩展Dropout到高阶球谐系数以减轻过拟合，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS Dropout方法存在邻居补偿效应（被丢弃的高斯常被邻居补偿，削弱正则化效果），且忽略了高阶球谐系数对过拟合的贡献。

Method: 提出锚点Dropout策略：随机选择高斯作为锚点并同时移除其空间邻居；扩展Dropout到颜色属性，随机丢弃高阶球谐系数，将外观信息集中到低阶系数。

Result: 实验结果表明DropAnSH-GS显著优于现有Dropout方法，计算开销可忽略，并能轻松集成到各种3DGS变体中提升性能。

Conclusion: 该方法有效解决了3DGS在稀疏视图下的过拟合问题，通过破坏局部冗余和学习更鲁棒的全局表示，同时支持通过球谐截断进行灵活的模型压缩。

Abstract: Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting. To address these issues, we propose DropAnSH-GS, a novel anchor-based Dropout strategy. Rather than dropping Gaussians independently, our method randomly selects certain Gaussians as anchors and simultaneously removes their spatial neighbors. This effectively disrupts local redundancies near anchors and encourages the model to learn more robust, globally informed representations. Furthermore, we extend the Dropout to color attributes by randomly dropping higher-degree SH to concentrate appearance information in lower-degree SH. This strategy further mitigates overfitting and enables flexible post-training model compression via SH truncation. Experimental results demonstrate that DropAnSH-GS substantially outperforms existing Dropout methods with negligible computational overhead, and can be readily integrated into various 3DGS variants to enhance their performances. Project Website: https://sk-fun.fun/DropAnSH-GS

</details>


### [77] [UFO: Unifying Feed-Forward and Optimization-based Methods for Large Driving Scene Modeling](https://arxiv.org/abs/2602.20943)
*Kaiyuan Tan,Yingying Shen,Mingfei Tu,Haohui Zhu,Bing Wang,Guang Chen,Hangjun Ye,Haiyang Sun*

Main category: cs.CV

TL;DR: UFO是一种新颖的循环范式，结合了基于优化和前馈方法的优势，用于高效的长距离4D场景重建，特别针对自动驾驶模拟中的动态驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有前馈方法在长距离驾驶序列中因二次复杂度和动态物体建模困难而表现不佳的问题。

Method: 采用循环范式维护4D场景表示，使用基于可见性的过滤机制选择信息丰富的场景token，并引入物体姿态引导的建模方法来准确捕获长距离运动。

Result: 在Waymo Open Dataset上的实验表明，该方法在各种序列长度下显著优于逐场景优化和现有前馈方法，能够在0.5秒内重建16秒的驾驶日志，同时保持卓越的视觉质量和几何精度。

Conclusion: UFO方法为长距离4D重建提供了高效的解决方案，结合了优化和前馈方法的优势，在计算效率和重建质量方面都有显著提升。

Abstract: Dynamic driving scene reconstruction is critical for autonomous driving simulation and closed-loop learning. While recent feed-forward methods have shown promise for 3D reconstruction, they struggle with long-range driving sequences due to quadratic complexity in sequence length and challenges in modeling dynamic objects over extended durations. We propose UFO, a novel recurrent paradigm that combines the benefits of optimization-based and feed-forward methods for efficient long-range 4D reconstruction. Our approach maintains a 4D scene representation that is iteratively refined as new observations arrive, using a visibility-based filtering mechanism to select informative scene tokens and enable efficient processing of long sequences. For dynamic objects, we introduce an object pose-guided modeling approach that supports accurate long-range motion capture. Experiments on the Waymo Open Dataset demonstrate that our method significantly outperforms both per-scene optimization and existing feed-forward methods across various sequence lengths. Notably, our approach can reconstruct 16-second driving logs within 0.5 second while maintaining superior visual quality and geometric accuracy.

</details>


### [78] [See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis](https://arxiv.org/abs/2602.20951)
*Jaehyun Park,Minyoung Ahn,Minkyu Kim,Jonghyun Lee,Jae-Gil Lee,Dongmin Park*

Main category: cs.CV

TL;DR: ArtiAgent是一个自动化系统，通过三个智能体（感知、合成、策展）生成包含丰富伪影标注的图像数据集，无需人工标注，有效解决了AI生成图像中的视觉伪影问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成图像常包含视觉伪影影响真实感，现有方法依赖昂贵的人工标注数据集，需要自动化方法来可靠获取伪影标注数据。

Method: 提出三智能体框架：感知智能体识别和定位图像实体，合成智能体通过扩散变换器中的补丁嵌入操作注入伪影，策展智能体过滤合成伪影并生成局部和全局解释。

Result: 成功合成10万张带有丰富伪影标注的图像，在不同应用中展现出高效性和多功能性。

Conclusion: ArtiAgent提供了一种可扩展的自动化解决方案，为伪影缓解研究提供了高质量数据集，代码已开源。

Abstract: Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.

</details>


### [79] [Are Multimodal Large Language Models Good Annotators for Image Tagging?](https://arxiv.org/abs/2602.20972)
*Ming-Kun Xie,Jia-Hao Xiao,Zhiqiang Kou,Zhongnian Li,Gang Niu,Masashi Sugiyama*

Main category: cs.CV

TL;DR: TagLLM框架通过结构化分组提示和交互式标签消歧，显著缩小了MLLM生成标注与人工标注之间的差距，将下游任务性能差距缩小了60-80%，同时将标注成本降至人工成本的千分之一。


<details>
  <summary>Details</summary>
Motivation: 传统图像标注依赖人工标注成本高昂，而多模态大语言模型(MLLM)的自动标注能力尚未充分探索。研究发现MLLM标注质量可达人工的50-80%，但存在明显差距，需要开发有效解决方案。

Method: 提出TagLLM框架，包含两个核心组件：1)候选生成-使用结构化分组提示高效生成紧凑候选标签集；2)标签消歧-通过交互式校准提示中的语义概念并有效精炼候选标签。

Result: 实验表明TagLLM显著缩小了MLLM与人工标注的差距，在下游训练性能方面缩小了约60-80%的差异，同时标注成本仅为人工成本的千分之一。

Conclusion: TagLLM证明了MLLM在图像标注任务中替代人工标注的可行性，通过高效的候选生成和精确的标签消歧机制，实现了接近人工标注质量的低成本自动化解决方案。

Abstract: Image tagging, a fundamental vision task, traditionally relies on human-annotated datasets to train multi-label classifiers, which incurs significant labor and costs. While Multimodal Large Language Models (MLLMs) offer promising potential to automate annotation, their capability to replace human annotators remains underexplored. This paper aims to analyze the gap between MLLM-generated and human annotations and to propose an effective solution that enables MLLM-based annotation to replace manual labeling. Our analysis of MLLM annotations reveals that, under a conservative estimate, MLLMs can reduce annotation cost to as low as one-thousandth of the human cost, mainly accounting for GPU usage, which is nearly negligible compared to manual efforts. Their annotation quality reaches about 50\% to 80\% of human performance, while achieving over 90\% performance on downstream training tasks.Motivated by these findings, we propose TagLLM, a novel framework for image tagging, which aims to narrow the gap between MLLM-generated and human annotations. TagLLM comprises two components: Candidates generation, which employs structured group-wise prompting to efficiently produce a compact candidate set that covers as many true labels as possible while reducing subsequent annotation workload; and label disambiguation, which interactively calibrates the semantic concept of categories in the prompts and effectively refines the candidate labels. Extensive experiments show that TagLLM substantially narrows the gap between MLLM-generated and human annotations, especially in downstream training performance, where it closes about 60\% to 80\% of the difference.

</details>


### [80] [Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models](https://arxiv.org/abs/2602.20981)
*Christian Simon,MAsato Ishii,Wei-Yao Wang,Koichi Saito,Akio Hayakawa,Dongseok Shim,Zhi Zhong,Shuyang Cui,Shusuke Takahashi,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: MMHNet是一种多模态分层网络，通过整合分层方法和非因果Mamba架构，解决了视频到音频生成长时音频的挑战，能够在短实例训练下泛化到长时测试，实现超过5分钟的高质量音频生成。


<details>
  <summary>Details</summary>
Motivation: 解决多模态视频到音频生成中的缩放挑战，特别是数据有限以及文本描述与帧级视频信息不匹配的问题，探索在短实例训练下能否泛化到长时音频生成。

Method: 提出多模态分层网络MMHNet，整合分层方法和非因果Mamba架构，支持长时音频生成，无需在长时数据上训练即可实现短训长测。

Result: 方法显著改善了长达5分钟以上的音频生成，在长视频到音频基准测试中取得优异结果，超越了先前工作，证明了短训长测在视频到音频生成任务中的可行性。

Conclusion: MMHNet成功解决了视频到音频生成长时内容的挑战，为多模态对齐提供了有效的解决方案，展示了在有限数据下实现长时生成的能力。

Abstract: Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.

</details>


### [81] [VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models](https://arxiv.org/abs/2602.20999)
*Bowen Zheng,Yongli Xiang,Ziming Hong,Zerong Lin,Chaojian Yu,Tongliang Liu,Xinge You*

Main category: cs.CV

TL;DR: VII是一种无需训练、可迁移的越狱框架，通过将恶意文本指令伪装成参考图像中的良性视觉指令，成功攻击商业图像到视频生成模型，攻击成功率高达83.5%。


<details>
  <summary>Details</summary>
Motivation: 发现图像到视频生成模型中视觉指令跟随能力的潜在安全风险，攻击者可能通过图像模态注入恶意意图，而现有研究对此风险关注不足。

Method: VII框架包含恶意意图重编程模块（从恶意文本中提取意图并降低其静态危害性）和视觉指令接地模块（将意图以语义一致的视觉指令形式嵌入安全图像中）。

Result: 在四个先进商业I2V模型上的实验显示，VII攻击成功率最高达83.5%，拒绝率降至接近零，显著优于现有基线方法。

Conclusion: 视觉指令注入攻击揭示了I2V模型的安全漏洞，需要开发更强大的防御机制来应对这种新型多模态攻击。

Abstract: Image-to-Video (I2V) generation models, which condition video generation on reference images, have shown emerging visual instruction-following capability, allowing certain visual cues in reference images to act as implicit control signals for video generation. However, this capability also introduces a previously overlooked risk: adversaries may exploit visual instructions to inject malicious intent through the image modality. In this work, we uncover this risk by proposing Visual Instruction Injection (VII), a training-free and transferable jailbreaking framework that intentionally disguises the malicious intent of unsafe text prompts as benign visual instructions in the safe reference image. Specifically, VII coordinates a Malicious Intent Reprogramming module to distill malicious intent from unsafe text prompts while minimizing their static harmfulness, and a Visual Instruction Grounding module to ground the distilled intent onto a safe input image by rendering visual instructions that preserve semantic consistency with the original unsafe text prompt, thereby inducing harmful content during I2V generation. Empirically, our extensive experiments on four state-of-the-art commercial I2V models (Kling-v2.5-turbo, Gemini Veo-3.1, Seedance-1.5-pro, and PixVerse-V5) demonstrate that VII achieves Attack Success Rates of up to 83.5% while reducing Refusal Rates to near zero, significantly outperforming existing baselines.

</details>


### [82] [Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design](https://arxiv.org/abs/2602.21010)
*Jiannan Huang,Aditya Kane,Fengzhe Zhou,Yunchao Wei,Humphrey Shi*

Main category: cs.CV

TL;DR: Le-DETR：一种低预训练成本的高效实时目标检测Transformer，通过改进backbone架构和混合编码器设计，在COCO数据集上达到SOTA性能，比现有方法节省80%预训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前实时DETR模型由于backbone预训练开销过大而难以从零复现，限制了新backbone架构的探索。研究目标是证明通过良好设计可以在低预训练成本下实现高性能。

Method: 1）对backbone架构进行深入研究，提出不同规模的EfficientNAT，融合高效卷积和局部注意力机制；2）重新设计带有局部注意力的混合编码器，提升性能和推理速度；3）仅使用ImageNet1K和COCO2017数据集进行训练。

Result: Le-DETR-M/L/X在COCO Val2017上分别达到52.9/54.3/55.1 mAP，RTX4090上推理时间为4.45/5.01/6.68 ms。相比YOLOv12-L/X提升+0.6/-0.1 mAP且速度提升20%，相比DEIM-D-FINE也有明显优势。

Conclusion: 通过精心设计，实时DETR模型无需复杂昂贵的预训练即可实现强大性能，预训练阶段节省约80%图像数据，为实时目标检测研究提供了更易复现和探索的解决方案。

Abstract: Real-time object detection is crucial for real-world applications as it requires high accuracy with low latency. While Detection Transformers (DETR) have demonstrated significant performance improvements, current real-time DETR models are challenging to reproduce from scratch due to excessive pre-training overheads on the backbone, constraining research advancements by hindering the exploration of novel backbone architectures. In this paper, we want to show that by using general good design, it is possible to have \textbf{high performance} with \textbf{low pre-training cost}. After a thorough study of the backbone architecture, we propose EfficientNAT at various scales, which incorporates modern efficient convolution and local attention mechanisms. Moreover, we re-design the hybrid encoder with local attention, significantly enhancing both performance and inference speed. Based on these advancements, we present Le-DETR (\textbf{L}ow-cost and \textbf{E}fficient \textbf{DE}tection \textbf{TR}ansformer), which achieves a new \textbf{SOTA} in real-time detection using only ImageNet1K and COCO2017 training datasets, saving about 80\% images in pre-training stage compared with previous methods. We demonstrate that with well-designed, real-time DETR models can achieve strong performance without the need for complex and computationally expensive pretraining. Extensive experiments show that Le-DETR-M/L/X achieves \textbf{52.9/54.3/55.1 mAP} on COCO Val2017 with \textbf{4.45/5.01/6.68 ms} on an RTX4090. It surpasses YOLOv12-L/X by \textbf{+0.6/-0.1 mAP} while achieving similar speed and \textbf{+20\%} speedup. Compared with DEIM-D-FINE, Le-DETR-M achieves \textbf{+0.2 mAP} with slightly faster inference, and surpasses DEIM-D-FINE-L by \textbf{+0.4 mAP} with only \textbf{0.4 ms} additional latency. Code and weights will be open-sourced.

</details>


### [83] [From Perception to Action: An Interactive Benchmark for Vision Reasoning](https://arxiv.org/abs/2602.21015)
*Yuhao Wu,Maojia Song,Yihuai Lan,Lei Wang,Zhiqiang Hu,Yao Xiao,Heng Zhou,Weihua Zheng,Dylan Raharja,Soujanya Poria,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: CHAIN基准测试：一个交互式3D物理驱动测试平台，用于评估模型在物理约束下理解和执行结构化动作序列的能力，从被动感知转向主动问题解决。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型评估主要关注结构无关的单轮设置（如VQA），无法评估智能体理解几何、接触和支持关系如何共同约束动态环境中可能动作的能力。

Method: 开发CHAIN基准测试，包含互锁机械拼图和3D堆叠包装等任务，在统一交互设置下对最先进的视觉语言模型和基于扩散的模型进行全面研究。

Result: 顶级模型仍然难以内化物理结构和因果约束，经常无法产生可靠的长时程计划，也不能稳健地将感知到的结构转化为有效动作。

Conclusion: 需要开发能够更好地理解和推理物理结构约束的模型，CHAIN为评估这类能力提供了重要测试平台。

Abstract: Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.

</details>


### [84] [MIP Candy: A Modular PyTorch Framework for Medical Image Processing](https://arxiv.org/abs/2602.21033)
*Tianhao Fu,Yucheng Chen*

Main category: cs.CV

TL;DR: MIPCandy是一个基于PyTorch的模块化医学图像处理框架，提供从数据加载到评估的完整流程，通过单一build_network方法实现全功能工作流，支持运行时模块替换和丰富的内置功能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像处理框架要么需要大量集成工作，要么采用僵化的整体流程难以修改，需要一种既能提供完整流程又能保持灵活控制的解决方案。

Method: 采用模块化设计，核心是LayerT延迟配置机制实现运行时模块替换，提供k折交叉验证、自动ROI检测、深度监督、EMA、多前端实验跟踪、训练状态恢复和商回归验证分数预测等功能。

Result: 开发出完整的开源框架MIPCandy，支持Python 3.12+，提供可扩展的预构建模型生态系统，遵循一致的trainer-predictor模式。

Conclusion: MIPCandy成功解决了医学图像处理中灵活性与完整性的平衡问题，为研究者提供了即用且可高度定制的框架，有望促进医学图像分析研究的发展。

Abstract: Medical image processing demands specialized software that handles high-dimensional volumetric data, heterogeneous file formats, and domain-specific training procedures. Existing frameworks either provide low-level components that require substantial integration effort or impose rigid, monolithic pipelines that resist modification. We present MIP Candy (MIPCandy), a freely available, PyTorch-based framework designed specifically for medical image processing. MIPCandy provides a complete, modular pipeline spanning data loading, training, inference, and evaluation, allowing researchers to obtain a fully functional process workflow by implementing a single method, $\texttt{build_network}$, while retaining fine-grained control over every component. Central to the design is $\texttt{LayerT}$, a deferred configuration mechanism that enables runtime substitution of convolution, normalization, and activation modules without subclassing. The framework further offers built-in $k$-fold cross-validation, dataset inspection with automatic region-of-interest detection, deep supervision, exponential moving average, multi-frontend experiment tracking (Weights & Biases, Notion, MLflow), training state recovery, and validation score prediction via quotient regression. An extensible bundle ecosystem provides pre-built model implementations that follow a consistent trainer--predictor pattern and integrate with the core framework without modification. MIPCandy is open-source under the Apache-2.0 license and requires Python~3.12 or later. Source code and documentation are available at https://github.com/ProjectNeura/MIPCandy.

</details>


### [85] [Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning](https://arxiv.org/abs/2602.21035)
*Junhao Xiao,Zhiyu Wu,Hao Lin,Yi Chen,Yahui Liu,Xiaoran Zhao,Zixu Wang,Zejiang He*

Main category: cs.CV

TL;DR: CLIPGlasses是一个即插即用框架，通过双阶段设计（Lens模块解耦否定语义，Frame模块预测上下文感知排斥强度）来增强CLIP模型对否定视觉描述的理解能力，在跨域泛化方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）难以理解否定概念，经常将肯定和否定描述嵌入到相似的表示空间中（例如将"没有狗"与狗图像匹配），现有微调方法存在过拟合风险。

Method: 提出CLIPGlasses框架：1) Lens模块从文本嵌入中解耦否定语义；2) Frame模块预测上下文感知的排斥强度；3) 修改相似度计算，通过惩罚与否定语义的对齐来减少误匹配。

Result: 实验表明，配备CLIPGlasses的CLIP在域内性能具有竞争力，在跨域泛化方面优于最先进方法，在低资源条件下表现尤其突出，显示出更强的跨域鲁棒性。

Conclusion: CLIPGlasses有效解决了CLIP模型理解否定语义的难题，通过创新的双阶段设计实现了更好的泛化能力和鲁棒性，特别是在资源受限场景下表现优异。

Abstract: Vision-Language Models (VLMs) like CLIP struggle to understand negation, often embedding affirmatives and negatives similarly (e.g., matching "no dog" with dog images). Existing methods refine negation understanding via fine-tuning CLIP's text encoder, risking overfitting. In this work, we propose CLIPGlasses, a plug-and-play framework that enhances CLIP's ability to comprehend negated visual descriptions. CLIPGlasses adopts a dual-stage design: a Lens module disentangles negated semantics from text embeddings, and a Frame module predicts context-aware repulsion strength, which is integrated into a modified similarity computation to penalize alignment with negated semantics, thereby reducing false positive matches. Experiments show that CLIP equipped with CLIPGlasses achieves competitive in-domain performance and outperforms state-of-the-art methods in cross-domain generalization. Its superiority is especially evident under low-resource conditions, indicating stronger robustness across domains.

</details>


### [86] [OmniOCR: Generalist OCR for Ethnic Minority Languages](https://arxiv.org/abs/2602.21042)
*Bonan Liu,Zeyu Zhang,Bingbing Meng,Han Wang,Hanshuo Zhang,Chengping Wang,Daji Ergu,Ying Cai*

Main category: cs.CV

TL;DR: OmniOCR是一个针对少数民族文字OCR的通用框架，采用动态LoRA和稀疏正则化技术，在四个少数民族文字数据集上相比现有基线模型准确率提升39%-66%，且不增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前OCR技术主要关注拉丁文和中文等资源丰富的文字，少数民族文字由于书写系统复杂、标注稀缺、古今形式多样等问题，在低资源和零样本场景下泛化能力不足。

Method: 提出动态低秩适应（Dynamic LoRA）技术，在模型层间和文字间分配容量；引入稀疏正则化修剪冗余更新，实现紧凑高效的适配。

Result: 在TibetanMNIST、Shui、古彝文和东巴文四个数据集上，OmniOCR在零样本和标准后训练场景下均优于基础模型，达到最先进准确率且参数效率优越。

Conclusion: OmniOCR通过动态LoRA和稀疏正则化有效解决了少数民族文字OCR的低资源挑战，为多文字OCR系统提供了通用高效的解决方案。

Abstract: Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.

</details>


### [87] [OCR-Agent: Agentic OCR with Capability and Memory Reflection](https://arxiv.org/abs/2602.21053)
*Shimin Wen,Zeyu Zhang,Xingdou Bian,Hongjie Zhu,Lulu He,Layi Shama,Daji Ergu,Ying Cai*

Main category: cs.CV

TL;DR: 提出了一个新颖的迭代自校正框架OCR-Agent，通过能力反思和记忆反思两大机制，使大视觉语言模型具备自我诊断错误和避免重复尝试的能力，在OCRBench v2基准测试中达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型缺乏有效的自校正机制，在多轮修正过程中容易陷入重复无效尝试，无法稳定提升答案质量，需要解决认知偏差和重复尝试的问题。

Method: 提出包含能力反思和记忆反思的迭代自校正框架：能力反思用于诊断错误并制定修正计划；记忆反思用于回顾过往尝试以避免重复并探索新方案；最后通过严格重新推理优化答案。

Result: 在OCRBench v2基准测试中，OCR-Agent在英文子集上比当前开源SOTA模型InternVL3-8B高出+2.0分，中文子集高出+1.2分，在视觉理解(79.9)和推理(66.5)方面达到最先进水平，甚至超过了更大的微调模型。

Conclusion: 结构化、自感知的反思机制能够显著增强视觉语言模型的推理鲁棒性，且无需额外训练，为模型自我优化提供了有效途径。

Abstract: Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.

</details>


### [88] [Optimizing Occupancy Sensor Placement in Smart Environments](https://arxiv.org/abs/2602.21098)
*Hao Lu,Richard J. Radke*

Main category: cs.CV

TL;DR: 提出一种基于整数线性规划和分支定界法的自动传感器布局方法，用于优化商业建筑中低分辨率ToF传感器的放置位置，以实时准确识别区域人数同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 商业建筑中需要实时准确识别人员位置以实现按需照明和温控节能，但现有低分辨率ToF传感器的性能高度依赖于传感器布局，需要自动化优化方法。

Method: 基于办公室几何约束模拟大量人员轨迹，将传感器布局问题建模为整数线性规划问题，采用分支定界法求解最优传感器布局方案。

Result: 通过多个不同办公室环境的仿真验证了该方法的有效性，能够为给定数量的传感器确定最优布局并预测计数精度。

Conclusion: 该方法能够自动生成最优传感器布局，解决了低分辨率ToF传感器网络在区域人数统计中依赖人工布置的问题，为实现建筑节能提供了有效技术支撑。

Abstract: Understanding the locations of occupants in a commercial built environment is critical for realizing energy savings by delivering lighting, heating, and cooling only where it is needed. The key to achieving this goal is being able to recognize zone occupancy in real time, without impeding occupants' activities or compromising privacy. While low-resolution, privacy-preserving time-of-flight (ToF) sensor networks have demonstrated good performance in zone counting, the performance depends on careful sensor placement. To address this issue, we propose an automatic sensor placement method that determines optimal sensor layouts for a given number of sensors, and can predict the counting accuracy of such a layout. In particular, given the geometric constraints of an office environment, we simulate a large number of occupant trajectories. We then formulate the sensor placement problem as an integer linear programming (ILP) problem and solve it with the branch and bound method. We demonstrate the effectiveness of the proposed method based on simulations of several different office environments.

</details>


### [89] [Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction](https://arxiv.org/abs/2602.21100)
*Noé Artru,Rukhshanda Hussain,Emeline Got,Alexandre Messier,David B. Lindell,Abdallah Dib*

Main category: cs.CV

TL;DR: 提出了一种结合基础模型和优化方法的混合方法，通过多视角表面法线预测和逆向渲染优化，实现从少量图像重建高质量3D头部几何，在保持高保真度的同时显著减少相机需求与计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有3D头部重建方法存在根本性限制：传统摄影测量需要大量相机视角和计算资源，而单图像基础模型缺乏几何细节，优化方法则需要密集视角。需要一种能兼顾效率与精度的解决方案。

Method: 1) 开发多视角表面法线预测模型，通过跨视角注意力扩展单目基础模型，前向传播生成几何一致的法线；2) 将这些预测作为强几何先验，在逆向渲染优化框架中恢复高频表面细节。

Result: 方法在单图像和多视角重建任务上均优于现有最先进方法，达到了与密集视角摄影测量相当的高保真重建质量，同时显著减少了相机需求（从25-200+减少到少量视角）和计算成本。

Conclusion: 该混合方法成功桥接了基础模型的高效性和优化方法的高保真度之间的差距，为高质量3D头部重建提供了一种实用且高效的解决方案，代码和模型将开源。

Abstract: Reconstructing high-fidelity 3D head geometry from images is critical for a wide range of applications, yet existing methods face fundamental limitations. Traditional photogrammetry achieves exceptional detail but requires extensive camera arrays (25-200+ views), substantial computation, and manual cleanup in challenging areas like facial hair. Recent alternatives present a fundamental trade-off: foundation models enable efficient single-image reconstruction but lack fine geometric detail, while optimization-based methods achieve higher fidelity but require dense views and expensive computation. We bridge this gap with a hybrid approach that combines the strengths of both paradigms. Our method introduces a multi-view surface normal prediction model that extends monocular foundation models with cross-view attention to produce geometrically consistent normals in a feed-forward pass. We then leverage these predictions as strong geometric priors within an inverse rendering optimization framework to recover high-frequency surface details. Our approach outperforms state-of-the-art single-image and multi-view methods, achieving high-fidelity reconstruction on par with dense-view photogrammetry while reducing camera requirements and computational cost. The code and model will be released.

</details>


### [90] [Event-Aided Sharp Radiance Field Reconstruction for Fast-Flying Drones](https://arxiv.org/abs/2602.21101)
*Rong Zou,Marco Cannici,Davide Scaramuzza*

Main category: cs.CV

TL;DR: 提出一种融合事件相机和运动模糊图像的统一框架，用于从高速无人机飞行中重建高质量神经辐射场(NeRF)，解决了高速飞行导致的运动模糊和位姿估计漂移问题。


<details>
  <summary>Details</summary>
Motivation: 高速飞行的无人机在基础设施检查、地形勘探和搜救等领域有重要应用，但高速飞行会导致图像严重运动模糊和位姿估计噪声，传统NeRF方法对此类退化非常敏感，难以实现密集3D重建。

Method: 开发了统一框架，将事件流与运动模糊帧融合嵌入到NeRF优化中，联合使用事件和帧模态精炼基于事件的视觉-惯性里程计先验，无需地面真值监督。

Result: 在合成数据和真实世界高速无人机序列上验证，即使在高度动态飞行条件下，RGB帧严重退化且位姿先验不可靠时，仍能重建高保真辐射场并保留精细场景细节，在真实数据上相比最先进方法性能提升超过50%。

Conclusion: 该方法成功解决了高速无人机应用中NeRF重建的关键挑战，通过事件-图像融合和联合优化策略，实现了在恶劣条件下的高质量3D重建，为快速飞行机器人的实际应用提供了有效解决方案。

Abstract: Fast-flying aerial robots promise rapid inspection under limited battery constraints, with direct applications in infrastructure inspection, terrain exploration, and search and rescue. However, high speeds lead to severe motion blur in images and induce significant drift and noise in pose estimates, making dense 3D reconstruction with Neural Radiance Fields (NeRFs) particularly challenging due to their high sensitivity to such degradations. In this work, we present a unified framework that leverages asynchronous event streams alongside motion-blurred frames to reconstruct high-fidelity radiance fields from agile drone flights. By embedding event-image fusion into NeRF optimization and jointly refining event-based visual-inertial odometry priors using both event and frame modalities, our method recovers sharp radiance fields and accurate camera trajectories without ground-truth supervision. We validate our approach on both synthetic data and real-world sequences captured by a fast-flying drone. Despite highly dynamic drone flights, where RGB frames are severely degraded by motion blur and pose priors become unreliable, our method reconstructs high-fidelity radiance fields and preserves fine scene details, delivering a performance gain of over 50% on real-world data compared to state-of-the-art methods.

</details>


### [91] [BrepGaussian: CAD reconstruction from Multi-View Images with Gaussian Splatting](https://arxiv.org/abs/2602.21105)
*Jiaxing Yu,Dongyang Ren,Hangyu Xu,Zhouyuxiao Yang,Yuanqi Li,Jie Guo,Zhengkang Zhou,Yanwen Guo*

Main category: cs.CV

TL;DR: BrepGaussian是一个从2D图像学习3D参数化B-rep表示的新框架，采用高斯泼溅渲染器和两阶段学习策略，在几何重建和特征学习方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 从非结构化数据恢复B-rep表示是计算机视觉和图形学中的重要挑战，现有深度学习方法依赖密集干净的点云且难以泛化到新形状。

Method: 使用带有可学习特征的高斯泼溅渲染器，采用两阶段学习框架：先捕获几何和边缘，再细化面片特征以实现清晰几何和一致的实例表示。

Result: 大量实验表明该方法在性能上优于最先进的方法。

Conclusion: BrepGaussian框架能够有效从2D图像学习3D参数化B-rep表示，为几何重建提供了新的解决方案。

Abstract: The boundary representation (B-rep) models a 3D solid as its explicit boundaries: trimmed corners, edges, and faces. Recovering B-rep representation from unstructured data is a challenging and valuable task of computer vision and graphics. Recent advances in deep learning have greatly improved the recovery of 3D shape geometry, but still depend on dense and clean point clouds and struggle to generalize to novel shapes. We propose B-rep Gaussian Splatting (BrepGaussian), a novel framework that learns 3D parametric representations from 2D images. We employ a Gaussian Splatting renderer with learnable features, followed by a specific fitting strategy. To disentangle geometry reconstruction and feature learning, we introduce a two-stage learning framework that first captures geometry and edges and then refines patch features to achieve clean geometry and coherent instance representations. Extensive experiments demonstrate the superior performance of our approach to state-of-the-art methods. We will release our code and datasets upon acceptance.

</details>


### [92] [UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics](https://arxiv.org/abs/2602.21137)
*Joseph Raj Vishal,Nagasiri Poluri,Katha Naik,Rutuja Patil,Kashyap Hegde Kota,Krishna Vinod,Prithvi Jai Ramesh,Mohammad Farhadi,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: UDVideoQA是一个专门针对城市交通多智能体动态分析的视频问答基准数据集，包含28K个问答对，用于系统评估视频语言模型的视觉定位和因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 理解城市交通的复杂多智能体动态是视频语言模型面临的根本挑战，需要能够捕捉真实世界未脚本行为的基准数据集。

Method: 从16小时城市交叉口交通录像中筛选，采用事件驱动的动态模糊技术保护隐私，通过统一标注流程生成密集标注的问答对，构建分层推理级别的分类体系。

Result: 实验显示模型存在感知-推理差距，Gemini Pro在零样本准确率最高，但经过微调的Qwen2.5-VL 7B能弥合这一差距，达到与专有系统相当的性能。

Conclusion: UDVideoQA套件为推进鲁棒、隐私感知和真实世界的多模态推理提供了基础，揭示了当前模型在语言多样性方面的局限性，需要以人为中心的评估。

Abstract: Understanding the complex, multi-agent dynamics of urban traffic remains a fundamental challenge for video language models. This paper introduces Urban Dynamics VideoQA, a benchmark dataset that captures the unscripted real-world behavior of dynamic urban scenes. UDVideoQA is curated from 16 hours of traffic footage recorded at multiple city intersections under diverse traffic, weather, and lighting conditions. It employs an event-driven dynamic blur technique to ensure privacy preservation without compromising scene fidelity. Using a unified annotation pipeline, the dataset contains 28K question-answer pairs generated across 8 hours of densely annotated video, averaging one question per second. Its taxonomy follows a hierarchical reasoning level, spanning basic understanding and attribution to event reasoning, reverse reasoning, and counterfactual inference, enabling systematic evaluation of both visual grounding and causal reasoning. Comprehensive experiments benchmark 10 SOTA VideoLMs on UDVideoQA and 8 models on a complementary video question generation benchmark. Results reveal a persistent perception-reasoning gap, showing models that excel in abstract inference often fail with fundamental visual grounding. While models like Gemini Pro achieve the highest zero-shot accuracy, fine-tuning the smaller Qwen2.5-VL 7B model on UDVideoQA bridges this gap, achieving performance comparable to proprietary systems. In VideoQGen, Gemini 2.5 Pro, and Qwen3 Max generate the most relevant and complex questions, though all models exhibit limited linguistic diversity, underscoring the need for human-centric evaluation. The UDVideoQA suite, including the dataset, annotation tools, and benchmarks for both VideoQA and VideoQGen, provides a foundation for advancing robust, privacy-aware, and real-world multimodal reasoning. UDVideoQA is available at https://ud-videoqa.github.io/UD-VideoQA/UD-VideoQA/.

</details>


### [93] [SynthRender and IRIS: Open-Source Framework and Dataset for Bidirectional Sim-Real Transfer in Industrial Object Perception](https://arxiv.org/abs/2602.21141)
*Jose Moises Araya-Martinez,Thushar Tom,Adrián Sanchis Reig,Pablo Rey Valiente,Jens Lambrecht,Jörg Krüger*

Main category: cs.CV

TL;DR: SynthRender开源框架结合领域随机化技术，通过合成图像生成和3D资产创建解决工业物体感知中的数据稀缺问题，在多个基准测试中达到98%以上的mAP@50性能。


<details>
  <summary>Details</summary>
Motivation: 工业物体感知任务需要大量标注数据，但获取和标注专有零件的真实数据成本高昂，成为广泛部署的主要障碍。

Method: 开发SynthRender开源框架，具备引导式领域随机化功能；利用Reality-to-Simulation技术从2D图像创建3D合成资产；发布包含32个类别、约20,000个标注的IRIS工业数据集。

Result: 在多个基准测试中表现优异：公共机器人数据集99.1% mAP@50，汽车基准98.3% mAP@50，IRIS数据集95.3% mAP@50。

Conclusion: 合成数据生成与领域随机化相结合的方法能够有效解决工业物体感知中的数据稀缺问题，为缺乏3D文件的零件提供低成本、可迁移的数据解决方案。

Abstract: Object perception is fundamental for tasks such as robotic material handling and quality inspection. However, modern supervised deep-learning perception models require large datasets for robust automation under semi-uncontrolled conditions. The cost of acquiring and annotating such data for proprietary parts is a major barrier for widespread deployment. In this context, we release SynthRender, an open source framework for synthetic image generation with Guided Domain Randomization capabilities. Furthermore, we benchmark recent Reality-to-Simulation techniques for 3D asset creation from 2D images of real parts. Combined with Domain Randomization, these synthetic assets provide low-overhead, transferable data even for parts lacking 3D files. We also introduce IRIS, the Industrial Real-Sim Imagery Set, containing 32 categories with diverse textures, intra-class variation, strong inter-class similarities and about 20,000 labels. Ablations on multiple benchmarks outline guidelines for efficient data generation with SynthRender. Our method surpasses existing approaches, achieving 99.1% mAP@50 on a public robotics dataset, 98.3% mAP@50 on an automotive benchmark, and 95.3% mAP@50 on IRIS.

</details>


### [94] [LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis](https://arxiv.org/abs/2602.21142)
*Zhifan Jiang,Dong Yang,Vishwesh Nath,Abhijeet Parida,Nishad P. Kulkarni,Ziyue Xu,Daguang Xu,Syed Muhammad Anwar,Holger R. Roth,Marius George Linguraru*

Main category: cs.CV

TL;DR: LUMEN是一个针对纵向胸部X光片分析优化的训练框架，通过多图像多任务指令微调提升预后和诊断性能，在MIMIC-CXR数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 放射科医生分析纵向影像的时间消耗大，需要开发能够提供预后能力的训练框架来辅助决策支持。

Method: 提出LUMEN框架，采用多图像和多任务指令微调策略，构建包含纵向研究的指令跟随数据集，开发预后视觉问答任务。

Result: 在诊断性VQA任务中显著优于基线模型，展现出良好的预后能力潜力。

Conclusion: 精心设计的指令微调视觉语言模型能够实现对纵向放射影像数据更准确和临床意义更强的解读。

Abstract: Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are essential for accurate diagnosis and prognosis. The manual longitudinal analysis is a time-consuming process, motivating the development of a training framework that can provide prognostic capabilities. We introduce a novel training framework LUMEN, that is optimized for longitudinal CXR interpretation, leveraging multi-image and multi-task instruction fine-tuning to enhance prognostic and diagnostic performance. We conduct experiments on the publicly available MIMIC-CXR and its associated Medical-Diff-VQA datasets. We further formulate and construct a novel instruction-following dataset incorporating longitudinal studies, enabling the development of a prognostic VQA task. Our method demonstrates significant improvements over baseline models in diagnostic VQA tasks, and more importantly, shows promising potential for prognostic capabilities. These results underscore the value of well-designed, instruction-tuned VLMs in enabling more accurate and clinically meaningful radiological interpretation of longitudinal radiological imaging data.

</details>


### [95] [SPRITETOMESH: Automatic Mesh Generation for 2D Skeletal Animation Using Learned Segmentation and Contour-Aware Vertex Placement](https://arxiv.org/abs/2602.21153)
*Bastien Gimbert*

Main category: cs.CV

TL;DR: SPRITETOMESH是一个全自动的2D游戏精灵转三角形网格管线，通过混合学习-算法方法将精灵图像转换为兼容骨骼动画框架的网格，处理速度比人工快300-1200倍


<details>
  <summary>Details</summary>
Motivation: 传统的手工创建动画就绪网格过程繁琐耗时，每个精灵需要15-60分钟人工放置顶点，需要自动化解决方案来提高游戏开发效率

Method: 采用混合方法：使用EfficientNet-B0编码器和U-Net解码器的分割网络生成精确二进制掩码，然后通过Douglas-Peucker简化提取外部轮廓顶点，使用双边滤波多通道Canny边缘检测和轮廓跟踪放置内部顶点，最后通过Delaunay三角剖分生成最终网格

Result: 分割网络在172个游戏的100,000+精灵-掩码对上达到0.87 IoU，完整管线处理一个精灵只需不到3秒，相比人工制作速度提升300-1200倍

Conclusion: 神经网络热图回归不适合顶点位置预测任务，验证了混合设计的有效性：在明确标注处使用学习分割，在需要领域启发式时使用算法放置，该方法成功实现了高效自动化的精灵网格生成

Abstract: We present SPRITETOMESH, a fully automatic pipeline for converting 2D game sprite images into triangle meshes compatible with skeletal animation frameworks such as Spine2D. Creating animation-ready meshes is traditionally a tedious manual process requiring artists to carefully place vertices along visual boundaries, a task that typically takes 15-60 minutes per sprite. Our method addresses this through a hybrid learned-algorithmic approach. A segmentation network (EfficientNet-B0 encoder with U-Net decoder) trained on over 100,000 sprite-mask pairs from 172 games achieves an IoU of 0.87, providing accurate binary masks from arbitrary input images. From these masks, we extract exterior contour vertices using Douglas-Peucker simplification with adaptive arc subdivision, and interior vertices along visual boundaries detected via bilateral-filtered multi-channel Canny edge detection with contour-following placement. Delaunay triangulation with mask-based centroid filtering produces the final mesh. Through controlled experiments, we demonstrate that direct vertex position prediction via neural network heatmap regression is fundamentally not viable for this task: the heatmap decoder consistently fails to converge (loss plateau at 0.061) while the segmentation decoder trains normally under identical conditions. We attribute this to the inherently artistic nature of vertex placement - the same sprite can be meshed validly in many different ways. This negative result validates our hybrid design: learned segmentation where ground truth is unambiguous, algorithmic placement where domain heuristics are appropriate. The complete pipeline processes a sprite in under 3 seconds, representing a speedup of 300x-1200x over manual creation. We release our trained model to the game development community.

</details>


### [96] [Seeing Through Words: Controlling Visual Retrieval Quality with Language Models](https://arxiv.org/abs/2602.21175)
*Jianglin Lu,Simon Jenni,Kushal Kafle,Jing Shi,Handong Zhao,Yun Fu*

Main category: cs.CV

TL;DR: 提出质量可控检索新范式，通过生成语言模型将简短查询扩展为包含细粒度视觉属性和质量控制的描述性查询，兼容现有视觉语言模型且具有可解释性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中文本-图像检索常面临简短查询的挑战：语义模糊、视觉解释多样且缺乏图像质量控制，需要增强查询表达能力和质量意识。

Method: 利用生成语言模型作为查询补全函数，基于离散化质量等级（相关性和美学评分）扩展简短查询为描述性形式，捕获姿态、场景、美学等细粒度视觉属性。

Result: 实验表明该方法显著改善检索结果并提供有效的质量控制，弥合了现代视觉语言模型表达能力与简短用户查询之间的差距。

Conclusion: 提出的质量可控检索框架为简短查询提供了语义增强和质量控制的解决方案，具有灵活性、透明度和可控性三大优势，有效提升了检索性能。

Abstract: Text-to-image retrieval is a fundamental task in vision-language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, rendering them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queries with contextual details while incorporating explicit notions of image quality. Our key idea is to leverage a generative language model as a query completion function, extending underspecified queries into descriptive forms that capture fine-grained visual attributes such as pose, scene, and aesthetics. We introduce a general framework that conditions query completion on discretized quality levels, derived from relevance and aesthetic scoring models, so that query enrichment is not only semantically meaningful but also quality-aware. The resulting system provides three key advantages: 1) flexibility, it is compatible with any pretrained vision-language model (VLMs) without modification; 2) transparency, enriched queries are explicitly interpretable by users; and 3) controllability, enabling retrieval results to be steered toward user-preferred quality levels. Extensive experiments demonstrate that our proposed approach significantly improves retrieval results and provides effective quality control, bridging the gap between the expressive capacity of modern VLMs and the underspecified nature of short user queries. Our code is available at https://github.com/Jianglin954/QCQC.

</details>


### [97] [XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence](https://arxiv.org/abs/2602.21178)
*Sepehr Salem Ghahfarokhi,M. Moein Esfahani,Raj Sunderraman,Vince Calhoun,Mohammed Alser*

Main category: cs.CV

TL;DR: XMorph框架：结合信息加权边界归一化和双通道可解释AI模块的脑肿瘤分类系统，在保持96.0%高精度的同时提供临床可解释性


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在脑肿瘤诊断中存在的可解释性不足和计算效率问题，传统模型作为黑盒无法量化复杂的肿瘤边界特征

Method: 提出信息加权边界归一化(IWBN)机制强调诊断相关边界区域，结合非线性混沌和临床验证特征；双通道可解释AI模块整合GradCAM++可视化提示和LLM生成的文本解释

Result: 在三种主要脑肿瘤类型(胶质瘤、脑膜瘤、垂体瘤)分类中达到96.0%的准确率

Conclusion: 证明可解释性和高性能可以在AI医疗影像系统中共存，XMorph为临床采用提供了技术基础

Abstract: Deep learning has significantly advanced automated brain tumor diagnosis, yet clinical adoption remains limited by interpretability and computational constraints. Conventional models often act as opaque ''black boxes'' and fail to quantify the complex, irregular tumor boundaries that characterize malignant growth. To address these challenges, we present XMorph, an explainable and computationally efficient framework for fine-grained classification of three prominent brain tumor types: glioma, meningioma, and pituitary tumors. We propose an Information-Weighted Boundary Normalization (IWBN) mechanism that emphasizes diagnostically relevant boundary regions alongside nonlinear chaotic and clinically validated features, enabling a richer morphological representation of tumor growth. A dual-channel explainable AI module combines GradCAM++ visual cues with LLM-generated textual rationales, translating model reasoning into clinically interpretable insights. The proposed framework achieves a classification accuracy of 96.0%, demonstrating that explainability and high performance can co-exist in AI-based medical imaging systems. The source code and materials for XMorph are all publicly available at: https://github.com/ALSER-Lab/XMorph.

</details>


### [98] [Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision](https://arxiv.org/abs/2602.21179)
*Nicolás Gaggion,Maria J. Ledesma-Carbayo,Stergios Christodoulidis,Maria Vakalopoulou,Enzo Ferrante*

Main category: cs.CV

TL;DR: Mask-HybridGNet是一个无需手动标注地标即可训练基于图的医学图像分割框架，通过结合Chamfer距离监督和边缘正则化，直接从标准像素级掩码中学习固定拓扑的解剖结构表示，实现跨患者的隐式对应关系学习。


<details>
  <summary>Details</summary>
Motivation: 传统基于图的医学图像分割需要具有点对点对应关系的手动标注地标数据集，这在临床实践中极为罕见，限制了该技术的临床应用。

Method: 结合Chamfer距离监督和边缘正则化，将可变长度的真实边界与固定长度的地标预测对齐，通过可微分光栅化进行精炼，确保局部平滑性和规则地标分布。

Result: 在胸部X光、心脏超声、心脏MRI和胎儿成像等多个医学影像模态上取得与最先进像素方法竞争的结果，同时通过固定图邻接矩阵确保解剖合理性。

Conclusion: 该框架利用大量标准分割掩码构建结构化模型，保持拓扑完整性并提供隐式对应关系，支持时间跟踪、跨切片重建和形态学群体分析等临床应用。

Abstract: Graph-based medical image segmentation represents anatomical structures using boundary graphs, providing fixed-topology landmarks and inherent population-level correspondences. However, their clinical adoption has been hindered by a major requirement: training datasets with manually annotated landmarks that maintain point-to-point correspondences across patients rarely exist in practice. We introduce Mask-HybridGNet, a framework that trains graph-based models directly using standard pixel-wise masks, eliminating the need for manual landmark annotations. Our approach aligns variable-length ground truth boundaries with fixed-length landmark predictions by combining Chamfer distance supervision and edge-based regularization to ensure local smoothness and regular landmark distribution, further refined via differentiable rasterization. A significant emergent property of this framework is that predicted landmark positions become consistently associated with specific anatomical locations across patients without explicit correspondence supervision. This implicit atlas learning enables temporal tracking, cross-slice reconstruction, and morphological population analyses. Beyond direct segmentation, Mask-HybridGNet can extract correspondences from existing segmentation masks, allowing it to generate stable anatomical atlases from any high-quality pixel-based model. Experiments across chest radiography, cardiac ultrasound, cardiac MRI, and fetal imaging demonstrate that our model achieves competitive results against state-of-the-art pixel-based methods, while ensuring anatomical plausibility by enforcing boundary connectivity through a fixed graph adjacency matrix. This framework leverages the vast availability of standard segmentation masks to build structured models that maintain topological integrity and provide implicit correspondences.

</details>


### [99] [Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning](https://arxiv.org/abs/2602.21186)
*Haoyi Jiang,Liu Liu,Xinjie Wang,Yonghao He,Wei Sui,Zhizhong Su,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: Spa3R是一个自监督框架，通过预测性空间场建模从无位姿多视角图像学习统一视角不变空间表示，无需显式3D数据即可提升视觉语言模型的3D空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型(VLMs)的3D空间理解能力有限，现有方法依赖显式3D模态或部分几何先验，导致可扩展性差且需要语言模型隐式重建3D几何。

Method: 提出预测性空间场建模(PSFM)范式，从无位姿多视角图像学习紧凑潜在表示，合成任意未见视角的特征场，形成Spa3R编码器并通过轻量适配器集成到VLMs中。

Result: 在VSI-Bench基准测试中，Spa3-VLM达到58.6%的3D视觉问答准确率，显著超越先前方法。

Conclusion: PSFM为提升空间智能提供了可扩展的路径，证明仅从2D视觉即可内在涌现空间智能，无需显式空间指令调优。

Abstract: While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.

</details>


### [100] [Human Video Generation from a Single Image with 3D Pose and View Control](https://arxiv.org/abs/2602.21188)
*Tiantian Wang,Chun-Han Yao,Tao Hu,Mallikarjun Byrasandra Ramalinga Reddy,Ming-Hsuan Yang,Varun Jampani*

Main category: cs.CV

TL;DR: HVG是一个潜在视频扩散模型，通过三个关键技术设计从单张图像生成高质量、多视角、时空一致的人体视频，支持3D姿态和视角控制。


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法在图像到视频合成中面临挑战，特别是在人体视频生成中，从单张图像推断视角一致、运动相关的衣物褶皱仍然是一个难题。

Method: 采用三种关键设计：(i)关节姿态调制，通过双维骨骼图捕捉3D关节解剖关系并引入3D信息解决跨视角自遮挡；(ii)视角和时间对齐，确保多视角一致性和参考图像与姿态序列的对齐；(iii)渐进时空采样，保持长多视角动画的平滑过渡。

Result: 在图像到视频任务上的广泛实验表明，HVG在从多样化人体图像和姿态输入生成高质量4D人体视频方面优于现有方法。

Conclusion: HVG通过创新的技术设计成功解决了从单张图像生成多视角一致人体视频的挑战，为4D人体视频生成提供了有效解决方案。

Abstract: Recent diffusion methods have made significant progress in generating videos from single images due to their powerful visual generation capabilities. However, challenges persist in image-to-video synthesis, particularly in human video generation, where inferring view-consistent, motion-dependent clothing wrinkles from a single image remains a formidable problem. In this paper, we present Human Video Generation in 4D (HVG), a latent video diffusion model capable of generating high-quality, multi-view, spatiotemporally coherent human videos from a single image with 3D pose and view control. HVG achieves this through three key designs: (i) Articulated Pose Modulation, which captures the anatomical relationships of 3D joints via a novel dual-dimensional bone map and resolves self-occlusions across views by introducing 3D information; (ii) View and Temporal Alignment, which ensures multi-view consistency and alignment between a reference image and pose sequences for frame-to-frame stability; and (iii) Progressive Spatio-Temporal Sampling with temporal alignment to maintain smooth transitions in long multi-view animations. Extensive experiments on image-to-video tasks demonstrate that HVG outperforms existing methods in generating high-quality 4D human videos from diverse human images and pose inputs.

</details>


### [101] [Region of Interest Segmentation and Morphological Analysis for Membranes in Cryo-Electron Tomography](https://arxiv.org/abs/2602.21195)
*Xingyi Cheng,Julien Maufront,Aurélie Di Cicco,Daniël M. Pelt,Manuela Dezi,Daniel Lévy*

Main category: cs.CV

TL;DR: TomoROIS-SurfORA是一个两阶段框架，通过深度学习直接分割冷冻电镜断层扫描中的感兴趣区域，并进行表面形态学定量分析，特别适用于复杂膜结构的自动分析。


<details>
  <summary>Details</summary>
Motivation: 传统ROI识别方法需要先进行完整结构分割再进行后处理分析，对于连续且几何复杂的膜结构（通常被分割为单一实体）存在明显局限性，需要更直接的形状无关ROI分割和形态学分析方法。

Method: 开发了TomoROIS-SurfORA两步骤框架：1) TomoROIS使用深度学习进行形状无关的ROI分割，可从小型标注数据集训练；2) SurfORA将分割结构处理为点云和表面网格，提取定量形态特征（膜间距、曲率、表面粗糙度），支持封闭和开放表面。

Result: 在体外重构的复杂几何变形囊泡膜系统中成功演示，实现了膜接触位点和重构事件（如内陷）的自动定量分析，证明了方法的有效性。

Conclusion: 该组合方法虽然针对冷冻电镜膜数据开发，但适用于更广泛的科学成像场景中的ROI检测和表面分析，为复杂生物结构的定量形态学研究提供了实用工具。

Abstract: Cryo-electron tomography (cryo-ET) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. Identification of regions of interest (ROIs) is central to scientific imaging, as it enables isolation and quantitative analysis of specific structural features within complex datasets. In practice, however, ROIs are typically derived indirectly through full structure segmentation followed by post hoc analysis. This limitation is especially apparent for continuous and geometrically complex structures such as membranes, which are segmented as single entities. Here, we developed TomoROIS-SurfORA, a two step framework for direct, shape-agnostic ROI segmentation and morphological surface analysis. TomoROIS performs deep learning-based ROI segmentation and can be trained from scratch using small annotated datasets, enabling practical application across diverse imaging data. SurfORA processes segmented structures as point clouds and surface meshes to extract quantitative morphological features, including inter-membrane distances, curvature, and surface roughness. It supports both closed and open surfaces, with specific considerations for open surfaces, which are common in cryo-ET due to the missing wedge effect. We demonstrate both tools using in vitro reconstituted membrane systems containing deformable vesicles with complex geometries, enabling automatic quantitative analysis of membrane contact sites and remodeling events such as invagination. While demonstrated here on cryo-ET membrane data, the combined approach is applicable to ROI detection and surface analysis in broader scientific imaging contexts.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [102] [Talking to Yourself: Defying Forgetting in Large Language Models](https://arxiv.org/abs/2602.20162)
*Yutao Sun,Mingshuai Chen,Tiancheng Zhao,Phillip Miao,Zilun Zhang,Haozhan Shen,Ruizhe Zhu,Jianwei Yin*

Main category: cs.CL

TL;DR: SA-SFT是一种轻量级自增强方法，通过在微调前让大语言模型生成自对话数据，并与任务数据混合训练，无需外部数据或额外调优即可有效缓解灾难性遗忘问题，同时提升领域内性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在特定任务微调时出现的灾难性遗忘问题，即模型在获得新能力的同时会严重损害其原有的一般知识和推理能力。

Method: 提出SA-SFT方法：1) LLM在微调前生成自对话数据；2) 将自生成数据与任务数据混合；3) 保持原有优化和训练计划不变。无需外部数据或额外调优。

Result: 在50个评估场景中，SA-SFT保持与原模型相当的性能，在40个案例中取得最佳结果，优于层冻结和外部数据混合等基线方法。同时改善领域内性能并有效缓解遗忘问题。

Conclusion: 自增强提供了一种简单有效的机制，可实现鲁棒的LLM适应而不会导致灾难性遗忘。理论分析表明遗忘部分源于风格诱导的参数漂移，而通过自生成数据的自对齐能有效抵消这种效应。

Abstract: Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules.
  Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.

</details>


### [103] [Benchmarking Distilled Language Models: Performance and Efficiency in Resource-Constrained Settings](https://arxiv.org/abs/2602.20164)
*Sachin Gopal Wani,Eric Page,Ajay Dholakia,David Ellison*

Main category: cs.CL

TL;DR: 知识蒸馏技术能够开发出强大且高效的小语言模型，在资源受限环境中表现优异。研究表明蒸馏模型的性能-计算效率曲线优于传统模型，8B蒸馏模型的计算效率比传统训练高2000多倍，推理能力达到甚至超越10倍大小的标准模型。


<details>
  <summary>Details</summary>
Motivation: 研究知识蒸馏在开发高效小语言模型方面的潜力，为资源受限环境提供高性能AI解决方案，验证蒸馏技术作为构建先进可访问AI的主要策略。

Method: 通过基准测试比较蒸馏模型与原始模型及专有模型的性能和计算成本，进行定量效率分析，评估不同规模模型的性能-计算效率曲线。

Result: 蒸馏8B模型的计算效率比传统训练方法高2000多倍，推理能力与10倍大小的标准模型相当甚至更优，证明了蒸馏在性能与效率方面的显著优势。

Conclusion: 知识蒸馏不仅是模型压缩技术，更是构建先进、可访问AI的主要策略，能够实现计算效率的极大提升同时保持卓越性能表现。

Abstract: Knowledge distillation offers a transformative pathway to developing powerful, yet efficient, small language models (SLMs) suitable for resource-constrained environments. In this paper, we benchmark the performance and computational cost of distilled models against their vanilla and proprietary counterparts, providing a quantitative analysis of their efficiency. Our results demonstrate that distillation creates a superior performance-tocompute curve. We find that creating a distilled 8B model is over 2,000 times more compute-efficient than training its vanilla counterpart, while achieving reasoning capabilities on par with, or even exceeding, standard models ten times its size. These findings validate distillation not just as a compression technique, but as a primary strategy for building state-of-the-art, accessible AI

</details>


### [104] [ConceptRM: The Quest to Mitigate Alert Fatigue through Consensus-Based Purity-Driven Data Cleaning for Reflection Modelling](https://arxiv.org/abs/2602.20166)
*Yongda Yu,Lei Zhang,Xinxin Guo,Minghui Yu,Zhengqi Zhuang,Guoping Rong,Haifeng Shen,Zhengfeng Li,Boge Wang,Guoan Zhang,Bangyu Xiang,Xiaobin Xu*

Main category: cs.CL

TL;DR: ConceptRM提出了一种基于专家锚点和协同学习的方法，从噪声用户反馈数据中构建高质量语料库，有效拦截智能代理产生的虚假警报，显著降低标注成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 智能代理产生的海量警报（大部分为虚假）会导致用户脱敏和关键问题遗漏（警报疲劳）。现有方法依赖用户验证反馈的标注数据训练反射模型，但生产环境收集的数据噪声严重，人工清洗成本高昂。

Method: 使用少量专家标注作为锚点，创建不同噪声比例的扰动数据集，采用协同教学训练多个不同模型进行协作学习。通过分析模型共识决策，从噪声数据集中有效识别可靠负样本。

Result: 实验表明ConceptRM显著提升虚假警报拦截效果，在领域内数据集上比最先进LLM基线提升53.31%，在领域外数据集上提升41.67%，且标注成本极低。

Conclusion: ConceptRM为解决警报疲劳问题提供了一种高效的噪声数据处理方案，通过协同学习和共识分析有效构建高质量训练语料，在降低标注成本的同时显著提升模型性能。

Abstract: In many applications involving intelligent agents, the overwhelming volume of alerts (mostly false) generated by the agents may desensitize users and cause them to overlook critical issues, leading to the so-called ''alert fatigue''. A common strategy is to train a reflection model as a filter to intercept false alerts with labelled data collected from user verification feedback. However, a key challenge is the noisy nature of such data as it is often collected in production environments. As cleaning noise via manual annotation incurs high costs, this paper proposes a novel method ConceptRM for constructing a high-quality corpus to train a reflection model capable of effectively intercepting false alerts. With only a small amount of expert annotations as anchors, ConceptRM creates perturbed datasets with varying noise ratios and utilizes co-teaching to train multiple distinct models for collaborative learning. By analyzing the consensus decisions of these models, it effectively identifies reliable negative samples from a noisy dataset. Experimental results demonstrate that ConceptRM significantly enhances the interception of false alerts with minimal annotation cost, outperforming several state-of-the-art LLM baselines by up to 53.31% on in-domain datasets and 41.67% on out-of-domain datasets.

</details>


### [105] [InterviewSim: A Scalable Framework for Interview-Grounded Personality Simulation](https://arxiv.org/abs/2602.20294)
*Yu Li,Pranav Narayanan Venkit,Yada Pruksachatkun,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: 论文提出了一个基于真实访谈数据的个性化模拟评估框架，通过67.1万个问答对的大规模验证，开发了四个互补的评估指标，发现基于检索增强的方法在个性风格捕捉方面表现优异，而基于时间顺序的方法在事实一致性方面更优。


<details>
  <summary>Details</summary>
Motivation: 现有的人格模拟评估方法依赖人口统计调查、人格问卷或简短AI访谈作为代理，缺乏对个体实际言论的直接评估，需要建立基于真实访谈数据的评估框架。

Method: 从1000位公众人物的23,000份验证访谈记录中提取67.1万个问答对，提出包含内容相似性、事实一致性、人格对齐和事实知识保留的四维度评估框架，系统比较不同数据利用方法。

Result: 基于真实访谈数据的方法显著优于仅依赖传记资料或模型参数知识的方法，揭示了访谈数据利用的权衡：检索增强方法擅长捕捉个性风格和回答质量，时间顺序方法更好地保持事实一致性和知识保留。

Conclusion: 该评估框架支持基于应用需求的原理性方法选择，为推进人格模拟研究提供了可操作的见解，强调了真实数据在个性化模拟中的重要性。

Abstract: Simulating real personalities with large language models requires grounding generation in authentic personal data. Existing evaluation approaches rely on demographic surveys, personality questionnaires, or short AI-led interviews as proxies, but lack direct assessment against what individuals actually said. We address this gap with an interview-grounded evaluation framework for personality simulation at a large scale. We extract over 671,000 question-answer pairs from 23,000 verified interview transcripts across 1,000 public personalities, each with an average of 11.5 hours of interview content. We propose a multi-dimensional evaluation framework with four complementary metrics measuring content similarity, factual consistency, personality alignment, and factual knowledge retention. Through systematic comparison, we demonstrate that methods grounded in real interview data substantially outperform those relying solely on biographical profiles or the model's parametric knowledge. We further reveal a trade-off in how interview data is best utilized: retrieval-augmented methods excel at capturing personality style and response quality, while chronological-based methods better preserve factual consistency and knowledge retention. Our evaluation framework enables principled method selection based on application requirements, and our empirical findings provide actionable insights for advancing personality simulation research.

</details>


### [106] [What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance](https://arxiv.org/abs/2602.20300)
*William Watson,Nicole Cho,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: 该研究发现查询语句的结构特征（如从句嵌套深度、指代模糊性等）与LLM幻觉风险存在系统性关联，建立了22维查询特征向量来预测幻觉倾向，为查询重写和干预策略提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 传统上LLM幻觉被视为模型缺陷，但研究团队从语言学角度提出查询形式本身也会影响模型响应，旨在识别哪些查询特征会增加幻觉风险。

Method: 构建包含从句复杂性、词汇稀有性、指代、否定、可回答性和意图明确性等22个维度的查询特征向量，基于369,837个真实查询进行大规模分析。

Result: 发现一致的"风险图谱"：深度从句嵌套和指代模糊与高幻觉倾向相关，而意图明确和可回答性特征则与低幻觉率相关；域特异性等特征则呈现混合效应。

Conclusion: 查询特征与幻觉风险存在可观测的相关性，这为引导性查询重写和未来干预研究奠定了基础，表明优化查询形式可有效降低LLM幻觉。

Abstract: Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.

</details>


### [107] [No One Size Fits All: QueryBandits for Hallucination Mitigation](https://arxiv.org/abs/2602.20332)
*Nicole Cho,William Watson,Alec Koppel,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

TL;DR: QueryBandits是一个模型无关的上下文多臂赌博机框架，通过在线学习选择最优查询重写策略来减少闭源大语言模型的幻觉问题，无需模型重训练或梯度调整。


<details>
  <summary>Details</summary>
Motivation: 闭源大语言模型在企业部署中占主导地位，但现有研究主要关注开源模型的幻觉缓解方法，缺乏针对闭源模型的解决方案。

Method: 采用上下文多臂赌博机框架，利用经验验证的奖励函数自适应学习选择最优查询重写策略，通过语义特征进行在线策略学习。

Result: 在16个问答场景中，最佳QueryBandit策略（Thompson Sampling）相比无重写基线获得87.5%的胜率，分别比零-shot静态策略（如Paraphrase或Expand）高出42.6%和60.3%。

Conclusion: 没有单一重写策略对所有查询都最优，QueryBandits通过前向传递机制纯粹改变模型行为，可在闭源模型中使用，无需重训练或基于梯度的适配。

Abstract: Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.

</details>


### [108] [Natural Language Processing Models for Robust Document Categorization](https://arxiv.org/abs/2602.20336)
*Radoslaw Roszczyk,Pawel Tecza,Maciej Stodolski,Krzysztof Siwek*

Main category: cs.CL

TL;DR: 评估三种机器学习模型在文本分类中的性能比较：BERT准确率最高但计算成本大，BiLSTM提供最佳平衡，朴素贝叶斯最快但准确率最低。开发了演示系统验证实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估不同复杂度的机器学习方法在自动文本分类中的表现，特别关注分类准确率与计算效率之间的平衡，这对实际AI自动化管道的集成至关重要。

Method: 采用三种不同复杂度的模型进行比较：朴素贝叶斯分类器、双向LSTM网络和微调的基于BERT的Transformer模型。设计并实现了一个演示系统来处理不平衡文档分类和分发。

Result: BERT表现最佳（准确率>99%），但训练时间长且计算资源需求大；BiLSTM达到98.56%准确率，计算成本适中；朴素贝叶斯训练最快（毫秒级）但准确率最低（约94.5%）。类别不平衡对所有方法都有影响。

Conclusion: BiLSTM在研究的场景中提供了最平衡的解决方案，既保证了较高的准确率又控制了计算成本。研究还指出了未来改进的方向和Transformer架构进一步探索的机会。

Abstract: This article presents an evaluation of several machine learning methods applied to automated text classification, alongside the design of a demonstrative system for unbalanced document categorization and distribution. The study focuses on balancing classification accuracy with computational efficiency, a key consideration when integrating AI into real world automation pipelines. Three models of varying complexity were examined: a Naive Bayes classifier, a bidirectional LSTM network, and a fine tuned transformer based BERT model.
  The experiments reveal substantial differences in performance. BERT achieved the highest accuracy, consistently exceeding 99\%, but required significantly longer training times and greater computational resources. The BiLSTM model provided a strong compromise, reaching approximately 98.56\% accuracy while maintaining moderate training costs and offering robust contextual understanding. Naive Bayes proved to be the fastest to train, on the order of milliseconds, yet delivered the lowest accuracy, averaging around 94.5\%. Class imbalance influenced all methods, particularly in the recognition of minority categories.
  A fully functional demonstrative system was implemented to validate practical applicability, enabling automated routing of technical requests with throughput unattainable through manual processing. The study concludes that BiLSTM offers the most balanced solution for the examined scenario, while also outlining opportunities for future improvements and further exploration of transformer architectures.

</details>


### [109] [How communicatively optimal are exact numeral systems? Once more on lexicon size and morphosyntactic complexity](https://arxiv.org/abs/2602.20372)
*Chundra Cathcart,Arne Rubehn,Katja Bocklage,Luca Ciucci,Kellen Parker van Dam,Alžběta Kučerová,Jekaterina Mažara,Carlo Y. Meloni,David Snee,Johann-Mattis List*

Main category: cs.CL

TL;DR: 研究发现许多语言的实际数字系统效率低于理论预期，挑战了精确递归数字系统优化交流效率的现有观点


<details>
  <summary>Details</summary>
Motivation: 先前研究认为精确递归数字系统通过平衡数字词汇库大小和形态句法复杂性来优化交流效率，但未充分考虑到语言中表现出的复杂程度

Method: 使用来自52种基因多样性语言的数据，采用区分可预测和不可预测异形形式（形式变异）的注释方案进行分析

Result: 研究表明世界上许多语言的数字系统效率明显低于预期水平

Conclusion: 研究发现对数字系统研究和更广泛的语言演化研究具有重要意义，表明现有理论需要重新审视

Abstract: Recent research argues that exact recursive numeral systems optimize communicative efficiency by balancing a tradeoff between the size of the numeral lexicon and the average morphosyntactic complexity (roughly length in morphemes) of numeral terms. We argue that previous studies have not characterized the data in a fashion that accounts for the degree of complexity languages display. Using data from 52 genetically diverse languages and an annotation scheme distinguishing between predictable and unpredictable allomorphy (formal variation), we show that many of the world's languages are decisively less efficient than one would expect. We discuss the implications of our findings for the study of numeral systems and linguistic evolution more generally.

</details>


### [110] [Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems](https://arxiv.org/abs/2602.20379)
*Mukul Chhabra,Luigi Medrano,Arush Verma*

Main category: cs.CL

TL;DR: 提出面向企业多轮RAG系统的案例感知LLM-as-a-Judge评估框架，通过8个操作导向的指标评估检索质量、基础保真度、回答效用等维度，解决现有评估方法在企业场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 企业RAG助手在多轮案例工作流中运行，现有评估框架主要针对基准测试或单轮设置，无法捕捉企业特定的故障模式，如案例识别错误、工作流不对齐和跨轮次的部分解决等问题。

Method: 开发案例感知的LLM-as-a-Judge评估框架，使用8个操作基础指标评估每个轮次，采用严重性感知评分协议减少分数膨胀，使用确定性提示和严格JSON输出实现可扩展的批量评估。

Result: 比较研究表明，通用代理指标提供模糊信号，而提出的框架能够揭示对企业关键的可操作权衡，支持系统改进。

Conclusion: 该框架为企业多轮RAG系统提供了更准确和可操作的评估方法，能够有效识别企业特定的故障模式并支持生产环境监控和回归测试。

Abstract: Enterprise Retrieval-Augmented Generation (RAG) assistants operate in multi-turn, case-based workflows such as technical support and IT operations, where evaluation must reflect operational constraints, structured identifiers (e.g., error codes, versions), and resolution workflows. Existing RAG evaluation frameworks are primarily designed for benchmark-style or single-turn settings and often fail to capture enterprise-specific failure modes such as case misidentification, workflow misalignment, and partial resolution across turns.
  We present a case-aware LLM-as-a-Judge evaluation framework for enterprise multi-turn RAG systems. The framework evaluates each turn using eight operationally grounded metrics that separate retrieval quality, grounding fidelity, answer utility, precision integrity, and case/workflow alignment. A severity-aware scoring protocol reduces score inflation and improves diagnostic clarity across heterogeneous enterprise cases. The system uses deterministic prompting with strict JSON outputs, enabling scalable batch evaluation, regression testing, and production monitoring.
  Through a comparative study of two instruction-tuned models across short and long workflows, we show that generic proxy metrics provide ambiguous signals, while the proposed framework exposes enterprise-critical tradeoffs that are actionable for system improvement.

</details>


### [111] [Disentangling Geometry, Performance, and Training in Language Models](https://arxiv.org/abs/2602.20433)
*Atharva Kulkarni,Jacob Mitchell Springer,Arjun Subramonian,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 本文系统研究了Transformer解嵌入矩阵的几何特性（特别是有效秩）与模型性能的关系，通过对108个OLMo风格语言模型的实验发现：高有效秩与高性能相关但不具普适性，低有效秩与性能下降共存而非因果关系，有效秩受训练超参数显著影响，现有几何指标主要反映训练选择而非性能预测。


<details>
  <summary>Details</summary>
Motivation: Transformer权重（特别是解嵌入矩阵）的几何特性在可解释性研究中广泛应用，但其对下游性能估计的实用性尚不明确，需要系统研究几何特性与模型性能的关系。

Method: 使用108个OLMo风格语言模型进行受控变量实验，分析解嵌入矩阵的有效秩等几何指标，考察不同训练设置（批量大小、权重衰减等超参数）对几何特性和性能的影响。

Result: 发现高有效秩常与高性能相关但非普适；低有效秩与性能下降共存而非因果关系；有效秩强烈受训练超参数影响；现有几何指标无法可靠预测下游性能。

Conclusion: 模型几何特性主要反映训练选择而非性能表现，现有几何指标不能作为性能预测的可靠指标，需要开发更有效的性能评估方法。

Abstract: Geometric properties of Transformer weights, particularly the unembedding matrix, have been widely useful in language model interpretability research. Yet, their utility for estimating downstream performance remains unclear. In this work, we systematically investigate the relationship between model performance and the unembedding matrix geometry, particularly its effective rank. Our experiments, involving a suite of 108 OLMo-style language models trained under controlled variation, reveal several key findings. While the best-performing models often exhibit a high effective rank, this trend is not universal across tasks and training setups. Contrary to prior work, we find that low effective rank does not cause late-stage performance degradation in small models, but instead co-occurs with it; we find adversarial cases where low-rank models do not exhibit saturation. Moreover, we show that effective rank is strongly influenced by pre-training hyperparameters, such as batch size and weight decay, which in-turn affect the model's performance. Lastly, extending our analysis to other geometric metrics and final-layer representation, we find that these metrics are largely aligned, but none can reliably predict downstream performance. Overall, our findings suggest that the model's geometry, as captured by existing metrics, primarily reflects training choices rather than performance.

</details>


### [112] [From Performance to Purpose: A Sociotechnical Taxonomy for Evaluating Large Language Model Utility](https://arxiv.org/abs/2602.20513)
*Gavin Levinson,Keith Feldman*

Main category: cs.CL

TL;DR: 提出了LUX（语言模型效用分类法），一个结构化评估LLM在性能、交互、操作和治理四个领域效用的综合框架，包含分层维度和组件，支持定量比较和模型选择。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估缺乏统一框架，任务级成功不足以评估实际应用中的适用性，特别是在高风险场景中需要考虑更广泛的社会技术因素。

Method: 开发分层分类框架LUX，包含四个主要领域（性能、交互、操作、治理），每个领域细分为主题对齐的维度和组件，并建立度量指标库支持定量评估。

Result: 创建了全面的LLM效用评估框架LUX，提供外部动态网络工具连接各组件到相关度量指标库，支持实际应用评估。

Conclusion: LUX填补了LLM评估框架的空白，为跨用例评估和比较LLM效用提供了统一分类法，有助于将模型选择与预期用途对齐。

Abstract: As large language models (LLMs) continue to improve at completing discrete tasks, they are being integrated into increasingly complex and diverse real-world systems. However, task-level success alone does not establish a model's fit for use in practice. In applied, high-stakes settings, LLM effectiveness is driven by a wider array of sociotechnical determinants that extend beyond conventional performance measures. Although a growing set of metrics capture many of these considerations, they are rarely organized in a way that supports consistent evaluation, leaving no unified taxonomy for assessing and comparing LLM utility across use cases. To address this gap, we introduce the Language Model Utility Taxonomy (LUX), a comprehensive framework that structures utility evaluation across four domains: performance, interaction, operations, and governance. Within each domain, LUX is organized hierarchically into thematically aligned dimensions and components, each grounded in metrics that enable quantitative comparison and alignment of model selection with intended use. In addition, an external dynamic web tool is provided to support exploration of the framework by connecting each component to a repository of relevant metrics (factors) for applied evaluation.

</details>


### [113] [Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning](https://arxiv.org/abs/2602.20528)
*Justin Lovelace,Christian Belardi,Sofian Zalouk,Adhitya Polavaram,Srivatsa Kundurthy,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: STAR-LDM是一种结合潜在扩散规划和自回归生成的语言模型，通过添加"思考"阶段在连续空间进行全局语义规划，显著提升了语言理解和推理能力，并支持轻量级分类器的细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 传统自回归语言模型受限于逐令牌决策，缺乏全局规划能力，限制了其在复杂语言任务中的表现。STAR-LDM旨在通过引入扩散规划机制来克服这一局限。

Method: STAR-LDM整合了潜在扩散规划和自回归生成，在生成过程中加入"思考"阶段，通过扩散过程在连续空间精炼语义计划，然后再进行离散令牌的自回归生成。

Result: 在语言理解基准测试中显著优于同等规模模型，在LLM-as-judge比较中获得>70%的胜率（叙事连贯性和常识推理）。支持通过轻量级分类器实现细粒度控制，无需重新训练模型。

Conclusion: STAR-LDM通过扩散规划机制有效提升了语言模型的全局规划能力，在保持流畅性的同时实现了更好的控制效果，为语言模型架构设计提供了新思路。

Abstract: The Stop-Think-AutoRegress Language Diffusion Model (STAR-LDM) integrates latent diffusion planning with autoregressive generation. Unlike conventional autoregressive language models limited to token-by-token decisions, STAR-LDM incorporates a "thinking" phase that pauses generation to refine a semantic plan through diffusion before continuing. This enables global planning in continuous space prior to committing to discrete tokens. Evaluations show STAR-LDM significantly outperforms similar-sized models on language understanding benchmarks and achieves $>70\%$ win rates in LLM-as-judge comparisons for narrative coherence and commonsense reasoning. The architecture also allows straightforward control through lightweight classifiers, enabling fine-grained steering of attributes without model retraining while maintaining better fluency-control trade-offs than specialized approaches.

</details>


### [114] [Personal Information Parroting in Language Models](https://arxiv.org/abs/2602.20580)
*Nishant Subramani,Kshitish Ghate,Mona Diab*

Main category: cs.CL

TL;DR: 研究开发了R&R检测器套件来识别个人隐私信息，发现在Pythia-6.9b模型中有13.6%的个人信息被完全记忆并复述，模型规模和预训练时长与记忆程度呈正相关。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型在大量网络数据上训练，包含数百万条个人信息实例，其中许多被模型记忆，增加了隐私风险。需要开发更好的检测工具并量化记忆程度。

Method: 开发了基于正则表达式和规则的R&R检测器套件，用于检测电子邮件地址、电话号码和IP地址。在手动整理的483个PI实例上测量记忆程度，使用贪婪解码测试模型是否能够完全复述原始PI信息。

Result: R&R检测器优于最佳基于正则表达式的PI检测器。Pythia-6.9b模型完全复述了13.6%的PI实例。模型规模(160M-6.9B)和预训练步数(70k-143k迭代)都与记忆程度正相关，最小的Pythia-160m模型也能完全复述2.7%的实例。

Conclusion: 强烈建议对预训练数据集进行积极过滤和匿名化处理，以最小化个人信息被模型复述的风险。

Abstract: Modern language models (LM) are trained on large scrapes of the Web, containing millions of personal information (PI) instances, many of which LMs memorize, increasing privacy risks. In this work, we develop the regexes and rules (R&R) detector suite to detect email addresses, phone numbers, and IP addresses, which outperforms the best regex-based PI detectors. On a manually curated set of 483 instances of PI, we measure memorization: finding that 13.6% are parroted verbatim by the Pythia-6.9b model, i.e., when the model is prompted with the tokens that precede the PI in the original document, greedy decoding generates the entire PI span exactly. We expand this analysis to study models of varying sizes (160M-6.9B) and pretraining time steps (70k-143k iterations) in the Pythia model suite and find that both model size and amount of pretraining are positively correlated with memorization. Even the smallest model, Pythia-160m, parrots 2.7% of the instances exactly. Consequently, we strongly recommend that pretraining datasets be aggressively filtered and anonymized to minimize PI parroting.

</details>


### [115] [Enhancing Hate Speech Detection on Social Media: A Comparative Analysis of Machine Learning Models and Text Transformation Approaches](https://arxiv.org/abs/2602.20634)
*Saurabh Mishra,Shivani Thakur,Radhika Mamidi*

Main category: cs.CL

TL;DR: 评估机器学习模型在仇恨言论检测中的效果，比较CNN、LSTM、BERT等模型性能，探索文本转换技术中和有害内容，发现BERT准确性最高但混合模型在特定场景表现更优。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上仇恨言论泛滥，需要开发有效的检测和调节工具来应对这一挑战。

Method: 比较传统CNN、LSTM模型与先进BERT及其衍生模型，探索结合不同架构特征的混合模型，并引入创新的文本转换技术将负面表达转化为中性内容。

Result: BERT模型因深度上下文理解能力表现最佳准确性，混合模型在某些场景下展现改进能力，文本转换技术能有效中和有害内容影响。

Conclusion: 当前技术存在优势与局限，需要开发更鲁棒的仇恨言论检测系统，文本转换技术为内容调节提供了有前景的方向。

Abstract: The proliferation of hate speech on social media platforms has necessitated the development of effective detection and moderation tools. This study evaluates the efficacy of various machine learning models in identifying hate speech and offensive language and investigates the potential of text transformation techniques to neutralize such content. We compare traditional models like CNNs and LSTMs with advanced neural network models such as BERT and its derivatives, alongside exploring hybrid models that combine different architectural features. Our results indicate that while advanced models like BERT show superior accuracy due to their deep contextual understanding, hybrid models exhibit improved capabilities in certain scenarios. Furthermore, we introduce innovative text transformation approaches that convert negative expressions into neutral ones, thereby potentially mitigating the impact of harmful content. The implications of these findings are discussed, highlighting the strengths and limitations of current technologies and proposing future directions for more robust hate speech detection systems.

</details>


### [116] [Semantic Novelty at Scale: Narrative Shape Taxonomy and Readership Prediction in 28,606 Books](https://arxiv.org/abs/2602.20647)
*W. Frederick Zimmerman*

Main category: cs.CL

TL;DR: 该研究提出语义新颖性作为叙事结构的信息论度量，通过对28,606本PG19书籍的分析，识别出8种叙事形态原型，发现叙事动态与读者参与度显著相关，且体裁和历史时期对叙事结构有强烈约束作用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种基于信息论的叙事结构量化方法，通过语义新颖性分析大规模文本语料，探索叙事动态与读者参与度之间的关系，并揭示体裁和历史变迁对叙事模式的影响。

Method: 使用768维SBERT嵌入计算段落级语义新颖性（与前面段落质心的余弦距离），通过16段PAA降维处理，采用Ward连接聚类识别叙事形态，并分析叙事特征与读者参与度的相关性。

Result: 识别出8种叙事形态原型；发现叙事体积（新颖性轨迹方差）是读者参与度的最强预测因子（偏相关系数0.32）；体裁对叙事形态有强烈约束；历史分析显示1840-1910年间书籍变得越来越可预测；85%的书籍具有独特的语义路径特征。

Conclusion: 信息密度动态是叙事结构的一个基本维度，与情感或主题不同，对读者参与度具有可测量的影响，研究为大规模叙事分析提供了新的量化框架。

Abstract: I introduce semantic novelty--cosine distance between each paragraph's sentence embedding and the running centroid of all preceding paragraphs--as an information-theoretic measure of narrative structure at corpus scale. Applying it to 28,606 books in PG19 (pre-1920 English literature), I compute paragraph-level novelty curves using 768-dimensional SBERT embeddings, then reduce each to a 16-segment Piecewise Aggregate Approximation (PAA). Ward-linkage clustering on PAA vectors reveals eight canonical narrative shape archetypes, from Steep Descent (rapid convergence) to Steep Ascent (escalating unpredictability). Volume--variance of the novelty trajectory--is the strongest length-independent predictor of readership (partial rho = 0.32), followed by speed (rho = 0.19) and Terminal/Initial ratio (rho = 0.19). Circuitousness shows strong raw correlation (rho = 0.41) but is 93 percent correlated with length; after control, partial rho drops to 0.11--demonstrating that naive correlations in corpus studies can be dominated by length confounds. Genre strongly constrains narrative shape (chi squared = 2121.6, p < 10 to the power negative 242), with fiction maintaining plateau profiles while nonfiction front-loads information. Historical analysis shows books became progressively more predictable between 1840 and 1910 (T/I ratio trend r = negative 0.74, p = 0.037). SAX analysis reveals 85 percent signature uniqueness, suggesting each book traces a nearly unique path through semantic space. These findings demonstrate that information-density dynamics, distinct from sentiment or topic, constitute a fundamental dimension of narrative structure with measurable consequences for reader engagement. Dataset: https://huggingface.co/datasets/wfzimmerman/pg19-semantic-novelty

</details>


### [117] [CARE: An Explainable Computational Framework for Assessing Client-Perceived Therapeutic Alliance Using Large Language Models](https://arxiv.org/abs/2602.20648)
*Anqi Li,Chenxiao Wang,Yu Lu,Renjun Xu,Lizhi Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: CARE是一个基于LLM的框架，通过心理咨询对话转录本自动预测多维度治疗联盟评分并生成可解释的推理。该方法在CounselingWAI数据集上训练，使用理性增强监督微调LLaMA-3.1-8B-Instruct模型，显著提升了与客户评价的相关性。


<details>
  <summary>Details</summary>
Motivation: 传统治疗后问卷存在负担重、延迟性等问题，现有计算方法只能产生粗糙评分、缺乏可解释推理，且无法建模完整的会话上下文。需要一种能够准确捕捉客户对治疗联盟感知的自动化方法。

Method: 基于LLaMA-3.1-8B-Instruct骨干模型，使用9,516个专家标注的理性数据进行理性增强监督微调，从心理咨询转录本预测多维度联盟评分并生成解释性推理。

Result: CARE在实验中优于主流LLM，将咨询师评估与客户感知联盟之间的差距大幅缩小，与客户评分的Pearson相关性提高70%以上。理性增强监督进一步提升了预测准确性，生成的理性质量高且上下文相关。

Conclusion: CARE展示了作为AI辅助工具支持心理健康的潜力，能够识别联盟建立中的常见挑战，揭示互动模式如何影响联盟发展，并提供可行的见解。

Abstract: Client perceptions of the therapeutic alliance are critical for counseling effectiveness. Accurately capturing these perceptions remains challenging, as traditional post-session questionnaires are burdensome and often delayed, while existing computational approaches produce coarse scores, lack interpretable rationales, and fail to model holistic session context. We present CARE, an LLM-based framework to automatically predict multi-dimensional alliance scores and generate interpretable rationales from counseling transcripts. Built on the CounselingWAI dataset and enriched with 9,516 expert-curated rationales, CARE is fine-tuned using rationale-augmented supervision with the LLaMA-3.1-8B-Instruct backbone. Experiments show that CARE outperforms leading LLMs and substantially reduces the gap between counselor evaluations and client-perceived alliance, achieving over 70% higher Pearson correlation with client ratings. Rationale-augmented supervision further improves predictive accuracy. CARE also produces high-quality, contextually grounded rationales, validated by both automatic and human evaluations. Applied to real-world Chinese online counseling sessions, CARE uncovers common alliance-building challenges, illustrates how interaction patterns shape alliance development, and provides actionable insights, demonstrating its potential as an AI-assisted tool for supporting mental health care.

</details>


### [118] [CAMEL: Confidence-Gated Reflection for Reward Modeling](https://arxiv.org/abs/2602.20670)
*Zirui Zhu,Hailun Xu,Yang Luo,Yong Liu,Kanchan Sarkar,Kun Xu,Yang You*

Main category: cs.CL

TL;DR: CAMEL：基于置信度门控的反射框架，通过单标记偏好决策和选择性反射机制，在保持高效率的同时提升奖励模型的准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型存在效率与解释性之间的权衡：标量判别模型高效但缺乏解释性，生成式评判模型解释性强但计算开销大。研究发现判决标记的对数概率边际与预测正确性高度相关，可作为实例难度的可靠代理指标

Method: 1. 轻量级单标记偏好决策 2. 基于置信度门控的选择性反射机制（仅对低置信度实例进行反射）3. 通过反事实前缀增强的强化学习训练，暴露模型于多样化初始判决并鼓励真正修正

Result: 在三个广泛使用的奖励模型基准测试中达到82.9%的平均准确率，比之前最佳模型提升3.2%，仅使用14B参数即超越70B参数模型的表现，建立了更优的准确率-效率帕累托前沿

Conclusion: CAMEL框架成功解决了奖励模型中效率与准确性的权衡问题，通过置信度门控机制和反事实训练方法，实现了在保持计算效率的同时显著提升模型性能，为大规模语言模型的人类偏好对齐提供了更优解决方案

Abstract: Reward models play a fundamental role in aligning large language models with human preferences. Existing methods predominantly follow two paradigms: scalar discriminative preference models, which are efficient but lack interpretability, and generative judging models, which offer richer reasoning at the cost of higher computational overhead. We observe that the log-probability margin between verdict tokens strongly correlates with prediction correctness, providing a reliable proxy for instance difficulty without additional inference cost. Building on this insight, we propose CAMEL, a confidence-gated reflection framework that performs a lightweight single-token preference decision first and selectively invokes reflection only for low-confidence instances. To induce effective self-correction, we train the model via reinforcement learning with counterfactual prefix augmentation, which exposes the model to diverse initial verdicts and encourages genuine revision. Empirically, CAMEL achieves state-of-the-art performance on three widely used reward-model benchmarks with 82.9% average accuracy, surpassing the best prior model by 3.2% and outperforming 70B-parameter models using only 14B parameters, while establishing a strictly better accuracy-efficiency Pareto frontier.

</details>


### [119] [ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition](https://arxiv.org/abs/2602.20727)
*Xindian Ma,Rundong Kong,Peng Zhang,Ruoxiang Huang,Yongyu Jiang*

Main category: cs.CL

TL;DR: ID-LoRA是一种新颖的参数高效微调框架，通过从预训练权重矩阵中提取和重用聚类参数组，形成多个共享单一可训练低秩矩阵的低秩组件，在减少可训练参数的同时保持模型能力。


<details>
  <summary>Details</summary>
Motivation: LoRA及其变体在大规模模型上仍会引入显著的可训练参数开销，而过度降低秩会导致复杂多任务场景下性能下降，需要打破这种权衡关系。

Method: 提取预训练权重矩阵中的聚类参数组，利用这些组形成多个低秩组件，所有组件共享单一初始化的可训练低秩矩阵。

Result: 在数学推理、代码生成、MMLU、常识问答和安全对齐五个基准测试中，ID-LoRA优于全微调和现有PEFT基线，使用比标准LoRA少46%的可训练参数。在多任务场景中，仅需传统LoRA 54%的参数即可在代码和MMLU任务上超越LoRA及其变体。

Conclusion: ID-LoRA成功打破了参数效率与性能之间的权衡，为大规模语言模型的高效微调提供了更优解决方案。

Abstract: LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.

</details>


### [120] [Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization](https://arxiv.org/abs/2602.20743)
*Gabriel Loiseau,Damien Sileo,Damien Riquet,Maxime Meyer,Marc Tommasi*

Main category: cs.CL

TL;DR: 提出自适应文本匿名化框架，通过自动生成任务特定提示来优化语言模型的匿名化策略，在多个数据集上实现了比现有基线更好的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有匿名化方法依赖静态手动策略，缺乏适应不同领域和需求变化的灵活性，无法在隐私保护和数据效用之间实现动态平衡。

Method: 提出任务特定的提示优化框架，自动构建语言模型的匿名化指令，适应不同的隐私目标、领域和下游使用模式。

Result: 在五个不同领域的数据集上评估，该框架在所有设置中都实现了比基线更好的隐私-效用权衡，计算效率高且在开源模型上表现优异，性能可与大型闭源模型媲美。

Conclusion: 自适应文本匿名化框架能够发现新颖的匿名化策略，探索隐私-效用权衡边界的不同点，为解决上下文敏感的文本匿名化问题提供了有效解决方案。

Abstract: Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.

</details>


### [121] [Explicit Grammar Semantic Feature Fusion for Robust Text Classification](https://arxiv.org/abs/2602.20749)
*Azrin Sultana,Firoz Ahmed*

Main category: cs.CL

TL;DR: 提出了一种结合语法规则和语义信息的轻量级文本分类模型，通过将句子级语法结构编码为紧凑的语法向量，并与冻结的上下文嵌入融合，形成统一特征表示，在保持高性能的同时显著降低计算需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的模型计算密集，不适合资源受限环境，需要开发轻量级但性能优越的文本分类方法。

Method: 显式编码句子级语法结构（句法组合、短语模式、复杂度指标）为语法向量，与冻结的上下文嵌入融合形成统一表示，使用DBNs、LSTMs、BiLSTMs、BERT和XLNET等深度学习模型进行训练评估。

Result: 统一特征表示模型在语义和结构特征捕捉方面优于基线模型2%-15%，在异构领域学习中表现更有效。

Conclusion: 该框架将语法作为显式归纳偏置而非可学习模块，实现了非常轻量化的模型，在边缘设备上提供更好性能，为资源受限环境提供了有效的自然语言处理解决方案。

Abstract: Natural Language Processing enables computers to understand human language by analysing and classifying text efficiently with deep-level grammatical and semantic features. Existing models capture features by learning from large corpora with transformer models, which are computationally intensive and unsuitable for resource-constrained environments. Therefore, our proposed study incorporates comprehensive grammatical rules alongside semantic information to build a robust, lightweight classification model without resorting to full parameterised transformer models or heavy deep learning architectures. The novelty of our approach lies in its explicit encoding of sentence-level grammatical structure, including syntactic composition, phrase patterns, and complexity indicators, into a compact grammar vector, which is then fused with frozen contextual embeddings. These heterogeneous elements unified a single representation that captures both the structural and semantic characteristics of the text. Deep learning models such as Deep Belief Networks (DBNs), Long Short-Term Memory (LSTMs), BiLSTMs, and transformer-based BERT and XLNET were used to train and evaluate the model, with the number of epochs varied. Based on experimental results, the unified feature representation model captures both the semantic and structural properties of text, outperforming baseline models by 2%-15%, enabling more effective learning across heterogeneous domains. Unlike prior syntax-aware transformer models that inject grammatical structure through additional attention layers, tree encoders, or full fine-tuning, the proposed framework treats grammar as an explicit inductive bias rather than a learnable module, resulting in a very lightweight model that delivers better performance on edge devices

</details>


### [122] [SibylSense: Adaptive Rubric Learning via Memory Tuning and Adversarial Probing](https://arxiv.org/abs/2602.20751)
*Yifei Xu,Guilherme Potje,Shivam Shandilya,Tiancheng Yuan,Leonardo de Oliveira Nunes,Rakshanda Agarwal,Saeid Asgari,Adam Atkinson,Emre Kıcıman,Songwu Lu,Ranveer Chandra,Tusher Chakraborty*

Main category: cs.CL

TL;DR: SibylSense是一种推理时学习方法，通过可调记忆库动态调整评分标准生成器，利用验证器基于候选答案的判别差距更新记忆，交替进行记忆调优和策略更新，从而产生更具判别力的评分标准并提升强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 开放式生成的奖励设计是RL后训练的关键障碍。现有评分标准构建方法存在成本高、表面化、不一致或容易饱和漂移等问题，导致奖励攻击。

Method: SibylSense采用推理时学习方法：1) 使用冻结的评分标准生成器和可调记忆库；2) 通过验证器基于参考-候选答案判别差距更新记忆项；3) 交替进行记忆调优和评分标准对抗策略更新。

Result: 在两个开放式任务上的实验表明，SibylSense产生了更具判别力的评分标准，相比静态和非自适应基线方法，显著提升了下游RL性能。

Conclusion: SibylSense通过动态自适应评分标准生成，有效解决了奖励设计中的饱和和漂移问题，为开放式生成的RL后训练提供了更稳健的奖励机制。

Abstract: Designing aligned and robust rewards for open-ended generation remains a key barrier to RL post-training. Rubrics provide structured, interpretable supervision, but scaling rubric construction is difficult: expert rubrics are costly, prompted rubrics are often superficial or inconsistent, and fixed-pool discriminative rubrics can saturate and drift, enabling reward hacking. We present SibylSense, an inference-time learning approach that adapts a frozen rubric generator through a tunable memory bank of validated rubric items. Memory is updated via verifier-based item rewards measured by reference-candidate answer discriminative gaps from a handful of examples. SibylSense alternates memory tuning with a rubric-adversarial policy update that produces rubric-satisfying candidate answers, shrinking discriminative gaps and driving the rubric generator to capture new quality dimensions. Experiments on two open-ended tasks show that SibylSense yields more discriminative rubrics and improves downstream RL performance over static and non-adaptive baselines.

</details>


### [123] [Overton Pluralistic Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.20759)
*Yu Fu,Seongho Son,Ilija Bogunovic*

Main category: cs.CL

TL;DR: OP-GRPO是一个强化学习框架，使单一大型语言模型无需显式提示或模块化编排即可生成多元视角的回应，通过相似性估计器和双奖励系统实现人类视角的广泛覆盖和独特性。


<details>
  <summary>Details</summary>
Motivation: 现有对齐范式在捕捉人类价值观的多元性方面存在局限，需要解决单一查询生成多元视角回应的需求。

Method: 采用两步骤工作流程：1) 训练Sentence Transformer相似性估计器用于准确评估生成回应的覆盖范围；2) OP-GRPO训练将相似性估计器整合到双奖励系统中，确保人类视角的广泛覆盖和每个视角的独特性。

Result: Qwen2.5-3B-Instruct模型在自然语言推理基准上相对20B GPT-OSS基线获得37.4%的相对准确率提升，相对模块化架构基线获得19.1%的相对改进。GPT-4.1评估进一步验证了方法的稳健性。

Conclusion: OP-GRPO框架成功实现了"小模型，大视角覆盖"的效果，为大型语言模型的多元对齐提供了有效解决方案。

Abstract: Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps. First, similarity estimator training fine-tunes a Sentence Transformer for Overton Pluralism tasks to provide more accurate coverage evaluation of generated responses. Second, OP-GRPO training incorporates this similarity estimator into a dual-reward system designed to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate a "small models, big perspective coverage" effect. The trained Qwen2.5-3B-Instruct model surpasses a 20B GPT-OSS baseline with a 37.4 percent relative accuracy gain on a Natural Language Inference benchmark, and also outperforms a modular architecture baseline with a 19.1 percent relative improvement. Additional evaluations using GPT-4.1 as a large language model judge further confirm the robustness of the approach.

</details>


### [124] [Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation](https://arxiv.org/abs/2602.20816)
*Sayantan Dasgupta,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出一种新的尾部分散度方法用于语言模型蒸馏，通过解耦教师模型top-K概率与低概率预测的贡献，在保持KL散度计算效率的同时提升分布尾部信息的影响。


<details>
  <summary>Details</summary>
Motivation: 传统KL散度在语言模型蒸馏中主要受教师模型最高概率的token主导，忽视了低概率但信息丰富的分布尾部成分，导致蒸馏效果受限。

Method: 设计尾部分散度方法，将教师模型的top-K预测概率与低概率预测解耦处理，减少教师模式的影响，同时增加分布尾部的贡献度。

Result: 实验结果显示，该方法在预训练和监督蒸馏任务中均取得竞争性性能，且计算高效，可在学术预算下处理大规模数据集。

Conclusion: 所提出的尾部分散度方法有效改进了语言模型蒸馏过程，在保持计算效率的同时提升了性能，为资源有限的研究环境提供了可行的蒸馏方案。

Abstract: The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.

</details>


### [125] [FinAnchor: Aligned Multi-Model Representations for Financial Prediction](https://arxiv.org/abs/2602.20859)
*Zirui He,Huopu Zhang,Yanguang Liu,Sirui Wu,Mengnan Du*

Main category: cs.CL

TL;DR: FinAnchor是一个轻量级框架，通过选择锚定嵌入空间并学习线性映射来整合多个LLM的嵌入表示，无需微调基础模型，在金融NLP任务中表现优于单一模型和标准集成方法。


<details>
  <summary>Details</summary>
Motivation: 金融长文档预测面临信号稀疏、噪声干扰以及不同任务和时间段最优LLM选择不一的挑战，需要解决异构特征空间的不兼容性问题。

Method: 选择锚定嵌入空间，学习线性映射将其他模型的表示对齐到该锚定空间，然后聚合对齐后的特征形成统一表示用于下游预测。

Result: 在多个金融NLP任务中，FinAnchor始终优于强大的单一模型基线和标准集成方法。

Conclusion: 锚定异构表示的方法能够有效实现稳健的金融预测，证明了该框架在处理多模型嵌入整合方面的有效性。

Abstract: Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.

</details>


### [126] [Exa-PSD: a new Persian sentiment analysis dataset on Twitter](https://arxiv.org/abs/2602.20892)
*Seyed Himan Ghaderi,Saeed Sarbazi Azad,Mohammad Mehdi Jaziriyan,Ahmad Akbari*

Main category: cs.CL

TL;DR: 本文介绍了一个波斯语情感分析数据集Exa，包含12,000条标注推文，使用预训练模型评估达到79.87的宏F分数


<details>
  <summary>Details</summary>
Motivation: 波斯语自然语言处理面临挑战，现有数据集局限于特定主题，社交媒体中存在反讽和口语化表达，需要专门的波斯语推特情感分析数据集

Method: 从波斯语推文收集数据，由5名母语标注者进行三分类标注（正面、中性、负面），使用Pars Bert和Roberta预训练模型进行评估

Result: 构建了包含12,000条标注推文的Exa数据集，评估结果显示宏F分数达到79.87%，表明数据集和模型对情感分析系统具有价值

Conclusion: Exa数据集为波斯语情感分析提供了有价值的资源，预训练模型在该数据集上表现良好，有助于解决波斯语社交媒体文本分析中的挑战

Abstract: Today, Social networks such as Twitter are the most widely used platforms for communication of people. Analyzing this data has useful information to recognize the opinion of people in tweets. Sentiment analysis plays a vital role in NLP, which identifies the opinion of the individuals about a specific topic. Natural language processing in Persian has many challenges despite the adventure of strong language models. The datasets available in Persian are generally in special topics such as products, foods, hotels, etc while users may use ironies, colloquial phrases in social media To overcome these challenges, there is a necessity for having a dataset of Persian sentiment analysis on Twitter. In this paper, we introduce the Exa sentiment analysis Persian dataset, which is collected from Persian tweets. This dataset contains 12,000 tweets, annotated by 5 native Persian taggers. The aforementioned data is labeled in 3 classes: positive, neutral and negative. We present the characteristics and statistics of this dataset and use the pre-trained Pars Bert and Roberta as the base model to evaluate this dataset. Our evaluation reached a 79.87 Macro F-score, which shows the model and data can be adequately valuable for a sentiment analysis system.

</details>


### [127] [The Art of Efficient Reasoning: Data, Reward, and Optimization](https://arxiv.org/abs/2602.20945)
*Taiqiang Wu,Zenan Zu,Bo Zhou,Ngai Wong*

Main category: cs.CL

TL;DR: 本文系统研究了LLM高效推理机制，提出了两阶段训练范式（长度适应和推理精炼），发现通过相对简单的提示训练可避免长度塌陷，并将长度偏差泛化到不同领域，在Qwen3系列模型上验证了方法的鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然受益于链式思维推理，但存在计算开销过大的问题，需要激励短而准确的推理轨迹来提升效率。

Method: 采用强化学习进行奖励塑造，通过统一协议进行大量实验（约20万GPU小时），解构训练提示、rollout策略、奖励塑造和优化策略，特别关注在不同token预算（2k-32k）下的性能表现。

Result: 揭示了训练过程的两阶段范式，发现使用相对简单的提示训练可确保正奖励信号密度，避免长度塌陷，且学习到的长度偏差可跨领域泛化。

Conclusion: 研究为高效推理提供了有价值的见解和实践指南，在Qwen3系列模型（0.6B-30B）上验证了方法的有效性和泛化能力。

Abstract: Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.

</details>


### [128] [Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models](https://arxiv.org/abs/2602.20966)
*Paola Merlo,Chunyang Jiang,Giuseppe Samo,Vivi Nastase*

Main category: cs.CL

TL;DR: BLM任务是一种受智力测试启发的多选语言矩阵任务，通过结构化数据集评估LLMs的语言对象识别、系统性模式检测能力，并支持可解释性研究。


<details>
  <summary>Details</summary>
Motivation: 解决当前大语言模型能力的核心问题：是否能检测语言对象及其属性、识别跨句子的系统性模式、以及语言错误与推理错误的交互作用。

Method: 构建多层级结构的Blackbird语言矩阵数据集（BLM），包含句子内、输入序列间和候选答案内的结构化问题，使用基线模型和定制模型进行实验验证。

Result: BLM任务虽然具有挑战性，但可通过简单基线模型达到良好性能，定制模型表现更优；模型表示包含解决语言任务所需的语法对象和属性，且通过检测跨句系统性模式实现求解。

Conclusion: 精心策划的结构化数据集支持对语言和大语言模型属性的多角度研究，BLM数据集因其结构化特性和人工构建成分，可作为解释性研究的重要工具，帮助理解LLMs的行为机制。

Abstract: This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core questions about current large language models abilities: do LLMs detect linguistic objects and their properties? Do they detect and use systematic patterns across sentences? Are they more prone to linguistic or reasoning errors, and how do these interact?
  We show that BLMs, while challenging, can be solved at good levels of performance, in more than one language, with simple baseline models or, at better performance levels, with more tailored models. We show that their representations contain the grammatical objects and attributes relevant to solve a linguistic task. We also show that these solutions are reached by detecting systematic patterns across sentences.
  The paper supports the point of view that curated, structured datasets support multi-faceted investigations of properties of language and large language models. Because they present a curated, articulated structure, because they comprise both learning contexts and expected answers, and because they are partly built by hand, BLMs fall in the category of datasets that can support explainability investigations, and be useful to ask why large language models behave the way they do.

</details>


### [129] [Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving](https://arxiv.org/abs/2602.20973)
*Yuliang Ji,Fuchen Shen,Jian Wu,Qiujie Xie,Yue Zhang*

Main category: cs.CL

TL;DR: 提出了PC-FOL数据集，专注于案例推理问题，填补了现有数学推理数据集在非线推理方法（如分情况证明）上的空白，并通过实验和理论分析揭示了LLMs在线推理与案例推理之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理数据集主要关注线性推理，缺乏对分情况证明等其他重要推理形式的评估，限制了LLMs推理能力的全面研究。

Method: 引入由专业数学家标注的PC-FOL一阶逻辑数据集，包含手动编写的自然语言证明；通过实验评估主流LLMs的性能，并基于图模型进行理论分析。

Result: 实验结果显示LLMs在案例推理问题上表现明显差于线性推理问题，理论分析从图模型角度解释了这种性能差异。

Conclusion: 该研究揭示了自动自然语言数学证明生成领域的核心挑战，为未来研究指明了方向，强调了非线推理方法在评估LLMs推理能力中的重要性。

Abstract: To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professional mathematicians, focusing on case-based reasoning problems. All instances in this dataset are equipped with a manually written natural language proof, clearly distinguishing it from conventional linear reasoning datasets. Our experimental results over leading LLMs demonstrate a substantial performance gap between linear reasoning and case-based reasoning problems. To further investigate this phenomenon, we provide a theoretical analysis grounded in graphical model, which provides an explanation for the observed disparity between the two types of reasoning problems. We hope this work can reveal the core challenges in the field of automated natural language mathematical proof generation, paving the way for future research.

</details>


### [130] [Evaluating Proactive Risk Awareness of Large Language Models](https://arxiv.org/abs/2602.20976)
*Xuan Luo,Yubin Chen,Zhiyu Hou,Linpu Yu,Geng Tu,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 论文提出了一个主动风险意识评估框架，通过Butterfly数据集测试大语言模型在环境生态领域预测潜在危害的能力，发现现有模型在响应长度限制、跨语言场景和多模态物种保护方面存在显著的主动意识缺陷。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在日常决策中的广泛应用，其安全责任需要从应对显性有害意图扩展到预测非故意但具有严重后果的风险，特别是在环境生态领域。

Method: 构建包含1,094个查询的Butterfly数据集，模拟可能产生潜在生态影响的日常解决方案寻求活动。通过五个广泛使用的LLM进行实验，分析响应长度、语言和模态对主动风险意识的影响。

Result: 实验结果显示：在长度受限的响应条件下主动意识显著下降；跨语言场景下表现相似；多模态物种保护存在持续盲点。所有测试模型在主动风险意识方面都存在明显不足。

Conclusion: 当前LLM的安全对齐与实际生态责任要求之间存在关键差距，需要在模型部署中建立主动安全保障机制，特别是在环境敏感应用场景中。

Abstract: As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological domain. It contains 1,094 queries that simulate ordinary solution-seeking activities whose responses may induce latent ecological impact. Through experiments across five widely used LLMs, we analyze the effects of response length, languages, and modality. Experimental results reveal consistent, significant declines in proactive awareness under length-restricted responses, cross-lingual similarities, and persistent blind spots in (multimodal) species protection. These findings highlight a critical gap between current safety alignment and the requirements of real-world ecological responsibility, underscoring the need for proactive safeguards in LLM deployment.

</details>


### [131] [Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification](https://arxiv.org/abs/2602.21082)
*Vishal Patil,Shree Vaishnavi Bacha,Revanth Yamani,Yidan Sun,Mayank Kejriwal*

Main category: cs.CL

TL;DR: 本研究提出了一种混合方法，结合LLM进行方面识别和传统机器学习进行情感分类，成功分析了470万条餐厅评论，证明了该框架在大规模客户反馈分析中的有效性。


<details>
  <summary>Details</summary>
Motivation: 客户评论是重要信息源，但分析数百万条非结构化评论具有挑战性。虽然LLM在自然语言理解方面有潜力，但其在大规模评论分析中的应用受计算成本和可扩展性限制。

Method: 使用ChatGPT分析餐厅评论样本进行方面识别，基于人工标注评论开发情感分类器，然后将该方法应用于从主要在线平台收集的470万条17年间的评论数据。

Result: 回归分析显示，机器学习标注的方面能显著解释不同餐饮体验方面、菜系和地理区域的餐厅总体评分方差。

Conclusion: LLM与传统机器学习方法结合可以有效自动化大规模客户反馈的基于方面的情感分析，为酒店业和其他服务行业的研究者和从业者提供了实用框架。

Abstract: Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing classic machine-learning methods for sentiment classification at scale. Using ChatGPT to analyze sampled restaurant reviews, we identified key aspects of dining experiences and developed sentiment classifiers using human-labeled reviews, which we subsequently applied to 4.7 million reviews collected over 17 years from a major online platform. Regression analysis reveals that our machine-labeled aspects significantly explain variance in overall restaurant ratings across different aspects of dining experiences, cuisines, and geographical regions. Our findings demonstrate that combining LLMs with traditional machine learning approaches can effectively automate aspect-based sentiment analysis of large-scale customer feedback, suggesting a practical framework for both researchers and practitioners in the hospitality industry and potentially, other service sectors.

</details>


### [132] [Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.21103)
*Sanket Badhe,Deep Shah*

Main category: cs.CL

TL;DR: Prompt-Level Distillation (PLD)是一种新方法，通过从教师模型中提取显式推理模式并组织成结构化指令列表，使小型学生模型能够实现与前沿模型相当的性能，同时保持低延迟和完全可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought提示的高延迟和高推理成本问题，以及微调小模型时牺牲可解释性和引入额外资源开销的局限性。

Method: 从教师模型中提取显式推理模式，将其组织成结构化、表达性指令列表，并作为学生模型的系统提示使用。

Result: 在StereoSet和Contract-NLI数据集上，使用Gemma-3 4B模型，PLD将Macro F1分数分别从57%提升到90.0%和从67%提升到83%，实现了与前沿性能匹配的效果且延迟开销可忽略不计。

Conclusion: PLD方法不仅显著提升了小型模型的性能，更重要的是保持了决策过程的完全透明性，允许人类验证逻辑，特别适用于法律、金融、内容审核等受监管行业以及高吞吐量用例和边缘设备。

Abstract: Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressive instructions for the Student model's System Prompt. Evaluated on the StereoSet and Contract-NLI datasets using Gemma-3 4B, PLD improved Macro F1 scores from 57\% to 90.0\% and 67\% to 83\% respectively, enabling this compact model to match frontier performance with negligible latency overhead. These expressive instructions render the decision-making process transparent, allowing for full human verification of logic, making this approach ideal for regulated industries such as law, finance, and content moderation, as well as high-volume use cases and edge devices.

</details>


### [133] [PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data](https://arxiv.org/abs/2602.21165)
*Samah Fodeh,Linhai Ma,Yan Wang,Srivani Talakokkul,Ganesh Puthiaraju,Afshan Khan,Ashley Hagaman,Sarah Lowe,Aimee Roundtree*

Main category: cs.CL

TL;DR: PVminer是一个专门用于分析患者生成文本的NLP框架，通过多标签多分类预测任务整合BERT编码器和主题建模，有效提取患者声音中的沟通行为和社会健康决定因素。


<details>
  <summary>Details</summary>
Motivation: 传统定性编码方法处理患者生成文本（如安全消息、调查问卷）耗时费力且难以规模化，现有机器学习方法往往将患者中心沟通和社会健康决定因素作为独立任务处理，且不适用于面向患者的语言。

Method: 开发PVminer框架，采用领域适应的BERT编码器（PV-BERT-base和PV-BERT-large），结合无监督主题建模（PV-Topic-BERT）进行主题增强，使用微调分类器处理代码级、子代码级和组合级标签。在微调和推理过程中融入主题表示以丰富语义输入。

Result: PVminer在分层任务中表现优异，F1分数达到：代码级82.25%、子代码级80.14%、组合级最高77.87%，优于生物医学和临床预训练基线模型。消融研究表明作者身份和主题增强均带来显著性能提升。

Conclusion: PVminer提供了一个可扩展的框架来结构化分析患者声音，有效整合患者中心沟通和社会健康决定因素分析，模型和代码将公开发布，标注数据集可供研究使用。

Abstract: Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.

</details>


### [134] [On Data Engineering for Scaling LLM Terminal Capabilities](https://arxiv.org/abs/2602.21193)
*Renjie Pi,Grace Lam,Mohammad Shoeybi,Pooya Jannaty,Bryan Catanzaro,Wei Ping*

Main category: cs.CL

TL;DR: 该论文提出了Terminal-Task-Gen合成任务生成管道和Terminal-Corpus大规模数据集，训练了Nemotron-Terminal模型系列，在Terminal-Bench 2.0基准上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型的终端能力快速进步，但最先进终端代理的训练数据策略仍未公开，需要系统研究终端代理的数据工程实践。

Method: 开发了支持种子基础和技能基础任务构建的轻量级合成任务生成管道Terminal-Task-Gen，分析了数据过滤、课程学习、长上下文训练和缩放行为等训练策略。

Result: 基于Qwen3(8B, 14B, 32B)初始化的Nemotron-Terminal模型在Terminal-Bench 2.0上表现大幅提升：8B模型从2.5%提升至13.0%，14B模型从4.0%提升至20.2%，32B模型从3.4%提升至27.4%，达到了更大模型的性能水平。

Conclusion: 研究填补了终端代理数据工程的空白，提供了开源数据集和模型检查点，显著推动了该领域的研究进展。

Abstract: Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.

</details>
