<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 43]
- [cs.CL](#cs.CL) [Total: 27]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding](https://arxiv.org/abs/2602.17768)
*Boda Lin,Yongjie Zhu,Xiaocheng Gong,Wenyu Qin,Meng Wang*

Main category: cs.CV

TL;DR: 该论文提出了KPM-Bench数据集和MoPE算法，通过运动学分析和语言解析来解决视频描述中细粒度运动细节缺失和幻觉问题，并开发了独立的幻觉评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有视频描述模型在描述细粒度运动细节方面存在显著限制，且在运动中心视频中幻觉问题严重，需要精确描述复杂肢体动态但往往被忽视。

Method: 构建自动化标注流程整合运动学计算和语言解析，创建KPM-Bench数据集包含细粒度视频-描述对、问答对和评估集；提出MoPE算法从文本描述中提取运动属性，并集成到GRPO后训练框架中。

Result: 开发了包含肢体级动态细节的数据集和专门的运动理解评估集，提出了独立于大模型的幻觉评估指标，有效减轻了运动中心视频描述的幻觉问题。

Conclusion: 通过运动学解析和语言基础的方法论，显著提高了运动中心视频描述模型的可靠性，为细粒度运动理解提供了新的基准和解决方案。

Abstract: Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.

</details>


### [2] [CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild](https://arxiv.org/abs/2602.17770)
*Balamurugan Thambiraja,Omid Taheri,Radek Danecek,Giorgio Becherini,Gerard Pons-Moll,Justus Thies*

Main category: cs.CV

TL;DR: 提出CLUTCH系统，结合3D-HIW数据集和SHIFT架构，实现野外环境下文本-手部动作的生成与描述，在文本到动作和动作到文本任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有手部动作建模方法依赖工作室采集数据，动作和场景有限，成本高昂难以扩展到野外环境，且文本-动作对齐的动画保真度不足

Method: 1) 构建3D-HIW数据集：使用视觉语言模型和3D手部跟踪器处理大规模第一人称动作视频；2) 提出CLUTCH系统：基于LLM，包含SHIFT（部件模态分解VQ-VAE架构）和几何精炼阶段

Result: 在文本到动作和动作到文本任务上实现最先进性能，建立了首个可扩展野外手部动作建模基准

Conclusion: 该方法通过大规模野外数据集和创新的架构设计，成功解决了手部动作建模在真实环境中的可扩展性和保真度问题，为手部动画研究提供了新基准

Abstract: Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to "in-the-wild" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.

</details>


### [3] [Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision](https://arxiv.org/abs/2602.17785)
*Xinwei Ju,Rema Daher,Danail Stoyanov,Sophia Bano,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: PRISM是一种自监督学习框架，通过边缘检测和亮度解耦利用解剖学和光照先验来指导结肠镜单目深度和姿态估计，在多个数据集上达到最先进性能，并发现真实数据训练优于仿真数据训练，视频帧率对性能至关重要


<details>
  <summary>Details</summary>
Motivation: 结肠镜辅助导航需要准确的深度和姿态估计来改善筛查效果，但面临纹理缺失表面、复杂光照模式、形变以及缺乏可靠真实标注数据等挑战

Method: 提出PRISM框架，结合学习型边缘检测器（DexiNed或HED）提取高频边界，通过本征分解模块分离着色和反射分量，利用着色线索进行深度估计

Result: 在多个真实和合成数据集上实现最先进性能，消融研究表明：真实数据自监督训练优于仿真数据监督训练；视频帧率是影响模型性能的关键因素

Conclusion: PRISM通过解剖学和光照先验有效解决了结肠镜深度估计的挑战，证明了真实数据的重要性，为结肠镜导航提供了实用的训练策略和最佳实践

Abstract: Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.

</details>


### [4] [LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge](https://arxiv.org/abs/2602.17793)
*Peide Zhu,Linbin Lu,Zhiqin Chen,Xiong Chen*

Main category: cs.CV

TL;DR: LGD-Net提出了一种从H&E切片直接预测HER2表达水平的双流网络框架，通过跨模态特征幻觉而非像素级图像生成，避免了传统虚拟染色方法的计算开销和重建伪影问题。


<details>
  <summary>Details</summary>
Motivation: 传统IHC染色方法资源密集、昂贵且耗时，许多地区无法使用。从H&E切片预测HER2水平是潜在替代方案，但现有像素级虚拟染色方法计算昂贵且易产生重建伪影导致诊断错误。

Method: 提出LGD-Net框架，使用跨模态特征幻觉技术，学习将形态学H&E特征映射到分子潜在空间，通过教师IHC编码器引导训练。采用核分布和膜染色强度的轻量级辅助正则化任务确保特征捕获临床相关表型。

Result: 在公开BCI数据集上的大量实验表明，LGD-Net实现了最先进的性能，显著优于基线方法，同时支持使用单模态H&E输入进行高效推理。

Conclusion: LGD-Net提供了一种高效准确的HER2表达预测方法，避免了传统虚拟染色的计算负担和伪影问题，为乳腺癌评估和靶向治疗选择提供了可行的替代方案。

Abstract: It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.

</details>


### [5] [Enabling Training-Free Text-Based Remote Sensing Segmentation](https://arxiv.org/abs/2602.17799)
*Jose Sosa,Danila Rukhovich,Anis Kacem,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出了一种无需额外训练或仅需轻量级LoRA微调的文本引导遥感图像分割方法，通过整合对比式和生成式视觉语言模型与SAM，在19个遥感基准测试中实现了先进的零样本分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型和视觉基础模型为遥感图像零样本文本引导分割提供了新机遇，但多数方法仍需额外可训练组件，限制了泛化能力和实际应用。

Method: 结合对比式方法（使用CLIP作为SAM网格建议的掩码选择器）和生成式方法（使用GPT-4V和LoRA微调的Qwen-VL生成SAM的点击提示），构建完全零样本或轻量微调的分割流水线。

Result: 在19个遥感基准测试中（包括开放词汇、参考和基于推理的任务），对比式方法实现了最先进的开放词汇语义分割，生成式方法中LoRA微调的Qwen-VL模型表现最佳。

Conclusion: 该方法展示了仅使用现有基础模型即可实现高质量文本引导遥感分割的潜力，为无需额外训练的分割任务提供了有效解决方案，代码将开源。

Abstract: Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.

</details>


### [6] [VidEoMT: Your ViT is Secretly Also a Video Segmentation Model](https://arxiv.org/abs/2602.17807)
*Narges Norouzi,Idil Esen Zulfikar,Niccol`o Cavagnero,Tommie Kerssies,Bastian Leibe,Gijs Dubbelman,Daan de Geus*

Main category: cs.CV

TL;DR: VidEoMT是一种仅使用编码器的视频分割模型，通过查询传播机制实现时序建模，无需专用跟踪模块，在保持竞争性精度的同时速度提升5-10倍。


<details>
  <summary>Details</summary>
Motivation: 现有视频分割模型需要复杂的专用跟踪模块，增加了架构复杂性和计算开销。研究发现大规模预训练的ViT编码器无需专用模块即可实现准确图像分割，因此希望开发简单高效的视频分割方案。

Method: 提出仅编码器的视频分割模型VidEoMT，采用轻量级查询传播机制跨帧传递信息，结合查询融合策略平衡时序一致性和新内容适应性。

Result: 模型在保持竞争性分割精度的同时，速度比现有方法快5-10倍，使用ViT-L骨干网络时可达160 FPS。

Conclusion: VidEoMT证明了仅使用编码器架构即可实现高效视频分割，无需复杂跟踪模块，为视频分割提供了更简洁高效的解决方案。

Abstract: Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/

</details>


### [7] [VQPP: Video Query Performance Prediction Benchmark](https://arxiv.org/abs/2602.17814)
*Adrian Catalin Lutu,Eduard Poesina,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 该论文提出了首个视频查询性能预测（VQPP）基准，包含两个文本到视频检索数据集和两个CBVR系统，共56K文本查询和51K视频，为视频领域的QPP研究提供了标准化评估框架。


<details>
  <summary>Details</summary>
Motivation: 查询性能预测（QPP）在文本和图像检索中已有深入研究，但在基于内容的视频检索（CBVR）领域仍未被充分探索，需要建立专门的基准来推动该领域发展。

Method: 构建包含两个文本到视频检索数据集和两个CBVR系统的VQPP基准，探索多种检索前和检索后性能预测器，并使用最佳检索前预测器作为奖励模型，通过直接偏好优化（DPO）训练大型语言模型进行查询重写。

Result: 检索前预测器表现出竞争性性能，可在检索步骤之前实现应用；VQPP基准为视频QPP研究提供了可复现的比较基础。

Conclusion: VQPP填补了视频领域QPP研究的空白，为未来研究提供了标准化基准，展示了检索前预测器在视频检索中的实用价值，特别是在查询重写等应用中的潜力。

Abstract: Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.

</details>


### [8] [On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective](https://arxiv.org/abs/2602.17854)
*Domonkos Varga*

Main category: cs.CV

TL;DR: 对Liu和Szirányi手势识别方法的评估协议有效性分析，发现其近乎完美的准确率源于帧级随机划分导致的数据泄露问题


<details>
  <summary>Details</summary>
Motivation: 质疑现有手势识别研究中评估协议的有效性，特别是针对无人机-人交互等需要泛化到未见个体应用场景的可靠性

Method: 通过分析已发布的混淆矩阵、学习曲线和数据集构建方式，检验评估协议是否存在数据泄露问题

Result: 发现帧级随机训练-测试划分导致同一受试者样本混合，造成严重数据泄露，评估结果无法反映对未见个体的泛化能力

Conclusion: 强调在基于视觉的手势识别研究中，特别是需要识别未见个体手势的应用中，必须采用主体独立的数据划分方法

Abstract: This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.

</details>


### [9] [Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models](https://arxiv.org/abs/2602.17869)
*Yuxiao Chen,Jue Wang,Zhikang Zhang,Jingru Yi,Xu Zhang,Yang Zou,Zhaowei Cai,Jianbo Yuan,Xinyu Li,Hao Yang,Davide Modolo*

Main category: cs.CV

TL;DR: 提出了一种用于长视频理解的新颖端到端框架，包含基于信息密度的自适应视频采样器和基于自动编码器的时空视频压缩器，与多模态大语言模型集成，有效处理长视频冗余问题。


<details>
  <summary>Details</summary>
Motivation: 长视频分析面临两大挑战：内存限制下高效处理大量帧，以及从海量输入数据中提取判别性信息。视频序列的固有冗余性对现有最先进模型构成显著障碍。

Method: 开发了信息密度自适应视频采样器（AVS）和自动编码器时空视频压缩器（SVC），与多模态大语言模型（MLLM）集成，形成端到端的长视频理解方案。

Result: 框架在多个基准测试中表现优异，在长视频理解任务和标准视频理解基准上都取得出色性能，实现了高压缩率同时保留关键判别信息。

Conclusion: 该方法能自适应有效地从不同时长视频序列中捕获关键信息，在处理长视频序列复杂性方面展现出卓越的通用性和有效性。

Abstract: With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.

</details>


### [10] [Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models](https://arxiv.org/abs/2602.17871)
*Dhruba Ghosh,Yuhui Zhang,Ludwig Schmidt*

Main category: cs.CV

TL;DR: VLMs在视觉问答基准测试中表现优异，但在细粒度图像分类任务中表现不佳。研究发现更好的视觉编码器能显著提升细粒度分类性能，预训练阶段（特别是语言模型权重未冻结时）对细粒度性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在各种视觉问答基准测试中取得显著进展，但在测试细粒度视觉知识的传统图像分类基准中表现落后，需要探究这种性能差异的原因。

Method: 测试大量最新VLMs在细粒度分类基准上的表现，通过消融实验分析影响性能的因素，包括不同LLM、视觉编码器和预训练策略的影响。

Result: 发现使用更好的LLM能同等提升所有基准分数，而更好的视觉编码器能不成比例地提升细粒度分类性能；预训练阶段（特别是语言模型权重未冻结时）对细粒度性能至关重要。

Conclusion: 这些发现为增强VLMs的细粒度视觉理解和以视觉为中心的能力提供了重要见解和方向。

Abstract: Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.

</details>


### [11] [A Single Image and Multimodality Is All You Need for Novel View Synthesis](https://arxiv.org/abs/2602.17909)
*Amirhosein Javadi,Chi-Shiang Gau,Konstantinos D. Polyzos,Tara Javidi*

Main category: cs.CV

TL;DR: 提出一种多模态深度重建框架，利用稀疏雷达/LiDAR测距数据生成稠密深度图，为基于扩散模型的新视角合成提供更可靠的几何条件，显著提升单图像新视角合成的几何一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的新视角合成方法依赖单目深度估计的几何信息，但在低纹理、恶劣天气和遮挡严重的真实场景中，深度估计的可靠性有限，影响合成视图的质量和一致性。

Method: 采用多模态深度重建框架，使用极稀疏的测距传感数据（如汽车雷达或LiDAR），在角度域中通过局部高斯过程建模深度，实现计算高效推理并显式量化观测有限区域的不确定性。

Result: 在真实多模态驾驶场景实验中，用稀疏测距重建深度替代纯视觉深度，显著提高了单图像新视角视频生成的几何一致性和视觉质量。

Conclusion: 可靠几何先验对基于扩散模型的视图合成至关重要，即使在极端稀疏条件下，多模态传感也能带来实际效益，无需修改生成模型本身即可作为现有管道的即插即用替代方案。

Abstract: Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.

</details>


### [12] [ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging](https://arxiv.org/abs/2602.17929)
*Athanasios Angelakis*

Main category: cs.CV

TL;DR: ZACH-ViT是一种紧凑型视觉变换器，移除了位置嵌入和[CLS]标记，通过全局平均池化实现排列不变性，在医学图像任务中表现出色，特别适用于空间布局信息较弱或不一致的场景。


<details>
  <summary>Details</summary>
Motivation: 传统Vision Transformer依赖位置嵌入和类别标记编码固定空间先验，这在医学成像和临床边缘系统中可能阻碍泛化能力，因为空间布局信息往往较弱或不一致。

Method: 移除位置嵌入和[CLS]标记，使用全局平均池化对补丁表示进行聚合，实现排列不变性。采用自适应残差投影保持训练稳定性，同时严格控制参数量。

Result: 在7个MedMNIST数据集上评估，ZACH-ViT（0.25M参数）在BloodMNIST上表现最佳，在PathMNIST上与TransMIL竞争，在具有强解剖先验的数据集上相对优势降低，验证了架构假设。

Conclusion: 将架构归纳偏置与数据结构对齐比追求通用基准主导更重要。ZACH-ViT在资源受限的临床环境中具有部署潜力，实现了亚秒级推理时间和竞争性性能。

Abstract: Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term "Zero-token" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.
  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.

</details>


### [13] [ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models](https://arxiv.org/abs/2602.17951)
*Guoheng Sun,Tingting Du,Kaixi Feng,Chenxiang Luo,Xingguo Ding,Zheyu Shen,Ziyao Wang,Yexiao He,Ang Li*

Main category: cs.CV

TL;DR: ROCKET是一个残差导向的多层表示对齐框架，通过共享投影器将VLA模型的多层特征与3D视觉基础模型对齐，减少梯度冲突，仅需约4%计算预算即可在LIBERO上达到98.5%的SOTA成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型基于2D数据预训练，缺乏3D空间理解；单层监督方法无法充分利用深度信息，而朴素的多层对齐会导致梯度干扰。

Method: 使用共享投影器通过层不变映射对齐VLA主干网络与3D视觉基础模型的多层特征，提出Matryoshka式稀疏激活方案平衡多个对齐损失，结合训练无关的层选择策略。

Result: 在LIBERO上仅需4%计算预算达到98.5%成功率，在LIBERO-Plus和RoboTwin等多个数据集和VLA模型上均表现优异。

Conclusion: ROCKET框架有效解决了多层表示对齐中的梯度冲突问题，显著提升了VLA模型的3D空间理解能力，计算效率高且泛化性能好。

Abstract: Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.

</details>


### [14] [Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching](https://arxiv.org/abs/2602.18000)
*Xuting Lan,Mingliang Zhou,Xuekai Wei,Jielu Yan,Yueting Huang,Huayan Pu,Jun Luo,Weijia Jia*

Main category: cs.CV

TL;DR: 提出记忆驱动的质量感知框架MQAF，通过建立存储失真模式的记忆库，在有无参考图像情况下动态切换双模式质量评估策略，减少对高质量参考图像的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有全参考图像质量评估方法依赖高质量参考图像，限制了实际应用。受人类视觉系统积累视觉记忆能力的启发，希望开发能够基于记忆存储进行质量评估的方法。

Method: 建立存储失真模式的记忆库，采用双模式策略：有参考时自适应加权参考信息并与记忆库失真模式比较；无参考时依赖记忆库失真模式推断图像质量。

Result: 实验结果表明，该方法在多个数据集上优于最先进方法，同时适应无参考和全参考任务。

Conclusion: MQAF框架通过模拟人类视觉记忆机制，有效减少了图像质量评估对参考图像的依赖，在两种评估模式下都取得了优异性能，具有实际应用价值。

Abstract: Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.

</details>


### [15] [MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method](https://arxiv.org/abs/2602.18006)
*Ahsan Baidar Bakht,Mohamad Alansari,Muhayy Ud Din,Muzammal Naseer,Sajid Javed,Irfan Hussain,Jiri Matas,Arif Mahmood*

Main category: cs.CV

TL;DR: MUOT_3M是首个伪多模态水下目标跟踪基准数据集，包含3030个视频的300万帧数据，具有RGB、增强RGB、深度和语言四种模态。基于此提出的MUTrack多模态到单模态跟踪器，通过知识蒸馏将多模态知识迁移到单模态学生模型中，在五个基准测试中达到SOTA性能，运行速度24FPS。


<details>
  <summary>Details</summary>
Motivation: 水下目标跟踪(UOT)对海洋机器人、生态监测和海洋探索至关重要，但进展受到大型、多模态、多样化数据集稀缺的限制。现有基准数据集规模小且仅含RGB模态，在严重颜色失真、浑浊和低能见度条件下鲁棒性不足。

Method: 构建MUOT_3M多模态基准数据集，包含RGB、估计增强RGB、估计深度和语言四种同步模态。提出MUTrack跟踪器，采用SAM基础，包含视觉几何对齐、视觉语言融合和四级知识蒸馏，将多模态知识迁移到单模态学生模型中。

Result: MUOT_3M包含300万帧、3030个视频(27.8小时)，标注了32个跟踪属性、677个细粒度类别。MUTrack在五个UOT基准测试中比最强SOTA基线AUC提升8.40%，精度提升7.80%，运行速度达到24FPS。

Conclusion: MUOT_3M和MUTrack为可扩展、多模态训练但实际可部署的水下跟踪建立了新的基础，解决了水下环境中的颜色失真、浑浊和低能见度挑战。

Abstract: Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.

</details>


### [16] [Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating](https://arxiv.org/abs/2602.18016)
*Jiamin Luo,Xuqian Gu,Jingjing Wang,Jiahong Lu*

Main category: cs.CV

TL;DR: 本文提出基于LLM的情感视觉定制任务(L-AVC)，开发了高效精确的情感操纵方法(EPEM)，通过EIC模块实现语义层面的情感转换，PER模块保持情感无关内容，在自建数据集上验证了方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定制研究主要关注客观信号对齐，忽略了主观情感内容，且缺乏针对情感视觉定制的通用基础模型。

Method: 提出EPEM方法：1) EIC模块使LLM高效对齐编辑前后的语义情感转换；2) PER模块精确保留情感无关内容。

Result: 在自建L-AVC数据集上的综合实验表明，EPEM方法在L-AVC任务上优于多个最先进基线方法。

Conclusion: 情感信息对L-AVC任务至关重要，EPEM方法能够高效精确地操纵情感信息，证明了方法的有效性。

Abstract: Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.

</details>


### [17] [DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE](https://arxiv.org/abs/2602.18019)
*Yujie Jin,Wenxin Zhang,Jingjing Wang,Guodong Zhou*

Main category: cs.CV

TL;DR: 该论文提出了深度安全导向视频理解(DeepSVU)新任务，不仅检测威胁还要分析成因，并提出了统一物理世界正则化MoE(UPRM)方法来解决建模和权衡物理世界信息的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有安全导向视频理解研究主要关注威胁检测和定位，缺乏对威胁成因生成和评估的有效能力，存在研究空白。

Method: 提出UPRM方法，包含统一物理世界增强MoE块(UPE)和物理世界权衡正则化器(PTR)两个核心组件，分别解决粗到细物理信息建模和自适应权衡问题。

Result: 在DeepSVU指令数据集(UCF-C和CUVA指令集)上的实验表明，UPRM优于多个先进的视频-LLM和非VLM方法。

Conclusion: 研究证实了粗到细物理世界信息在DeepSVU任务中的重要性，并证明了UPRM方法在捕获此类信息方面的有效性。

Abstract: In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.

</details>


### [18] [UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models](https://arxiv.org/abs/2602.18020)
*Jiabing Yang,Yixiang Chen,Yuan Xu,Peiyan Li,Xiangnan Wu,Zichen Wen,Bowen Fang,Tao Yu,Zhengbo Zhang,Yingda Li,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: UAOR是一种无需训练、即插即用的不确定性感知观察重注入模块，通过注意力检索机制在VLA模型推理过程中动态重注入关键观察信息，提高动作生成的置信度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA方法通常需要额外的观察线索（如深度图、点云）或辅助模块（如目标检测器）来提升性能，但这些方法需要昂贵的数据收集和额外训练。受到语言模型中前馈网络作为'键值记忆'的启发，作者希望开发一种无需训练即可提升VLA模型性能的方法。

Method: 当当前语言模型层表现出高不确定性（通过动作熵衡量）时，UAOR通过注意力检索机制将关键观察信息重新注入到下一层的前馈网络中。这种机制帮助VLA模型在推理过程中更好地关注观察信息。

Result: 综合实验表明，该方法在仿真和真实世界任务中持续提升了多种VLA模型的性能，且开销极小。UAOR无需额外的观察线索或模块，成为现有VLA管道的通用实用插件。

Conclusion: UAOR提供了一种有效、训练免费且即插即用的解决方案，能够显著提升VLA模型的性能，同时避免了传统方法所需的高成本和额外复杂性，为机器人操作任务提供了更实用的VLA模型增强方案。

Abstract: Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as "key-value memory", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.

</details>


### [19] [Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers](https://arxiv.org/abs/2602.18022)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: DCAG提出了一种无需训练的双通道注意力引导框架，同时操控DiT中的Key和Value空间，通过理论分析和实验验证在图像编辑的保真度-编辑效果权衡上优于仅使用Key空间的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于DiT的扩散图像编辑模型仅关注Key空间来调制注意力路由，而完全忽略了控制特征聚合的Value空间，这限制了编辑强度的无训练控制能力。

Method: 发现DiT多模态注意力层中Key和Value投影均呈现偏差-增量结构，提出DCAG框架同时操控Key通道（控制注意力位置）和Value通道（控制特征聚合），形成二维参数空间(δ_k, δ_v)进行精细调控。

Result: 在PIE-Bench基准测试（700张图像，10个编辑类别）中，DCAG在所有保真度指标上均优于仅使用Key引导的方法，在对象删除（LPIPS降低4.9%）和对象添加（LPIPS降低3.2%）等局部编辑任务中改进最显著。

Conclusion: Key通道通过非线性softmax函数提供粗粒度控制，Value通道通过线性加权求和提供细粒度补充，双通道协同工作能够实现比单通道方法更精确的编辑-保真度权衡。

Abstract: Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).

</details>


### [20] [Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition](https://arxiv.org/abs/2602.18043)
*Hongyu Qu,Xiangbo Shu,Rui Yan,Hailiang Gao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: DiST是一个创新的少样本动作识别框架，通过解耦大语言模型提供的空间和时间知识来学习表达性多粒度原型，在五个标准数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用语义粗糙的类别名称作为辅助上下文，但这类上下文过于有限，无法为捕获动作中的新颖空间和时间概念提供足够的背景知识。

Method: 提出分解-融合框架：1)分解阶段将动作名称解耦为多样化的时空属性描述；2)融合阶段通过空间/时间知识补偿器(SKC/TKC)分别发现判别性对象级和帧级原型，SKC在空间知识指导下自适应聚合重要补丁标记，TKC利用时间属性辅助帧间时间关系建模。

Result: 在五个标准少样本动作识别数据集上取得了最先进的结果。

Conclusion: DiST框架通过利用大语言模型提供的解耦空间和时间知识，能够学习表达性多粒度原型，有效捕获细粒度空间细节和多样化时间模式，为少样本动作识别提供了新的解决方案。

Abstract: Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.

</details>


### [21] [CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras](https://arxiv.org/abs/2602.18047)
*Rong Fu,Wenxin Zhang,Yibo Meng,Jia Yee Tan,Jiaxuan Lu,Rui Lu,Jiekai Wu,Zhaolu Kang,Simon Fong*

Main category: cs.CV

TL;DR: CityGuard是一个用于分散式监控中隐私保护身份检索的拓扑感知Transformer框架，通过分散自适应度量学习、空间条件注意力和差分隐私嵌入映射三个组件，在遵守数据保护规则的同时实现跨摄像头的人员重识别。


<details>
  <summary>Details</summary>
Motivation: 解决城市尺度下分布式摄像头人员重识别面临的视角变化、遮挡和域偏移等外观变化挑战，同时遵守禁止共享原始图像的数据保护规定。

Method: 1) 分散自适应度量学习根据特征分布调整实例级边界；2) 空间条件注意力将粗略几何信息注入图自注意力机制；3) 差分隐私嵌入映射与紧凑近似索引结合实现安全部署。

Result: 在Market-1501等基准测试中显示出检索精度和查询吞吐量的持续提升，验证了框架在隐私关键城市身份匹配中的实用性。

Conclusion: 该框架能够有效处理视角变化、遮挡和域偏移，在严格的差分隐私核算下实现隐私与效用的可调平衡，适用于隐私关键的城市身份匹配应用。

Abstract: City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.

</details>


### [22] [Temporal Consistency-Aware Text-to-Motion Generation](https://arxiv.org/abs/2602.18057)
*Hongsong Wang,Wenjing Yan,Qiuxia Lai,Xin Geng*

Main category: cs.CV

TL;DR: TCA-T2M：一种时间一致性感知的文本到运动生成框架，通过TCaS-VQ-VAE实现跨序列时间对齐，结合掩码运动transformer和运动学约束块，显著提升运动生成的语义对齐和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段文本到运动生成方法忽视跨序列时间一致性，导致语义错位和物理上不可行的运动，需要解决这一关键问题。

Method: 提出TCA-T2M框架：1）时间一致性感知空间VQ-VAE（TCaS-VQ-VAE）实现跨序列时间对齐；2）掩码运动transformer进行文本条件运动生成；3）运动学约束块减少离散化伪影确保物理合理性。

Result: 在HumanML3D和KIT-ML基准测试中达到最先进性能，证明了时间一致性在鲁棒和连贯文本到运动生成中的重要性。

Conclusion: TCA-T2M通过引入时间一致性机制有效解决了现有方法的局限性，为文本到运动生成提供了更准确和物理合理的解决方案。

Abstract: Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.

</details>


### [23] [3DMedAgent: Unified Perception-to-Understanding for 3D Medical Analysis](https://arxiv.org/abs/2602.18064)
*Ziyue Wang,Linghan Cai,Chang Han Low,Haofeng Liu,Junde Wu,Jingyu Wang,Rui Wang,Lei Song,Jiang Bian,Jingjing Fu,Yueming Jin*

Main category: cs.CV

TL;DR: 3DMedAgent是一个统一的代理框架，使2D多模态大语言模型能够无需3D特定微调即可执行通用3D CT分析，通过协调异构视觉和文本工具逐步分解复杂任务，在40多个任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D CT分析方法要么采用孤立的任务特定建模，要么使用任务无关的端到端范式，阻碍了感知证据的系统积累；同时，多模态大语言模型主要面向2D设计，限制了其在体积医学数据分析中的能力。

Method: 提出3DMedAgent框架，通过灵活的MLLM代理协调异构视觉和文本工具，将复杂3D分析逐步分解为可处理的子任务（从全局到局部视图，从3D体积到信息丰富的2D切片，从视觉证据到结构化文本表示），并维护长期结构化记忆来聚合中间工具输出。

Result: 在DeepChestVQA基准测试中，3DMedAgent在40多个任务中一致优于通用、医学和3D特定的MLLM方法，展示了在3D胸部成像中从感知到理解能力的统一评估。

Conclusion: 3DMedAgent为构建通用3D临床助手提供了一条可扩展的路径，成功地将2D MLLM的能力扩展到3D医学图像分析领域，无需专门的3D微调。

Abstract: 3D CT analysis spans a continuum from low-level perception to high-level clinical understanding. Existing 3D-oriented analysis methods adopt either isolated task-specific modeling or task-agnostic end-to-end paradigms to produce one-hop outputs, impeding the systematic accumulation of perceptual evidence for downstream reasoning. In parallel, recent multimodal large language models (MLLMs) exhibit improved visual perception and can integrate visual and textual information effectively, yet their predominantly 2D-oriented designs fundamentally limit their ability to perceive and analyze volumetric medical data. To bridge this gap, we propose 3DMedAgent, a unified agent that enables 2D MLLMs to perform general 3D CT analysis without 3D-specific fine-tuning. 3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent, progressively decomposing complex 3D analysis into tractable subtasks that transition from global to regional views, from 3D volumes to informative 2D slices, and from visual evidence to structured textual representations. Central to this design, 3DMedAgent maintains a long-term structured memory that aggregates intermediate tool outputs and supports query-adaptive, evidence-driven multi-step reasoning. We further introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging. Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs, highlighting a scalable path toward general-purpose 3D clinical assistants.Code and data are available at \href{https://github.com/jinlab-imvr/3DMedAgent}{https://github.com/jinlab-imvr/3DMedAgent}.

</details>


### [24] [Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation](https://arxiv.org/abs/2602.18066)
*Daniel Busch,Christian Bohn,Thomas Kurbiel,Klaus Friedrichs,Richard Meyes,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出两阶段训练策略，通过自监督预训练和半监督微调，减少对BEV标注数据的依赖，在nuScenes数据集上性能提升2.5pp mIoU，同时减少50%标注数据和2/3训练时间。


<details>
  <summary>Details</summary>
Motivation: 当前多摄像头BEV语义地图方法依赖昂贵且标注不一致的地面真值，需要解决标注成本高的问题。

Method: 两阶段训练：1）自监督预训练阶段，将BEVFormer预测结果可微重投影到图像平面，使用Mask2Former生成的多视角语义伪标签进行训练；2）监督微调阶段仅需50%数据集，加入时序一致性损失。

Result: 在nuScenes数据集上性能超越全监督基线2.5pp mIoU，标注数据使用量减半，总训练时间减少三分之二。

Conclusion: 可微重投影加相机视角伪标签可产生可迁移的BEV特征，为减少标注的自动驾驶感知提供了可扩展路径。

Abstract: Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.

</details>


### [25] [Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation](https://arxiv.org/abs/2602.18083)
*Ioannis Kontogiorgakis,Athanasios Askitopoulos,Iason Tsardanidis,Dimitrios Bormpoudakis,Ilias Tsoumas,Fotios Balampanis,Charalampos Kontoes*

Main category: cs.CV

TL;DR: 提出一个10米分辨率的高精度土壤湿度估算框架，结合Sentinel-1 SAR、Sentinel-2光学影像和ERA-5再分析数据，通过机器学习实现欧洲植被区域的农场级土壤湿度监测。


<details>
  <summary>Details</summary>
Motivation: 现有卫星土壤湿度产品分辨率过低（>1公里），无法满足农场级应用需求，需要开发高分辨率土壤湿度估算方法。

Method: 使用113个国际土壤湿度网络站点数据，结合Sentinel-1 SAR、Sentinel-2光学影像和ERA-5数据，通过空间交叉验证比较不同模态组合和时间参数化方法，并评估IBM-NASA Prithvi基础模型嵌入与传统手工特征的效果。

Result: 混合时间匹配方法（Sentinel-2当日获取+Sentinel-1下降轨道）达到R²=0.514，10天ERA5回溯窗口提升至R²=0.518。基础模型嵌入相比手工特征改进有限（R²=0.515 vs 0.514）。

Conclusion: 领域特定的光谱指数结合基于树的集成方法为泛欧洲田间尺度土壤湿度监测提供了实用且计算高效的解决方案，传统特征工程在稀疏数据回归任务中仍具竞争力。

Abstract: Accurate soil moisture (SM) estimation is critical for precision agriculture, water resources management and climate monitoring. Yet, existing satellite SM products are too coarse (>1km) for farm-level applications. We present a high-resolution (10m) SM estimation framework for vegetated areas across Europe, combining Sentinel-1 SAR, Sentinel-2 optical imagery and ERA-5 reanalysis data through machine learning. Using 113 International Soil Moisture Network (ISMN) stations spanning diverse vegetated areas, we compare modality combinations with temporal parameterizations, using spatial cross-validation, to ensure geographic generalization. We also evaluate whether foundation model embeddings from IBM-NASA's Prithvi model improve upon traditional hand-crafted spectral features. Results demonstrate that hybrid temporal matching - Sentinel-2 current-day acquisitions with Sentinel-1 descending orbit - achieves R^2=0.514, with 10-day ERA5 lookback window improving performance to R^2=0.518. Foundation model (Prithvi) embeddings provide negligible improvement over hand-crafted features (R^2=0.515 vs. 0.514), indicating traditional feature engineering remains highly competitive for sparse-data regression tasks. Our findings suggest that domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring.

</details>


### [26] [DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text](https://arxiv.org/abs/2602.18089)
*Kunwar Arpit Singh,Ankush Prakash,Haroon R Lone*

Main category: cs.CV

TL;DR: DohaScript是一个大规模、多书写者的印地语手写数据集，包含531位不同书写者抄写的六首传统印地语对句（dohas），旨在解决天城文手写文本在公开基准数据集中代表性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有天城文手写数据集规模有限，主要关注孤立字符或短词，缺乏受控词汇内容和书写者多样性，无法捕捉天城文手写的连续性、融合性和结构复杂性特征。

Method: 构建平行风格语料库，所有书写者转录相同的六首传统印地语对句，确保词汇内容一致性。包含去标识化的人口统计元数据，基于清晰度和分辨率标准进行严格质量筛选，并提供页面级布局难度标注。

Result: 数据集展示了明显的质量分离和对未见书写者的强泛化能力，验证了数据集的可靠性和实用价值。

Conclusion: DohaScript可作为标准化、可复现的基准数据集，推动低资源脚本环境下连续天城文手写文本的研究进展，支持手写识别、书写者识别、风格分析和生成建模等任务。

Abstract: Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.

</details>


### [27] [Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.18093)
*Hanshuai Cui,Zhiqing Tang,Qianli Ma,Zhi Yao,Weijia Jia*

Main category: cs.CV

TL;DR: PrediT是一个无需训练的DiT加速框架，通过线性多步方法预测未来模型输出，结合校正器和动态步长调制机制，在保持生成质量的同时实现高达5.54倍的延迟降低。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器(DiT)的迭代去噪过程计算成本高昂，现有基于特征缓存和重用的免训练加速方法会导致潜在漂移和视觉质量下降。观察到模型输出在扩散轨迹上平滑演化，需要更智能的预测方法而非简单重用。

Method: 将特征预测构建为线性多步问题，使用经典线性多步方法从历史信息预测未来输出；引入校正器在高动态区域激活防止误差累积；动态步长调制机制通过监测特征变化率自适应调整预测范围。

Result: 在多种基于DiT的图像和视频生成模型上实现了高达5.54倍的延迟降低，同时质量退化可忽略不计。

Conclusion: PrediT通过智能特征预测和自适应控制机制，成功解决了DiT加速中的潜在漂移问题，为扩散模型的高效推理提供了有效的免训练解决方案。

Abstract: Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.

</details>


### [28] [OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2602.18094)
*Ling Lin,Yang Bai,Heng Su,Congcong Zhu,Yaoxing Wang,Yang Zhou,Huazhu Fu,Jingrun Chen*

Main category: cs.CV

TL;DR: OODBench是一个自动化构建的基准测试，用于评估视觉语言模型在处理分布外数据时的性能表现，包含4万对实例-类别对，并提出了基本到高级的渐进式评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型通常在IID数据假设下训练，但现实场景中数据往往不满足这一假设，处理分布外数据不当可能带来安全风险，且缺乏全面评估VLMs处理OOD数据能力的基准。

Method: 提出OODBench方法，采用最小人工验证的自动化方式构建基准，包含40K实例级OOD实例-类别对，并设计基于基本到高级渐进提示问题的自动化评估指标。

Result: 实验表明当前VLMs在OODBench上表现出显著的性能下降，即使面对常见图像类别时也是如此，验证了基准的有效性和挑战性。

Conclusion: OODBench为评估VLMs处理分布外数据能力提供了有效基准，提出的评估方法和发现为未来OOD数据获取和评估研究提供了重要参考和洞察。

Abstract: Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.

</details>


### [29] [Evaluating Graphical Perception Capabilities of Vision Transformers](https://arxiv.org/abs/2602.18178)
*Poonam Poonam,Pere-Pau Vázquez,Timo Ropinski*

Main category: cs.CV

TL;DR: ViTs在可视化图形感知任务中与人类感知对齐度有限，尽管在通用视觉任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 评估ViTs在图形感知任务中的表现，填补ViTs在可视化领域感知能力的研究空白，与CNN和人类基准进行对比

Method: 基于Cleveland和McGill的基础研究，设计一系列受控图形感知任务，对ViTs、CNNs和人类参与者进行基准测试

Result: ViTs在通用视觉任务中表现强劲，但在可视化领域的类人图形感知能力存在明显差距

Conclusion: 研究揭示了ViTs在图形感知方面的关键局限性，为ViTs在可视化系统和图形感知建模中的应用提供了重要考量

Abstract: Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.

</details>


### [30] [BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards](https://arxiv.org/abs/2602.18193)
*Yiran Yang,Zhaowei Liu,Yuan Yuan,Yukun Song,Xiong Ma,Yinghao Song,Xiangji Zeng,Lu Sun,Yulu Wang,Hai Zhou,Shuai Cui,Zhaohan Gong,Jiefei Zhang*

Main category: cs.CV

TL;DR: BLM-Guard是一个用于短视频广告内容审核的框架，结合思维链推理、基于规则的政策原则和批判引导奖励，通过多任务架构检测模态内操作和跨模态不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 短视频平台上的多模态广告存在欺骗性视觉、语音和字幕内容，需要比社区安全过滤器更细粒度的政策驱动审核。

Method: 使用规则驱动的ICoT数据合成管道生成结构化场景描述、推理链和标签来启动训练，然后通过强化学习使用复合奖励（平衡因果一致性和政策遵循性）精炼模型，采用多任务架构建模模态内操作和跨模态不匹配。

Result: 在真实短视频广告上的实验显示，BLM-Guard在准确性、一致性和泛化能力方面优于强基线模型。

Conclusion: BLM-Guard框架有效解决了短视频多模态广告审核的挑战，通过结合规则驱动数据合成和强化学习优化，实现了高性能的内容审核系统。

Abstract: Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.

</details>


### [31] [A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion](https://arxiv.org/abs/2602.18199)
*Gahyeon Shim,Soogeun Park,Hyemin Ahn*

Main category: cs.CV

TL;DR: DMC是一个后处理模块，通过自监督数据驱动方法改进文本生成动作的物理合理性，同时保持语义一致性，可适配各种文本到动作生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成动作方法在语义对齐方面进展迅速，但难以同时保证语义和物理真实性（如脚部漂浮等问题）。

Method: 提出Distortion-aware Motion Calibrator (DMC)，采用自监督数据驱动方法，通过输入故意扭曲的动作和原始文本描述，学习生成物理合理的动作。

Result: DMC显著提升物理合理性：在T2M上FID降低42.74%，在T2M-GPT上降低13.20%，R-Precision最高；在MoMask上穿透减少33.0%，漂浮伪影更接近真实参考。

Conclusion: DMC可作为有前景的后处理运动优化框架，通过结合文本语义和物理合理性，适用于各类文本到动作生成模型。

Abstract: Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.

</details>


### [32] [On the Adversarial Robustness of Discrete Image Tokenizers](https://arxiv.org/abs/2602.18252)
*Rishika Bhagwatkar,Irina Rish,Nicolas Flammarion,Francesco Croce*

Main category: cs.CV

TL;DR: 首项研究离散图像分词器的对抗攻击脆弱性，提出高效应用无关的攻击方法，并通过无监督对抗训练提升分词器鲁棒性，无需标注数据即可有效防御攻击并泛化到未见任务。


<details>
  <summary>Details</summary>
Motivation: 离散图像分词器在多模态系统中日益流行，但其对抗攻击脆弱性尚未被研究，而CLIP编码器的类似研究已存在，需要填补这一空白并提升多模态基础模型的安全性。

Method: 1. 制定针对离散分词器的特征扰动攻击，改变提取的token序列；2. 受鲁棒CLIP编码器研究启发，采用无监督对抗训练微调流行分词器，保持其他组件冻结。

Result: 攻击方法在分类、多模态检索和字幕生成任务中均有效且计算高效；防御方法显著提升对无监督和端到端监督攻击的鲁棒性，并能良好泛化到未见任务和数据。

Conclusion: 研究强调了分词器鲁棒性在下游任务中的关键作用，为开发安全的多模态基础模型迈出了重要一步，无监督方法比监督对抗训练更具通用性。

Abstract: Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.

</details>


### [33] [DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control](https://arxiv.org/abs/2602.18282)
*Shiyan Du,Conghan Yue,Xinyu Cheng,Dongyu Zhang*

Main category: cs.CV

TL;DR: DEIG是一个用于细粒度可控多实例生成的新框架，通过实例细节提取器和细节融合模块解决现有方法在复杂文本描述下的语义理解挑战，在空间一致性、语义准确性和组合泛化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多实例生成方法在空间布局和属性绑定方面已有显著进展，但在处理复杂文本描述时仍面临细粒度语义理解的挑战，特别是存在跨实例属性泄露问题。

Method: 提出DEIG框架，包含实例细节提取器（IDE）将文本编码器嵌入转换为紧凑的实例感知表示，以及细节融合模块（DFM）应用基于实例的掩码注意力机制防止跨实例属性泄露。构建了由视觉语言模型生成的高质量细粒度监督数据集和DEIG-Bench基准测试。

Result: 实验表明DEIG在多个基准测试中在空间一致性、语义准确性和组合泛化方面一致优于现有方法，且可作为即插即用模块集成到标准扩散管道中。

Conclusion: DEIG通过创新的实例细节提取和融合机制，有效解决了多实例生成中的细粒度语义理解和属性控制问题，为复杂场景的文本到图像生成提供了有效的解决方案。

Abstract: Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.

</details>


### [34] [Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation](https://arxiv.org/abs/2602.18309)
*Ziyue Liu,Davide Talon,Federico Girella,Zanxi Ruan,Mattia Mondo,Loris Bazzani,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: LOTS是一个结合文本和草图的多模态时尚图像生成框架，通过全局草图引导和多层次局部草图-文本对来增强生成质量，在保持全局结构一致性的同时利用丰富的局部语义指导。


<details>
  <summary>Details</summary>
Motivation: 草图提供了时尚设计的结构和轮廓信息，而文本描述补充了材质、颜色和风格细节。需要一种方法能够有效结合这两种模态，在利用文本局部属性指导的同时保持草图视觉结构的一致性。

Method: 提出LOTS框架，包含多级条件编码阶段（独立编码局部特征并保持全局协调）和扩散对引导阶段（通过注意力机制在去噪过程中整合局部和全局条件）。创建了首个多文本-草图对时尚数据集Sketchy。

Result: 实验表明该方法在保持全局结构一致性的同时，能够利用更丰富的局部语义指导，相比现有技术有所改进。

Conclusion: LOTS框架成功实现了文本和草图模态的有效结合，在时尚图像生成任务中取得了优异性能，数据集和代码已公开。

Abstract: Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an "in the wild" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.

</details>


### [35] [Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting](https://arxiv.org/abs/2602.18314)
*Tianyi Song,Danail Stoyanov,Evangelos Mazomenos,Francisco Vasconcelos*

Main category: cs.CV

TL;DR: Diff2DGS是一个用于手术场景三维重建的两阶段框架，第一阶段使用扩散模型修复被器械遮挡的组织，第二阶段采用2D高斯泼溅和可学习变形模型来捕捉组织变形和几何结构，在图像质量和深度精度方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的手术场景重建方法在遮挡区域的重建质量有限，且缺乏深度精度的全面评估，因为现有基准数据集缺乏3D真值数据。

Method: 两阶段方法：1) 基于扩散的视频修复模块处理器械遮挡；2) 结合2D高斯泼溅和可学习变形模型(2DGS+LDM)捕捉动态组织变形和几何结构。

Result: 在EndoNeRF上达到38.02 dB PSNR，在StereoMIS上达到34.40 dB PSNR，在SCARED数据集上的深度精度分析显示优于现有方法。

Conclusion: 仅优化图像质量不一定能获得最佳3D重建精度，Diff2DGS通过同时优化深度质量，在保持高保真外观的同时实现了更准确的几何重建。

Abstract: Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.

</details>


### [36] [Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis](https://arxiv.org/abs/2602.18322)
*Ziteng Cui,Shuhong Liu,Xiaoyu Dong,Xuangeng Chu,Lin Gu,Ming-Hsuan Yang,Tatsuya Harada*

Main category: cs.CV

TL;DR: Luminance-GS++是一个基于3D高斯泼溅的框架，通过全局视图自适应亮度调整和局部像素级残差精炼来解决多视角捕获中的光照不一致问题，保持实时渲染效率的同时提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 真实环境中的复杂光照变化和相机成像管道限制导致多视角捕获存在光度不一致问题，这违背了现代3D新视角合成方法的光度一致性假设，导致重建和渲染质量下降。

Method: 结合全局视图自适应亮度调整和局部像素级残差精炼进行精确色彩校正，设计无监督目标联合强制执行亮度校正以及多视角几何和光度一致性，保持显式3DGS公式不变。

Result: 在低光照、过曝和复杂亮度色彩变化等挑战性场景中实现了最先进的性能表现。

Conclusion: 该方法有效解决了多视角光照不一致问题，在保持3DGS实时渲染效率的同时显著提升了重建保真度，为复杂光照条件下的新视角合成提供了鲁棒解决方案。

Abstract: High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.

</details>


### [37] [G-LoG Bi-filtration for Medical Image Classification](https://arxiv.org/abs/2602.18329)
*Qingsong Wang,Jiaxing He,Bingzhe Hou,Tieru Wu,Yang Cao,Cailing Yao*

Main category: cs.CV

TL;DR: 本文提出G-LoG双过滤方法，通过高斯-拉普拉斯算子增强医学图像边界特征，构建适合多参数持久性模块的双过滤，在MedMNIST数据集上验证其优于单参数过滤，且MLP模型使用拓扑特征能达到复杂深度学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 在拓扑数据分析中，构建实用的对象过滤以检测拓扑和几何特征至关重要。医学图像边界信息对特征提取很重要，但现有单参数过滤方法可能不够充分。

Method: 利用高斯-拉普拉斯算子增强医学图像边界，定义G-LoG双过滤生成更适合多参数持久性模块的特征。将体积图像建模为有界函数，证明持久性模块的interleaving距离对有界函数的最大范数是稳定的。

Result: 在MedMNIST数据集上的实验表明，双过滤显著优于单参数过滤。使用双过滤生成的拓扑特征训练的简单MLP模型，其性能可与在原始数据集上训练的复杂深度学习模型（Google AutoML Vision、ResNet、AutoKeras、auto-sklearn）相媲美。

Conclusion: G-LoG双过滤方法有效提升了医学图像拓扑特征提取能力，证明了多参数持久性方法在医学图像分析中的优势，为拓扑数据分析在医疗影像领域的应用提供了新思路。

Abstract: Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.

</details>


### [38] [Self-Aware Object Detection via Degradation Manifolds](https://arxiv.org/abs/2602.18394)
*Stefan Becker,Simon Weiss,Wolfgang Hübner,Michael Arens*

Main category: cs.CV

TL;DR: 提出基于退化流形的退化感知自感知目标检测框架，通过对比学习在特征空间中显式组织图像退化信息，无需退化标签即可评估输入是否在检测器正常工作范围内。


<details>
  <summary>Details</summary>
Motivation: 目标检测器在正常成像条件下表现良好，但在模糊、噪声、压缩、恶劣天气或分辨率变化等退化条件下可能无声失效。安全关键应用中需要能够评估输入是否处于检测器正常工作状态的能力。

Method: 在标准检测骨干网络上添加轻量级嵌入头，通过多层对比学习训练：相同退化组成的图像在特征空间中靠近，不同退化配置的图像远离，形成几何组织的退化表示。从干净训练嵌入中估计原始原型作为参考点。

Result: 在合成损坏基准、跨数据集零样本迁移和自然天气引起的分布偏移实验中表现出良好的原始-退化可分性，在多种检测器架构中表现一致，在语义偏移下具有鲁棒泛化能力。

Conclusion: 退化感知表示几何为自感知目标检测提供了实用且与检测器无关的基础，通过几何偏离参考点提供独立于检测置信度的退化诱导偏移信号。

Abstract: Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.
  We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.
  To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.
  Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.

</details>


### [39] [Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges](https://arxiv.org/abs/2602.18406)
*Minh Dinh,Stéphane Deny*

Main category: cs.CV

TL;DR: 该研究展示了如何利用学习等变算子的架构成功处理训练中罕见对称变换的分布外分类问题，克服了传统网络和等变网络的局限性，但指出了在复杂数据集上扩展这些架构面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习在计算机视觉中虽成功，但在处理训练中罕见的群对称变换（如异常姿态、尺度、位置等）时仍存在困难。等变神经网络需要先验知识，而本研究探索从对称变换示例中学习等变算子的替代方案。

Method: 使用旋转和平移的噪声MNIST简单数据集，研究如何利用学习等变算子的架构来处理分布外分类问题。

Result: 该架构成功实现了对对称变换的分布外分类，证明了其克服传统网络和等变网络局限性的有效性。

Conclusion: 虽然概念上吸引人，但将这类架构扩展到更复杂数据集仍面临挑战，需要进一步研究解决。

Abstract: Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.

</details>


### [40] [Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control](https://arxiv.org/abs/2602.18422)
*Linxi Xie,Lisong C. Sun,Ashley Neall,Tong Wu,Shengqu Cai,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 开发了一个基于头部和手部姿态控制的人体中心视频世界模型，通过扩散变换器实现精确的3D控制，能够生成沉浸式第一人称虚拟环境，并在用户测试中显示出比基线更好的任务表现和控制感。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型仅接受文本或键盘等粗略控制信号，无法满足扩展现实(XR)中对用户真实世界运动跟踪的响应需求，限制了在具身交互中的实用性。

Method: 评估现有扩散变换器条件策略，提出有效的3D头部和手部控制机制，训练双向视频扩散模型教师，并蒸馏为因果交互系统生成第一人称虚拟环境。

Result: 通过人类受试者评估显示，相比相关基线，该系统在任务表现上有改进，且用户感知到的动作控制水平显著更高。

Conclusion: 该人体中心视频世界模型成功实现了对用户头手姿态的精确响应，为扩展现实应用提供了更自然的交互方式，提升了用户体验和控制感。

Abstract: Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.

</details>


### [41] [CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation](https://arxiv.org/abs/2602.18424)
*Xia Su,Ruiqi Chen,Benlin Liu,Jingwei Ma,Zonglin Di,Ranjay Krishna,Jon Froehlich*

Main category: cs.CV

TL;DR: CapNav是一个评估视觉语言模型在考虑智能体物理能力约束下进行室内导航的新基准，包含5种典型智能体、45个场景、473个导航任务和2365个QA对，测试结果显示当前VLM在移动约束增强时性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现实世界导航受智能体移动能力约束（如扫地机器人不能上楼梯，四足机器人可以），但现有视觉语言导航研究未充分考虑这种能力条件限制。

Method: 提出CapNav基准，定义5种代表性人形和机器人智能体，每个智能体都有详细的物理尺寸、移动能力和环境交互能力描述，使用45个真实室内场景和473个导航任务来测试VLM的能力感知导航性能。

Result: 评估13个现代VLM发现：随着移动约束增强，导航性能急剧下降；即使是SOTA模型在处理需要空间维度推理的障碍物类型时也表现不佳。

Conclusion: 强调了能力感知导航的重要性，为未来VLM在具身空间推理方面的发展提供了机遇和方向，相关基准已开源。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav

</details>


### [42] [SARAH: Spatially Aware Real-time Agentic Humans](https://arxiv.org/abs/2602.18432)
*Evonne Ng,Siwei Zhang,Zhang Chen,Michael Zollhoefer,Alexander Richard*

Main category: cs.CV

TL;DR: 提出首个实时、完全因果的空间感知对话动作生成方法，可在VR头显上实时部署，结合用户位置和音频生成全身动作，实现300+FPS的实时性能


<details>
  <summary>Details</summary>
Motivation: 当前虚拟化身方法缺乏空间感知能力，需要能够转向用户、响应用户动作并保持自然凝视的能力

Method: 结合因果Transformer VAE与流匹配模型，使用交错潜在token进行流式推理，并引入基于分类器自由引导的凝视评分机制来解耦学习与控制

Result: 在Embody 3D数据集上达到最先进的动作质量，比非因果基线快3倍，能够捕捉自然对话的细微空间动态

Conclusion: 该方法成功实现了实时空间感知对话代理，为VR、远程呈现和数字人应用提供了实用的解决方案

Abstract: As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.

</details>


### [43] [Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory](https://arxiv.org/abs/2602.18434)
*Vatsal Agarwal,Saksham Suri,Matthew Gwilliam,Pulkit Kumar,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: MemStream通过扩大token预算、自适应选择策略和无训练检索专家混合方法，显著提升了流式视频问答性能，在多个基准测试上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有流式视频理解方法使用有限的每帧token导致细粒度视觉细节丢失，且特征编码导致查询-帧相似度随时间增加，偏向检索后期帧。

Method: 提出扩大token预算实现更细粒度时空理解；引入自适应选择策略减少token冗余同时保留局部时空信息；采用无训练检索专家混合方法利用外部模型更好识别相关帧。

Result: 在CG-Bench上提升8.0%，LVBench上提升8.5%，VideoMME (Long)上提升2.4%，相比ReKV with Qwen2.5-VL-7B。

Conclusion: MemStream通过改进token分配策略和检索机制，有效解决了流式视频理解中的信息丢失和检索偏差问题，显著提升了视频问答性能。

Abstract: Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [44] [QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration](https://arxiv.org/abs/2602.17784)
*Meng Ye,Xiao Lin,Georgina Lukoczki,Graham W. Lederer,Yi Yao*

Main category: cs.CL

TL;DR: QueryPlot是一个基于语义检索的矿产远景预测框架，通过自然语言处理技术整合地质文本和地图数据，实现自动化、交互式的矿产远景分析。


<details>
  <summary>Details</summary>
Motivation: 传统矿产远景制图需要人工整合异构地质知识，过程繁琐且知识密集。需要开发自动化工具来高效处理文本矿床模型和地理空间数据。

Method: 使用预训练嵌入模型编码120多种矿床类型的描述性模型和地质地图多边形文本表示，通过语义相似度计算对区域进行排序和空间可视化，支持组合查询和多标准分析。

Result: 在钨矽卡岩矿床案例研究中，基于嵌入的检索方法实现了已知矿床的高召回率，生成的远景区域与专家定义区域高度吻合，相似度得分可提升监督学习分类性能。

Conclusion: QueryPlot成功实现了地质文本与空间数据的语义集成，为矿产勘探提供了有效的自动化工具，支持交互式查询和GIS兼容输出，代码和数据已开源。

Abstract: Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.

</details>


### [45] [Neural Synchrony Between Socially Interacting Language Models](https://arxiv.org/abs/2602.17815)
*Zhining Zhang,Wentao Zhu,Chi Han,Yizhou Wang,Heng Ji*

Main category: cs.CL

TL;DR: 该研究探索了多LLM系统在社交互动中的神经同步现象，将其作为分析LLM社交性的新指标，发现神经同步与LLM社交表现高度相关，揭示了LLM与人类社交互动内部动态的惊人相似性。


<details>
  <summary>Details</summary>
Motivation: 传统上认为社交心智是生物体的专属特性，虽然LLM被广泛认为是人类行为的强大近似，但LLM是否能与人类社交心智进行有意义的比较仍存在争议。研究旨在通过神经同步现象为这一争论提供实证证据。

Method: 引入社交模拟中的神经同步作为分析LLM社交性的新代理指标，通过精心设计的实验验证该指标能可靠反映LLM交互中的社交参与度和时间对齐性。

Result: 研究发现LLM之间的神经同步与其社交表现强烈相关，神经同步能有效反映LLM的社交参与程度和时间协调能力。

Conclusion: 神经同步为检验LLM的"社交心智"提供了新视角，揭示了LLM与人类社交互动在内部动态机制上的显著相似性，为理解LLM的社交能力提供了重要实证基础。

Abstract: Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the "social minds" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.

</details>


### [46] [On the scaling relationship between cloze probabilities and language model next-token prediction](https://arxiv.org/abs/2602.17848)
*Cassandra L. Jacobs,Morgan Grobol*

Main category: cs.CL

TL;DR: 大规模语言模型在预测眼动和阅读时间数据方面表现更好，虽然所有模型都低估人类反应概率，但大模型在完形填空任务中能提供更高质量的下一个词概率估计，因为其对词汇共现统计的敏感性降低，而与人类语义反应更加对齐。


<details>
  <summary>Details</summary>
Motivation: 研究大规模语言模型在语言处理预测任务中的表现差异，特别是探索模型规模如何影响其对人类语言处理行为的预测能力。

Method: 通过比较不同规模的语言模型在眼动数据、阅读时间数据和完形填空任务中的预测性能，分析模型对人类语言处理行为的预测质量。

Result: 大模型在完形填空任务中表现出更好的预测性能，能够提供更准确的下一词概率估计，但对低层次词汇信息的敏感性降低。所有模型都系统性地低估人类反应的概率。

Conclusion: 大模型的更大记忆容量使其能够猜测更语义合适的词汇，但同时使其对词汇识别相关的低层次信息敏感性降低，这支持了大模型在语义处理方面的优势但存在低层次信息处理局限性的观点。

Abstract: Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.

</details>


### [47] [Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations](https://arxiv.org/abs/2602.17881)
*Joschka Braun*

Main category: cs.CL

TL;DR: 论文研究了语言模型导向向量的可靠性问题，发现训练激活差异的余弦相似度越高、正负激活在导向方向上分离度越好，导向效果越可靠。不同提示变体训练的导向向量方向不同但效果相似，表明当潜在行为表征无法被线性导向方向有效近似时导向会不可靠。


<details>
  <summary>Details</summary>
Motivation: 导向向量虽然平均有效，但效果在不同样本间变化很大，对许多目标行为不可靠。研究旨在探究导向可靠性差异的原因及其与训练数据的关系。

Method: 分析导向向量训练数据中的激活差异，测量余弦相似度和正负激活在导向方向上的分离度，比较不同提示变体训练的导向向量性能。

Result: 发现余弦相似度高预测更可靠的导向；正负激活分离度好的数据集导向更可靠；不同提示训练的导向向量方向不同但效果相似且跨数据集效果相关。

Conclusion: 导向不可靠的原因是潜在行为表征无法被线性导向方向有效近似，研究结果为诊断导向不可靠性提供了实用方法，并激励开发考虑非线性潜在行为表征的更鲁棒导向方法。

Abstract: Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.

</details>


### [48] [Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions](https://arxiv.org/abs/2602.17907)
*Raymond Li,Amirhossein Abaskohi,Chuyuan Li,Gabriel Murray,Giuseppe Carenini*

Main category: cs.CL

TL;DR: 提出了一种基于语言模型生成语义软标签的新颖神经主题模型方法，通过投影下一个词概率到预定义词汇表来获得上下文丰富的监督信号，显著提升了主题质量和语义对齐能力。


<details>
  <summary>Details</summary>
Motivation: 传统神经主题模型仅重构词袋表示，忽略了上下文信息且面临数据稀疏问题，需要更有效的监督信号来提升主题质量。

Method: 使用语言模型根据特定提示生成下一个词的概率分布，投影到预定义词汇表创建语义软标签，然后训练主题模型基于LM隐藏状态重构这些软标签。

Result: 在三个数据集上的实验显示，该方法在主题连贯性和纯度方面显著优于现有基线，检索指标也表明在识别语义相似文档方面表现更优。

Conclusion: 该方法通过语言模型提供的上下文丰富监督信号，有效提升了主题模型的质量和语义理解能力，特别适用于检索导向的应用场景。

Abstract: Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.

</details>


### [49] [Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering](https://arxiv.org/abs/2602.17911)
*Jash Rajesh Parekh,Wonbin Kweon,Joey Chan,Rezarta Islamaj,Robert Leaman,Pengcheng Jiang,Chih-Hsuan Wei,Zhizheng Wang,Zhiyong Lu,Jiawei Han*

Main category: cs.CL

TL;DR: 提出CondMedQA首个条件生物医学问答基准和Condition-Gated Reasoning框架，解决现有方法无法处理临床条件推理的问题


<details>
  <summary>Details</summary>
Motivation: 现有生物医学QA系统假设医学知识普遍适用，但真实临床决策高度依赖患者特定条件，现有基准缺乏对此类条件推理的评估

Method: 构建CondMedQA基准测试集，提出Condition-Gated Reasoning框架，通过条件感知知识图谱和选择性激活/剪枝推理路径来实现条件敏感推理

Result: CGR框架在条件适应性答案选择方面表现更可靠，同时在生物医学QA基准上达到或超过最先进性能

Conclusion: 显式建模条件性对于稳健医学推理至关重要，CGR框架为条件生物医学问答提供了有效解决方案

Abstract: Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.

</details>


### [50] [Analyzing LLM Instruction Optimization for Tabular Fact Verification](https://arxiv.org/abs/2602.17937)
*Xiaotang Du,Giwon Hong,Wai-Chung Kwan,Rohit Saxena,Ivan Titov,Pasquale Minervini,Emily Allaway*

Main category: cs.CL

TL;DR: 系统比较了基于DSPy框架的指令优化方法在表格事实核查任务中的表现，评估了四种提示技术和三种优化器在多个基准和模型家族上的效果，发现指令优化能持续提升验证准确率，不同优化器对不同提示技术有差异化效果。


<details>
  <summary>Details</summary>
Motivation: 指令优化为大语言模型提供了一种轻量级、模型无关的推理性能增强方法，但缺乏在表格事实核查这一特定任务上的系统性比较研究。

Method: 使用DSPy优化框架，评估四种提示技术（直接预测、思维链、带SQL工具的ReAct、带Python执行的CodeAct）和三种优化器（COPRO、MiPROv2、SIMBA），在四个基准测试和三个模型家族上进行实验。

Result: 指令优化持续提升验证准确率：MiPROv2在思维链上表现最稳定，SIMBA在ReAct智能体上收益最大（尤其在大模型上）；SIMBA通过启发式方法鼓励更直接的推理路径，提升数值比较能力并减少不必要的工具调用。

Conclusion: 思维链在表格事实核查中保持有效（尤其对小模型），大模型构建的ReAct智能体虽能达到竞争性性能但需要仔细的指令优化，SIMBA优化器在复杂推理任务中表现出色。

Abstract: Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.

</details>


### [51] [CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications](https://arxiv.org/abs/2602.17949)
*Victoria Blake,Mathew Miller,Jamie Novak,Sze-yuan Ooi,Blanca Gallego*

Main category: cs.CL

TL;DR: CUICurate是一个基于图检索增强生成(GraphRAG)的自动化UMLS概念集管理框架，通过整合知识图谱检索和LLM推理，显著减少了临床概念集构建的人工工作量，在保持精度的同时提高了召回率。


<details>
  <summary>Details</summary>
Motivation: 临床命名实体识别工具通常将自由文本映射到UMLS CUI，但临床有意义的单位是包含相关同义词、子类型和超类型的概念集。构建此类概念集劳动密集、执行不一致且现有工具支持不足。

Method: 构建UMLS知识图谱并进行语义嵌入检索。对每个目标概念，从知识图谱检索候选CUI，然后使用LLM（GPT-5和GPT-5-mini）进行过滤和分类，并与人工标注基准进行比较评估。

Result: CUICurate在所有概念上产生了比人工基准更大更完整的概念集，同时保持了人类水平的精度。GPT-5-mini在过滤阶段召回率更高，GPT-5的分类更接近临床医生判断。输出稳定且计算成本低。

Conclusion: 该框架提供了一个可扩展且可重复的方法来支持UMLS概念集管理，通过图检索与LLM推理的整合，为不同表型和分析需求的临床NLP管道生成聚焦的候选概念集。

Abstract: Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.

</details>


### [52] [Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering](https://arxiv.org/abs/2602.17981)
*Amine Kobeissi,Philippe Langlais*

Main category: cs.CL

TL;DR: 研究分析了金融问答系统中检索增强生成的失败模式，重点关注正确文档被检索但答案所在页面或段落被遗漏的问题。通过多粒度检索评估和领域微调的页面评分器，显著提升了页面召回率和段落检索性能。


<details>
  <summary>Details</summary>
Motivation: 在金融监管文件问答中，虽然检索增强生成技术被广泛使用，但可靠性取决于能否检索到确切的上下文来支持高风险环境中的答案。研究发现常见失败模式是检索到正确文档但遗漏了包含答案的具体页面或段落，导致生成器基于不完整上下文进行推断。

Method: 在FinanceBench的150个问题子集上，评估了文档、页面和段落三个粒度的检索性能，包括密集检索、稀疏检索、混合检索和分层方法等多种策略。引入了基于预言机的分析来提供检索和生成性能的经验上限。提出了领域微调的页面评分器，将页面作为文档和段落之间的中间检索单元。

Result: 研究显示，文档发现性能的提升通常转化为更强的页面召回率，但预言机性能表明页面和段落级检索仍有改进空间。领域微调的页面评分器显著改善了页面召回率和段落检索性能，利用了页面的语义连贯性优势。

Conclusion: 通过引入专门针对金融文件页面级相关性微调的双编码器，可以有效解决文档内检索失败的问题，显著提升金融问答系统的检索准确性和可靠性，为高风险金融环境中的检索增强生成提供了重要改进方案。

Abstract: Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.

</details>


### [53] [Towards More Standardized AI Evaluation: From Models to Agents](https://arxiv.org/abs/2602.18029)
*Ali El Filali,Inès Bedar*

Main category: cs.CL

TL;DR: 本文批判了传统机器学习评估方法在AI系统向复合型工具使用智能体演进过程中的局限性，主张评估应从静态性能测试转变为动态信任建立和系统控制的核心功能。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从静态模型发展为复合型工具使用智能体，传统基于静态基准测试和聚合分数的评估方法已无法有效评估系统在变化环境中的可信行为，需要重新定义评估在AI时代的作用。

Method: 通过分析评估流程本身引入的隐性故障模式，揭示高基准分数误导团队的原因，并探讨智能体系统如何从根本上改变性能测量的意义。

Result: 识别出现有评估方法在智能体系统中的局限性，强调评估不应作为性能表演，而应成为在非确定性系统中建立信任、迭代和治理的测量学科。

Conclusion: 评估需要从模型中心的静态检查点转变为系统级的持续控制功能，重点关注系统在变化环境中的可信行为，而非仅仅追求基准测试的高分。

Abstract: Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer "How good is the model?" but "Can we trust the system to behave as intended, under change, at scale?". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.

</details>


### [54] [Perceived Political Bias in LLMs Reduces Persuasive Abilities](https://arxiv.org/abs/2602.18092)
*Matthew DiGiuseppe,Joshua Robison*

Main category: cs.CL

TL;DR: 研究表明，当用户被告知ChatGPT对其所属政党存在偏见时，AI对话系统的说服效果会下降28%，表明AI说服力受到政治立场感知的显著影响


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型进入党派冲突领域，精英阶层越来越多地将其描绘为具有意识形态倾向。研究旨在测试这些可信度攻击是否会降低基于LLM的说服效果

Method: 在美国进行了一项预注册调查实验（N=2144），参与者与ChatGPT进行三轮对话，讨论其个人持有的经济政策误解。实验组收到简短消息提示LLM对其政党存在偏见，对照组为中性信息

Result: 与中性对照组相比，被告知LLM存在党派偏见的参与者的说服效果降低了28%。转录分析显示警告改变了互动方式：受访者更频繁地反驳且接受度降低

Conclusion: 对话AI的说服效果具有政治条件性，受到党派一致性感知的限制。AI系统的政治中立性感知对其纠正公众误解和传播准确信息的能力至关重要

Abstract: Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.

</details>


### [55] [Agentic Adversarial QA for Improving Domain-Specific LLMs](https://arxiv.org/abs/2602.18137)
*Vincent Grari,Ciprian Tomoiaga,Sylvain Lamprier,Tatsunori Hashimoto,Marcin Detyniecki*

Main category: cs.CL

TL;DR: 提出了一种对抗性问答生成框架，通过比较待适应模型与基于参考文档的专家模型的输出，生成紧凑且具有语义挑战性的问题集合，显著提高了专业领域LLM微调的样本效率。


<details>
  <summary>Details</summary>
Motivation: LLM在专业领域适应方面面临高质量任务相关数据稀缺和覆盖范围有限的问题，现有合成数据生成方法虽然擅长事实回忆和概念知识，但缺乏对解释性推理能力的支持，且生成的语料库通常过于庞大冗余，样本效率低下。

Method: 采用对抗性问答生成框架，通过迭代反馈驱动过程比较待适应模型和基于参考文档的专家模型的输出，识别理解差距并生成语义挑战性问题。

Result: 在LegalBench语料库的专业子集上评估显示，该方法使用显著更少的合成样本实现了更高的准确性。

Conclusion: 该方法有效解决了专业领域LLM微调中的数据稀缺和样本效率问题，通过生成紧凑且具有挑战性的问题集合，提升了模型的解释性推理能力和适应性。

Abstract: Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.

</details>


### [56] [Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention](https://arxiv.org/abs/2602.18145)
*Siya Qi,Yudong Chen,Runcong Zhao,Qinglin Zhu,Zhanghao Hu,Wei Liu,Yulan He,Zheng Yuan,Lin Gui*

Main category: cs.CL

TL;DR: 基于信号处理视角，通过分析生成过程中注意力分布的高频成分来检测大语言模型的幻觉现象，提出了一种轻量级检测器并在多个基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的幻觉检测方法通常依赖粗粒度总结，无法捕捉注意力中的细粒度不稳定性，需要更精细的分析方法

Method: 将注意力分布建模为离散信号，提取反映注意力快速局部变化的高频成分，基于高频注意力能量特征构建轻量级检测器

Result: 在RAGTruth和HalluRAG基准测试中，该方法在多个模型和任务上优于基于验证、内部表示和注意力的现有方法

Conclusion: 注意力分布的高频成分能有效反映幻觉现象中的碎片化和不稳定接地行为，为幻觉检测提供了新的有效信号源

Abstract: Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.

</details>


### [57] [The Statistical Signature of LLMs](https://arxiv.org/abs/2602.18152)
*Ortal Hadad,Edoardo Loru,Jacopo Nudo,Niccolò Di Marco,Matteo Cinelli,Walter Quattrociocchi*

Main category: cs.CL

TL;DR: 该研究通过无损压缩技术分析LLM生成文本的结构统计特征，发现在受控和中介环境中LLM生成文本比人类文本具有更高的结构规律性和可压缩性，但在碎片化交互环境中这种差异会减弱。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示大型语言模型通过概率采样生成文本时如何重塑语言的结构统计组织，当前对此过程的理解尚不完整。

Method: 使用无损压缩作为模型无关的统计规律性度量方法，分析了三个渐进复杂的信息生态系统：受控人机续写、知识基础设施生成中介（维基百科vs Grokipedia）和完全合成的社交互动环境（Moltbook vs Reddit）。

Result: 压缩分析揭示了概率生成的结构特征：在受控和中介环境中，LLM生成语言表现出比人类文本更高的结构规律性和可压缩性；但在碎片化交互环境中，这种分离减弱，表明小尺度下表面可区分性存在基本限制。

Conclusion: 研究提供了一个简单而稳健的框架，用于量化生成系统如何重塑文本生产，为通信复杂性演变提供了结构视角，该方法可在不依赖模型内部结构或语义评估的情况下直接从表面文本观察到可压缩性差异。

Abstract: Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.

</details>


### [58] [FENCE: A Financial and Multimodal Jailbreak Detection Dataset](https://arxiv.org/abs/2602.18154)
*Mirae Kim,Seonghun Jeong,Youngjun Kwak*

Main category: cs.CL

TL;DR: FENCE是一个针对金融领域的双语多模态越狱检测数据集，包含韩英双语文本和图像威胁内容，用于训练和评估多模态语言模型的越狱检测器。


<details>
  <summary>Details</summary>
Motivation: 多模态语言模型在处理文本和图像时存在更广泛的攻击面，金融领域尤其需要专门的越狱检测资源，但现有资源稀缺。

Method: 构建包含金融相关查询和图像基础威胁的双语数据集，使用商业和开源VLMs进行实验验证，并训练基线检测器。

Result: 实验显示GPT-4o具有可测量的攻击成功率，开源模型更易受攻击；基于FENCE训练的检测器达到99%的分布内准确率，并在外部基准上保持强性能。

Conclusion: FENCE为金融领域多模态越狱检测提供了专注资源，支持在敏感领域开发更安全可靠的AI系统。

Abstract: Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.

</details>


### [59] [Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models](https://arxiv.org/abs/2602.18171)
*Wojciech Michaluk,Tymoteusz Urban,Mateusz Kubita,Soveatin Kuntur,Anna Wroblewska*

Main category: cs.CL

TL;DR: 提出一种结合Transformer文本嵌入和语言学特征混合方法检测点击诱饵标题，XGBoost模型在增强特征集上达到91% F1分数，优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 点击诱饵标题降低了在线信息质量并损害用户信任，需要开发有效的检测方法来提升信息可信度。

Method: 采用混合方法：结合基于Transformer的文本嵌入和15个语言学信息特征（如第二人称代词、最高级、数字和注意力标点），使用树基分类器（XGBoost）进行检测。

Result: 最佳模型XGBoost在增强特征集上达到91% F1分数，显著优于TF-IDF、Word2Vec、GloVe、LLM提示分类和仅特征基线方法。

Conclusion: 该方法不仅实现了高性能检测，还通过显式语言学特征增强了模型可解释性，支持透明和校准良好的预测，并发布了代码和模型促进可重复研究。

Abstract: Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.

</details>


### [60] [Improving Sampling for Masked Diffusion Models via Information Gain](https://arxiv.org/abs/2602.18176)
*Kaisen Yang,Jayden Teoh,Kaicheng Yang,Yitong Zhang,Alex Lamb*

Main category: cs.CL

TL;DR: 提出了Info-Gain Sampler解码框架，通过平衡即时不确定性和对未来掩码标记的信息增益，解决了现有MDM采样器忽视解码选择对后续步骤影响的局限性，在推理、编程、创意写作和图像生成等任务中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型采样器采用贪婪启发式方法，仅关注局部确定性，忽视了当前解码选择对后续步骤的下游影响，未能充分利用MDM的非因果特性来评估解码决策如何重塑剩余掩码位置的概率分布。

Method: 提出Info-Gain Sampler解码框架，通过计算信息增益来平衡即时不确定性和对未来掩码标记的影响，系统性地评估每个解码决策如何改变所有剩余掩码位置的概率分布。

Result: 在多样化架构和任务中表现优异：推理任务平均准确率提升3.6%，创意写作任务胜率达63.1%，推理任务的累积不确定性从78.4降至48.6，显著优于所有基线方法。

Conclusion: Info-Gain Sampler通过利用MDM的非因果特性，有效解决了现有采样器的局限性，为高质量生成提供了更优的解码策略，在多个领域展现出显著性能提升。

Abstract: Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.

</details>


### [61] [Information-Theoretic Storage Cost in Sentence Comprehension](https://arxiv.org/abs/2602.18217)
*Kohei Kajikawa,Shinnosuke Isono,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 本研究提出了一种基于信息论的处理存储成本度量方法，通过计算先前词汇在不确定性下对未来上下文的信息携带量来量化工作记忆负荷，该方法连续、理论中立且可从预训练神经语言模型中估计。


<details>
  <summary>Details</summary>
Motivation: 实时句子理解对工作记忆施加显著负荷，但现有基于符号语法的度量方法采用离散统一成本分配。需要一种更精确、连续的理论中立方法来量化处理存储成本。

Method: 基于信息论形式化处理存储成本，定义为先前词汇在不确定性条件下对未来上下文的信息携带量。使用预训练神经语言模型进行估计，并通过三个英语分析验证方法有效性。

Result: 该方法成功恢复了中心嵌入和关系从句中已知的处理不对称性，与语法基础存储成本在句法标注语料库中相关，并在两个大规模自然数据集上预测阅读时间方差，优于传统信息基础预测因子的基线模型。

Conclusion: 信息论基础的存储成本度量提供了一种连续、理论中立的替代方案，能够有效捕捉实时句子理解中的工作记忆负荷，为心理语言学理论提供了新的量化工具。

Abstract: Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.

</details>


### [62] [Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning](https://arxiv.org/abs/2602.18232)
*Lexiang Tang,Weihao Gao,Bingchen Zhao,Lu Ma,Qiao jin,Bang Yang,Yuexian Zou*

Main category: cs.CL

TL;DR: Thinking by Subtraction是一种基于置信度的对比解码方法，通过检测解码过程中的低置信度token并选择性干预，用最小占位符构建对比参考，在低置信位置减去参考分布来提升推理准确性并减少输出长度。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示推理不确定性高度局部化，少数低置信度token对推理错误和不必要输出扩展贡献最大，而传统的推理时缩放方法假设均匀增加计算能提升正确性，但存在计算冗余问题。

Method: 提出Confidence-Driven Contrastive Decoding (CCD)方法：1) 检测解码过程中的低置信度token；2) 在高置信度token位置使用最小占位符构建对比参考分布；3) 在低置信位置通过减去参考分布来精炼预测。

Result: 在数学推理基准测试中显著提升准确性，同时大幅减少输出长度，KV缓存开销最小。作为无需训练的方法，有效提升推理可靠性且无计算冗余。

Conclusion: CCD通过针对性的低置信度干预实现了更可靠的推理性能，证明了减法思维在语言模型推理中的有效性，为提升LLM推理效率提供了新思路。

Abstract: Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.

</details>


### [63] [Simplifying Outcomes of Language Model Component Analyses with ELIA](https://arxiv.org/abs/2602.18262)
*Aaron Louis Eidt,Nils Feldhus*

Main category: cs.CL

TL;DR: ELIA是一个交互式网络应用程序，通过整合归因分析、函数向量分析和电路追踪技术，并使用视觉语言模型自动生成自然语言解释，简化了大型语言模型的解释性分析，使非专家用户也能理解复杂模型分析结果。


<details>
  <summary>Details</summary>
Motivation: 解决机制可解释性工具的专业性门槛问题，大型语言模型的复杂性造成了可访问性差距，限制了这些工具只能被专家使用。

Method: 设计、构建和评估ELIA系统，整合三种关键技术（归因分析、函数向量分析、电路追踪），并引入新方法：使用视觉语言模型为复杂可视化自动生成自然语言解释。通过混合方法用户研究进行实证验证。

Result: 用户研究显示用户明显偏好交互式可探索界面而非静态可视化。AI生成的解释帮助非专家弥合知识差距，统计分析显示用户先前LLM经验与理解分数无显著相关性，表明系统降低了不同经验水平的理解障碍。

Conclusion: AI系统确实可以简化复杂模型分析，但其真正潜力在于与以用户为中心的设计相结合，优先考虑交互性、具体性和叙事指导。

Abstract: While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.

</details>


### [64] [PsihoRo: Depression and Anxiety Romanian Text Corpus](https://arxiv.org/abs/2602.18324)
*Alexandra Ciobotaru,Ana-Maria Bucur,Liviu P. Dinu*

Main category: cs.CL

TL;DR: 创建了首个罗马尼亚语心理健康语料库PsihoRo，包含205名受访者的文本数据和标准化筛查问卷，填补了罗马尼亚语心理NLP资源的空白。


<details>
  <summary>Details</summary>
Motivation: 罗马尼亚语缺乏开源心理健康语料库，而现有心理语料库在社交媒体数据收集中存在假设偏差问题，需要更实用的数据收集方法。

Method: 通过6个开放式问题和标准化PHQ-9、GAD-7筛查问卷收集数据，使用统计分析、罗马尼亚LIWC文本分析、情绪检测和主题建模方法。

Result: 成功构建了包含205个样本的罗马尼亚语抑郁和焦虑语料库，展示了该资源在心理文本分析中的重要特征。

Conclusion: PsihoRo是理解罗马尼亚人口心理健康文本分析的第一步，为NLP社区提供了重要的语言资源基础。

Abstract: Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.

</details>


### [65] [Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning](https://arxiv.org/abs/2602.18326)
*Tao Wu,Adam Kapelner*

Main category: cs.CL

TL;DR: 提出基于深度学习的系统，用于自动筛选高中语文词汇教学的优质上下文示例，比较三种建模方法，并引入新的评估指标RCC曲线来可视化模型性能权衡。


<details>
  <summary>Details</summary>
Motivation: 为高中语文词汇教学自动化筛选高质量上下文示例，解决人工筛选成本高且难以大规模应用的问题。

Method: 比较三种方法：(i)基于MPNet的无监督相似性策略；(ii)基于Qwen3微调嵌入和监督学习的非线性回归模型；(iii)方法(ii)加入手工特征。引入保留能力曲线(RCC)评估指标。

Result: 方法(iii)表现最佳，在仅丢弃70%优质上下文的情况下，达到了440:1的优质-劣质比例，显著优于其他方法。

Conclusion: 现代嵌入模型结合人工监督能够以低成本大规模生成近乎完美的词汇教学上下文，为语言教学提供了有效的自动化解决方案。

Abstract: We describe a modern deep learning system that automatically identifies informative contextual examples (\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.

</details>


### [66] [Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System](https://arxiv.org/abs/2602.18346)
*Pavithra PM Nair,Preethu Rose Anish*

Main category: cs.CL

TL;DR: Vichara是一个针对印度司法系统设计的法律判决预测框架，通过分解上诉案件文档为决策点，结合IRAC结构生成可解释的预测结果，在多个大型语言模型测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 印度法院面临大量案件积压，特别是上诉案件，需要利用人工智能技术提高判决预测效率和可解释性。

Method: 将英文上诉案件文档分解为包含法律问题、裁决机构、结果、推理和时间背景的决策点，采用基于IRAC框架的结构化表示方法，使用GPT-4o mini等四种大语言模型进行预测。

Result: 在PredEx和ILDC_expert数据集上超越现有基准，GPT-4o mini表现最佳（F1: 81.5和80.3），人类评估显示其解释在清晰度、关联性和实用性方面表现优越。

Conclusion: Vichara框架有效提升了印度上诉案件判决预测的准确性和可解释性，为法律专业人士提供了高效的决策支持工具。

Abstract: In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.

</details>


### [67] [Validating Political Position Predictions of Arguments](https://arxiv.org/abs/2602.18351)
*Jordan Robinson,Angus R. Williams,Katie Atkinson,Anthony G. Cohn*

Main category: cs.CL

TL;DR: 该研究提出双尺度验证框架处理主观连续属性知识表示，通过结合点对和配对人工标注来评估政治立场预测。使用22个语言模型对23,228个论点进行政治立场预测，点对验证显示中等程度的人机一致性（α=0.578），而配对验证显示更强的人机排名一致性（α=0.86）。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识表示需要捕捉主观连续属性（如政治立场），这与广泛接受的配对验证黄金标准存在冲突。需要解决主观连续知识验证的挑战。

Method: 采用双尺度验证框架，结合点对和配对人工标注方法。使用22个语言模型对来自英国政治电视节目《Question Time》30场辩论的23,228个论点进行政治立场预测，构建大规模知识库。

Result: 点对评估显示中等程度的人机一致性（Krippendorff's α=0.578），反映内在主观性；配对验证显示人机排名一致性显著更强（最佳模型α=0.86）。成功构建了经过验证的结构化论证知识库。

Conclusion: 该工作贡献了：(1)平衡可扩展性与可靠性的主观连续知识验证方法；(2)支持图推理和检索增强生成的政治领域验证知识库；(3)证明可以从点对语言模型预测中提取序数结构，推进传统符号或分类方法不足领域的知识表示能力。

Abstract: Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $α=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($α=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.

</details>


### [68] [SPQ: An Ensemble Technique for Large Language Model Compression](https://arxiv.org/abs/2602.18420)
*Jiamin Yao,Eren Gultepe*

Main category: cs.CL

TL;DR: SPQ是一种集成压缩技术，结合SVD、剪枝和量化三种方法，在LLaMA-2-7B上实现75%内存减少，同时保持或改善困惑度，推理速度提升1.9倍。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型部署中的内存效率问题，需要一种综合压缩方法来解决不同来源的冗余：MLP层的冗余神经元、注意力投影的高维特征和线性层的数值精度冗余。

Method: 采用三阶段集成压缩：1)基于激活的剪枝去除MLP冗余神经元；2)保留方差的SVD分解压缩注意力投影为低秩因子；3)8位线性量化压缩所有线性层。

Result: 在相同压缩比下，SPQ优于单一方法，WikiText-2困惑度从5.47降至4.91，内存使用6.86GB（比GPTQ的7.16GB更少），下游任务准确率保持，推理吞吐量提升1.9倍。

Conclusion: SPQ通过层感知和互补压缩技术实现了稳健压缩，为内存受限环境中的LLM实际部署提供了实用解决方案，代码已开源。

Abstract: This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/

</details>


### [69] [RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering](https://arxiv.org/abs/2602.18425)
*Deniz Qian,Hung-Ting Chen,Eunsol Choi*

Main category: cs.CL

TL;DR: RVR是一个多轮检索框架，通过检索-验证-检索的迭代过程最大化答案覆盖率，使用验证器筛选高质量文档并动态扩展查询，在多个数据集上显著提升完整召回率。


<details>
  <summary>Details</summary>
Motivation: 解决需要获取多样化有效答案的查询问题，传统检索方法难以全面覆盖所有可能的正确答案。

Method: 提出检索-验证-检索(RVR)框架：首轮检索原始查询获取候选文档，验证器筛选高质量子集，后续轮次将已验证文档加入查询进行扩展检索。支持现成检索器并可针对该推理过程微调。

Result: 在QAMPARI多答案检索数据集上相对提升10%、绝对提升3%的完整召回率；在QUEST和WebQuestionsSP两个域外数据集上对不同基础检索器均获得一致提升。

Conclusion: RVR提供了一个有前景的迭代方法，通过验证器和适应新推理场景的检索器实现全面的答案召回。

Abstract: Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.

</details>


### [70] [VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning](https://arxiv.org/abs/2602.18429)
*Harshul Raj Surana,Arijit Maji,Aryan Vats,Akash Ghosh,Sriparna Saha,Amit Sheth*

Main category: cs.CL

TL;DR: 提出了VIRAASAT数据集和SCoM框架，用于评估和改进LLM在印度文化多跳推理任务上的表现。VIRAASAT包含3200多个多跳问题，覆盖印度所有地区和文化属性。SCoM通过训练模型内部模拟知识图谱操作，相比标准CoT方法提升性能达20%。


<details>
  <summary>Details</summary>
Motivation: 现有文化基准测试存在三个主要问题：(1)手工制作成本高昂；(2)仅包含测试事实回忆的单跳问题；(3)难以扩展。LLM在需要丰富社会文化知识和多样化本地语境的任务中表现不佳，特别是在涉及印度文化的任务上。

Method: 1. 开发VIRAASAT数据集：采用半自动化多跳方法生成文化特定QA数据集，基于包含700多个专家策划文化实体的知识图谱，覆盖印度文化的13个关键属性和所有28个邦8个中央直辖区。
2. 提出SCoM框架：适应Chain-of-Manipulation范式，训练模型内部模拟原子知识图谱操作，可靠地遍历图谱拓扑结构。

Result: 1. VIRAASAT数据集包含超过3200个需要链式文化推理的多跳问题
2. 评估发现当前SOTA LLM在推理中存在关键限制：基于CoT轨迹的微调无法很好地处理低概率事实的接地和综合
3. SCoM在监督微调实验中比标准CoT基线性能提升高达20%

Conclusion: VIRAASAT数据集为构建文化感知推理模型奠定了坚实基础。SCoM框架通过教导模型可靠地遍历知识图谱拓扑结构，有效提升了LLM在文化多跳推理任务上的表现，为解决LLM在文化语境下的推理限制提供了有效方案。

Abstract: Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.

</details>
